var tipuesearch = {"pages":[{"title":"Multicollinearity:  Concept, Effects, Detection, Remedies and Exceptions in Python","text":"In statistics, multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy. In this situation the coefficient estimates of the multiple regression may change erratically in response to small changes in the model or the data. Multicollinearity does not reduce the predictive power or reliability of the model as a whole, at least within the sample data set; it only affects calculations regarding individual predictors. That is, a multivariate regression model with collinear predictors can indicate how well the entire bundle of predictors predicts the outcome variable, but it may not give valid results about any individual predictor, or about which predictors are redundant with respect to others. - From Wikipedia Data scientists love linear regression due to its high model interpretability. Unlike other complex machine learning models (ex: K-nearest neighbor, random forest, or neural net), the interpretation of model parameters is straightforward. Assume a linear regression model defined by $y = 110 x_1 + 50 x_2$ . Then, increasing $x_1$ by 2 will increase $y$ by 220, and increasing $x_2$ by 3 will increase y by 150, controlling the other variable constant. However, it turns out that you can't really \"control\" the other variable under multicollinearity. Sometimes the values or signs (+/-) of linear regression coefficients have special meanings. This is the case when a linear regression model is based on a physics model. For illustration, I show eq (1) , the material balance equation (MBE) used in petroleum engineering to calculate the amount of oil and gas trapped in a geologic hydrocarbon reservoir (source: Eq 14. from this petrowiki page ). $$ F = G_{fgi}x_{1} + N_{foi}x_{2} + W_e \\tag{1}$$ where $F$ : \"Dependent Variable\" total net fluid withdrawal or production $x_{1}$ : \"Independent Variable\" composite gas expansivity $x_{2}$ : \"Independent Variable\" composite oil expansivity $G_{fgi}$ : \"Regression Coefficient\" initial free gas $N_{foi}$ : \"Regression Coefficient\" initial free oil $W_{e}$ : \"Intercept\" cumulative water influx The individual value of the regression coefficients in eq (1) is important, because oil $N_{foi}$ is more expensive than gas $G_{fgi}$ in the market. Oil & gas companies make decisions about where to drill the next well based on economic projections made from the values of $N_{foi}$ and $G_{fgi}$ . However, sometimes the values of $N_{foi}$ and $G_{fgi}$ solved with a multiple linear regression model are unreliable due to multicollinearity. I numerically demonstrate this effect in Section 3.1: Model instability with sampling below . This post attempts to help your understanding of multicollinearity and introduce solutions in Python. All sections come with Python code snippets. Contents 0 Sample data description 1 Key takeaways 2 Concept 3 Effects 3.1 Model instability with sampling 3.2 Overestimated standard errors 3.3 Structural multicollinearity with interaction terms 4 Detection 4.1 VIF Notes: Calculation of VIF assumes a certain structure of a model Pythonic Tip: Three ways to compute VIF in Python 4.2 Correlation matrix Notes: Partial correlation matrix 5 Remedies 5.1 Increase sample size Notes: Sample size and width of confidence interval 5.2 Mean centering Notes: Statistical significance in linear regression analysis 5.3 Elimination 5.4 Principle component regression 6 Pitfalls Statistical significance in linear regression analysis Notes: Sample size and width of confidence interval Partial correlation matrix Width of confidence interval and sample size Warning! Be careful not to eliminate features just based on VIF score. Use feature ranking knowledge Notes: Variations of correlation matrix. Wrong data encoding - dummy variable Notes: Calculation of VIF assumes a certain structure of a model. Calculation of VIF assumes a certain structure of a model. 0. Sample data description We will generate synthetic linear data sets throughout this post. Note that I chose not to use real-life data, because it is difficult to demonstrate the instability of regression coefficients under multicollinearity; we do not know the true values of regression coefficients. With synthetic data, however, we can control the true values of regression coefficients and the degree of collinearity with Algorithm 1 . Assume that we want to generate collinear data with three features described by the linear model: $y = 2 x_1 + 4 x_2 + 10 x_3 + 2$ . We can set the true values of regression coefficients with true_coefs and true_intercept : In [1]: true_coefs = [ 2 , 4 , 10 ] # true values of the regression coefficients true_intercept = 2 # true vale of the intercept We can also control the degree of collinearity. If we want high collinearity, fill the diagonal elemens with 1 , and fill the non-diagonal elements of the positive semi-definite covariance matrix cov with values close to 1 : In [23]: correlation = 0.95 # HIGH COLLINEARITY cov = np . full (( len ( true_coefs ), len ( true_coefs )), correlation ) np . fill_diagonal ( cov , 1 ) In [24]: cov Out[24]: array([[1. , 0.95, 0.95], [0.95, 1. , 0.95], [0.95, 0.95, 1. ]]) If low collinearity is desired, fill the non-diagonal elements of cov with values close to 0: In [25]: correlation = 0.01 # LOW COLLINEARITY cov = np . full (( len ( true_coefs ), len ( true_coefs )), correlation ) np . fill_diagonal ( cov , 1 ) In [26]: cov Out[26]: array([[1. , 0.01, 0.01], [0.01, 1. , 0.01], [0.01, 0.01, 1. ]]) We generate collinear independent variables (features) with np.random.multivariate_normal . The function takes cov as one of the argument, which makes it easy to generate collinear features. The below code snippet simulates n = 100 observations of highly collinear data points, sampled from the population described by the linear model $y = 2 x_1 + 4 x_2 + 10 x_3 + 2$ with Gaussian noise. Algorithm 1: Random generation of highly collinear data In [2]: import numpy as np # settings n = 100 # sample size true_coefs = [ 2 , 4 , 10 ] # linear regression coefficients, 3 features true_intercept = 2 # y-intercept feature_means = [ 5 , 1 , 12 ] # means of gaussian features. This is not important correlation = 0.01 # degree of collinearity (LOW) # positive semi-definite covariance matrix. cov = np . full (( len ( true_coefs ), len ( true_coefs )), correlation ) np . fill_diagonal ( cov , 1 ) # random generation of 3D gaussian collinear features. 100 x 3 numpy array X = np . random . multivariate_normal ( mean = feature_means , cov = cov , size = n ) # generate gaussian white noise. mean = 0, standard deviation = 2. 100 x 1 numpy array gaussian_noise = np . random . normal ( loc = 0 , scale = 2 , size = n ) # make the outcome. 100 x 1 numpy array y = true_intercept + true_coefs [ 0 ] * X [:, 0 ] + true_coefs [ 1 ] * X [:, 1 ] + true_coefs [ 2 ] * X [:, 2 ] + gaussian_noise 1. Key takeaways 1. Confidence interval quantifies uncertainty of statistical estimation Confidence interval qunatifies the uncertainty related to a statistical estimation to mitigate the issue of Population vs. Samples . It is always expressed in a range like â€” $\\text{C.I.}: \\quad \\bar{x} \\pm 3.43$ or $-51.4 < \\bar{x} < -43.2$ 2. Confidence interval is the basis of parametric hypothesis tests Confidence interval is the basis of parametric hypothesis tests. For example, t-test computes its p-value using the confidence interval of difference in mean . When samples follow a normal distribution, and therefore their centeral tendency can be described by their means, t-test can be used to conclude if two distributions are significantly different from each other. However, the model fit statistics, such as adjusted R-squared and RMSE are not affected by multicollinearity. It's entirely possible that the variable with the the insignificant p-value should actually be significant. A classic hallmark of multicollinearity is that it makes variables appear to be insignificant even when they're not The goal of remedial procedures are to increase precision. 2. Concept Wikipedia has a good & concise definition of collinearity and multicollinearity. Collinearity is a linear association between two explanatory variables. Two variables are perfectly collinear if there is an exact linear relationship between them. For example, $X_1$ and $X_2$ are perfectly collinear if there exist parameters $\\lambda_0$ and $\\lambda_1$ such that, for all observations $i$ , we have: $$ X_{2i} = \\lambda_0 + \\lambda_1 X_{1i} \\tag{2}$$ Multicollinearity refers to a situation in which two or more explanatory variables in a multiple regression model are linearly related. A feature does not have to be directly linearly related to a response variable, if it affects an another feature that is linearly related to the response variable. For example in figure (1-C) , we see that although $X_2$ is not related to $Y$ , $X_2$ is related to $X_1$ , which in turn affects $Y$ . Eq (2) shows the case of perfect collinearity, in which the correlation between $X_1$ and $X_2$ is equal to 1 or -1. In practice, we rarely face perfect multicollinearity in a data set. More commonly, the issue of multicollinearity arises when there is an approximate linear relationship among two or more independent variables. For example in figure (1-B) , we see that most of the variability of the Venn diagram can be explained by just $X_1$ and $Y$ , and that we don't really need $X_2$ , due to the high degree of multicollinearity between $X_1$ and $X_2$ , represented by the area of overlap (yellow color). Figure 1: Multicollinearity Venn diagram. Yellow regions represent the degree of multicollinearity between the independent variables $X_1$ and $X_2$ Let's try to understand multicollinearity in greater detail with thought experiments. Consider the following two scenarios. Scenario 1 As a microbiologist, you want to fix the number of bacteria A growing in a petri dish to be $Y = 10,000$ . The number of bacteria A is a function of temperature and the amount of toxic chemical, described by the following linear model: $$ Y = \\beta_1 \\cdot \\text{temperature} - \\beta_2 \\cdot \\text{toxic chemical} + \\beta_0 \\tag{3}$$ Based on this linear model, you can make an informed decision about how to manipulate the features (temperature and the amount of toxic chemicals) to control the number of bacteria in the petri dish. With eq (3) , you have full control of the number of bacteria, because the impact of each features is understood by their respective regression coefficients ( $\\beta_1$ for temperature, and $\\beta_2$ for toxic chemicals), and the features are independent (no multicollinearity). In the other words, temperature does not affect the amount of toxic chemicals, and vice versa. Scenario 2 Now, let's assume a scenario in which the features are not independent from each other. You are growing bacteria A and bacteria B together in the same petri dish, and want to fix the number of bacteria A to be $Y = 10,000$ . The following information is known: Bacteria B preys on bacteria A. Temperature increases the number of the both bacteria groups. Each bacteria group emits different amount of methane. Methane traps heat, and increases temperature. Toxic chemical negatively affects bacteria A. The impact of the toxic chemical on bacteria B is unknown. The number of bacteria A is a function of temperature, the amount of toxic chemical, and the number of bacteria B, described by the following linear model: $$ Y = \\beta_1 \\cdot \\text{temperature} - \\beta_2 \\cdot \\text{toxic chemical} -\\beta_3 \\cdot \\text{bacteria B} + \\beta_0 \\tag{4}$$ It is difficult to make an informed decision about how to manipulate the features to get the target number $Y = 10,000$ , because the features are correlated with one another. For example, increased temperature results in increased number of bacteria A, but it also results in increased number of bacteria B which in turn decreases the number of bacteria A. On the other hand, increasing the number of bacteria B decreases the number of bacteria A, but it affects the total amount of methane emitted, which in turn affects temperature. Furthermore, we do not understand the net impact of the toxic chemical, since its relationship with the number of bacteria B is unknown. This blackbox relationship among features is quite common in real-life applications, which makes it difficult to understand the impact of individual regression coefficients on the response variable. The situation in which features are correlated with one another is called multicollinearity. Under multicollinearity, the impact of individual regression coefficients on a response variable is obfuscated, and the values of the individual regression coefficiens are unreliable. I demonstrate the effects of multicollinearity in Python in Section 4: Effects below . WARNING! Multicollinearity does not reduce the predictive power or reliability of the model as a whole. That is, a multiple regression model with collinear features can indicate how well the \"entire bundle\" of features predicts the response variable, but it may not give valid results for individual features in which the features are somewhat redundant with each other. Multicollinearity is an issue only for problems in which the values of the individual regression coefficients has special meanings. If predicting the response varialbe is your only interest, you can safely ignore multicollinearity. I discuss more situations in which you can ignore multicollinearity below . 3. Effects This section discusses & demonstrates the effects of multicollinearity in Python. Model instability with sampling Overestimated standard errors Structural multicollinearity with interaction terms 3.1. Model instability with sampling Under multicollinearity, the regression coefficients of features change erratically in response to small changes in the sample data. Assume that you trained the following linear regression model with your current data set: $$ y = 11.7 x_1 - 0.47 x_2 + 540 \\tag{5}$$ You obtain a few additional samples and re-train your regression model. This time, you observe drastic change in the values of the individual regression coefficient: $$ y = 14.1 x_1 + 0.74 x_2 + 231 \\tag{6}$$ The change was so drastic that the sign of the coefficient for $x_2$ changed from negative (-) to positive (+). This is unacceptable if you wish to know whether a particular feature negatively or positively affects a response variable by the sign of its coefficient. I show how multicollinearity causes such instability in regression coefficients with sampling through a Python simulation. For the simulation, we fit many multiple linear regression models with 2 features and an intercept. We generate synthetic data set with the procedures described in Section 0. Sample data description above , for both low and high multicollinearity populations. We set true_coefs = [13, 0.5] and true_intercept = 2 , and add Gaussian noise to the model. For each model, we use n = 100 data points to fit coefficients and an intercept. We fit m = 2000 different models. This essentially mimicks drawing samples from a population described by the true model $y = 13 x_1 + 0.5 x_2 + 2$ with Gaussian noise. This is a type of Monte-Carlo method , in which the simulation relies on repeated generation of random numbers to investigate some characteristic of a statistic which is hard to derive analytically. The result is shown in figure (2) . Note that the population with low multicollinearity forms a tight circular cluster of regression coefficiens around the true coefficient values true_coefs = [13, 0.5] , whereas the population with high multicollinearity shows greater variance, represented by its wider elliptical cluster. You can see that the values of coefficient for $x_2$ ranges between -1.5 to 2. This explains the sign change of the regression coefficient for $x_2$ between eq (5) and eq (6) . Figure 2: Model instability due to multicollinearity Source Code For Figure (2) import numpy as np from sklearn import linear_model import matplotlib.pyplot as plt def generate_collinear_data(cov, n=10, true_coefs=[0, 0], true_intercept=0, feature_means=[0, 0, 0], loc=0, scale=1): # random generation of 3D gaussian collinear features. X = np.random.multivariate_normal(mean=feature_means, cov=cov, size=n) # generate gaussian white noise. gaussian_noise = np.random.normal(loc=loc, scale=scale, size=n) # make the outcome. y = true_intercept + gaussian_noise for i in range(len(true_coefs)): y += true_coefs[i] * X[:, i] return X, y # settings m = 2000 # number of simulations correlation_lo = 0.01 # degree of collinearity (LOW) correlation_hi = 0.98 # degree of collinearity (HIGH) kwargs = { 'n': 100, # sample size 'true_coefs': [13, 0.5], # linear regression coefficients, 2 features 'true_intercept': 2, # y-intercept 'feature_means': [0, 0], # means multivariate normal distribution. This is not important 'loc': 0, # mean of gaussian noise 'scale': 1 # standard deviation of gaussian noise } # high collinearity covariance matrix cov_hi = np.full((len(kwargs['true_coefs']), len(kwargs['true_coefs'])), correlation_hi) np.fill_diagonal(cov_hi, 1) # low collinearity covariance matrix cov_lo = np.full((len(kwargs['true_coefs']), len(kwargs['true_coefs'])), correlation_lo) np.fill_diagonal(cov_lo, 1) # simulation model_hi_col_coefs = [] model_lo_col_coefs = [] for i in range(m): X_hi, y_hi = generate_collinear_data(cov_hi, **kwargs) ols_hi = linear_model.LinearRegression() model_hi = ols_hi.fit(X_hi, y_hi) model_hi_col_coefs.append(model_hi.coef_) X_lo, y_lo = generate_collinear_data(cov_lo, **kwargs) ols_lo = linear_model.LinearRegression() model_lo = ols_lo.fit(X_lo, y_lo) model_lo_col_coefs.append(model_lo.coef_) # list to numpy array conversion model_hi_col_coefs = np.array(model_hi_col_coefs) model_lo_col_coefs = np.array(model_lo_col_coefs) # plotting plt.style.use('default') plt.style.use('ggplot') fig, ax = plt.subplots(figsize=(8, 4.5)) ax.scatter(model_hi_col_coefs[:, 0], model_hi_col_coefs[:, 1], s=10, label='High multicollinearity', edgecolor='red', facecolor='firebrick', alpha=0.7) ax.scatter(model_lo_col_coefs[:, 0], model_lo_col_coefs[:, 1], s=10, label='Low multicollinearity', edgecolor='dimgrey', facecolor='k', alpha=0.7) ax.set_xlim(10.5, 15.5) ax.set_ylim(-2, 3) ax.set_xlabel('$x_1$ coefficient values', fontsize=16) ax.set_ylabel('$x_2$ coefficient values', fontsize=16) ax.set_title('Model instability due to multicollinearity', fontsize=20) ax.legend(facecolor='white', fontsize=11) ax.text(0.2, 0.1, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) fig.tight_layout() The above scatter plot shows the effect of multicollinearity for linear models with two features. How does multicollinearity affect regression coefficients when there are more than two features? In figure (3) , I simulated 10,000 linear models with five features and an intercept: 'true_coefs': [13, 0.5, 5, -1, -24] , and 'true_intercept': 2 . Similar to figure (2) , linear models trained from low multicollinearity population showed narrower range of coefficients than those of high multicollinearity population. Note that multicollinearity does not affect the range of intercepts. Figure 3: Model instability due to multicollinearity, 5 features. Source Code For Figure (3) import numpy as np import statsmodels.api as sm import matplotlib.pyplot as plt def generate_collinear_data(cov, n=10, true_coefs=[0, 0], true_intercept=0, feature_means=[0, 0, 0], loc=0, scale=1): # random generation of 3D gaussian collinear features. X = np.random.multivariate_normal(mean=feature_means, cov=cov, size=n) # generate gaussian white noise. gaussian_noise = np.random.normal(loc=loc, scale=scale, size=n) # make the outcome. y = true_intercept + gaussian_noise for i in range(len(true_coefs)): y += true_coefs[i] * X[:, i] return X, y # settings m = 10000 # number of simulations correlation_lo = 0.01 # degree of collinearity (LOW) correlation_hi = 0.98 # degree of collinearity (HIGH) kwargs = { 'n': 100, # sample size 'true_coefs': [13, 0.5, 5, -1, -24], # linear regression coefficients, 2 features 'true_intercept': 2, # y-intercept 'feature_means': [0, 0, 0, 0, 0], # means multivariate normal distribution. This is not important 'loc': 0, # mean of gaussian noise 'scale': 1 # standard deviation of gaussian noise } # high collinearity covariance matrix cov_hi = np.full((len(kwargs['true_coefs']), len(kwargs['true_coefs'])), correlation_hi) np.fill_diagonal(cov_hi, 1) # low collinearity covariance matrix cov_lo = np.full((len(kwargs['true_coefs']), len(kwargs['true_coefs'])), correlation_lo) np.fill_diagonal(cov_lo, 1) # simulation hi_col_params = [] lo_col_params = [] for i in range(m): # high collinearity data X_hi, y_hi = generate_collinear_data(cov_hi, **kwargs) X_st_hi = sm.add_constant(X_hi) model_hi = sm.OLS(y_hi, X_st_hi).fit() hi_col_params.append(model_hi.params) # low collinearity data X_lo, y_lo = generate_collinear_data(cov_lo, **kwargs) X_st_lo = sm.add_constant(X_lo) model_lo = sm.OLS(y_lo, X_st_lo).fit() lo_col_params.append(model_lo.params) # list to numpy conversion hi_col_params = np.asarray(hi_col_params) lo_col_params = np.asarray(lo_col_params) # plotting def styling(ax): ax.set_facecolor('#eeeeee') ax.grid(True, linestyle='--', color='#acacac') ax.tick_params(color='grey') _ = [spine.set_edgecolor('grey') for spine in ax.spines.values()] ax.text(0.5, 0.1, 'aegis4048.github.io', fontsize=12, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) ax.set_xticklabels(['High', 'Low'], fontsize=15) boxplot_styling = { 'sym': '', 'whis': [2.5, 97.5], 'showfliers': False, 'boxprops': dict(linewidth=2.0, color='#4e98c3'), 'whiskerprops': dict(linewidth=2.0, color='#4e98c3', linestyle='--'), 'vert': True, 'capprops': dict(linewidth=2.0, color='k'), 'medianprops': dict(linewidth=2.0, color='#ad203e'), 'widths': (0.4, 0.4) } labels = ['Intercept', '$X_1$', '$X_2$', '$X_3$', '$X_4$', '$X_5$'] fig, axes = plt.subplots(1, len(labels), figsize=(16, 6)) for i, (ax, label) in enumerate(zip(axes, labels)): ax.boxplot([list(hi_col_params[:, i]), list(lo_col_params[:, i])], **boxplot_styling) styling(ax) ax.set_title(label, fontsize=20, y=-0.2) fig.tight_layout(rect=[0, 0.05, 1, 0.91]) fig.suptitle('Range of coefficients with low & high multicollinearity - 10,000 simulations', fontsize=25); 3.2. Overestimated standard errors (Notes: There is a version conflict between Scipy and Statsmodel. You need Scipy version 1.2 or below or 1.4 or above to use Statsmodel. In my virtual environment, I have statsmodels==0.10.2 and scipy==1.4.1 ) In this section, I demonstrate the the unreliability of linear regression analysis report due to multicollinearity by comparing the analytical solutions of the report, and the actual parameter values obtained with numerical simulations. Throughout your study of multiple linear regression, you have probably encountered the linear regression analysis report that looks like figure (4) , which is generated with statsmodels Python library (the figure is generated with the same simulation settings of figure (3) 's high multicollinearity data). The problem is that multicollinearity makes this report unreliable, especially with the standard errors of the individual regression coefficients. Note that this is not the problem of statsmodels library, but the inherent statistical problem caused by multicollinearity. Problems with standard errors sequentially cause problems with the t-statistic t , p-value P>|t| , and 95% confidence interval [0.025 0.975] of the regression coefficients. This is because standard error is used to compute a confidence interval, and confidence interval is closely related to hypothesis testing, which involves t-statistic and p-value (if you are curious about the relationship between confidence interval and hypothesis testing, check Section 4.2: Confidence interval of difference in mean of my previous post ). Overestimated standard errors sometimes result in failure to reject the null hypothesis on coefficient significance, when it shouldn't be rejected. Note that the overestimated standard errors affect the significance test and confidence intervals of coefficients, but do not affect the coefficients themselves. However, recall that the coefficients are already unstable due to multicollinearity, as explained in Section 3.1: Model instability with sampling above . So now you have two sources of pain; not only you can't trust the values of the coefficient themselves, but also their significance test results and confidence intervals. Check Section 4.1: VIF below to understand mathematical relationship between multicollinearity and standard errors. Figure 4: Linear regression analysis becomes unreliable with multicollinearity. Source Code For Figure (4) import numpy as np import statsmodels.api as sm import matplotlib.pyplot as plt def generate_collinear_data(cov, n=10, true_coefs=[0, 0], true_intercept=0, feature_means=[0, 0, 0], loc=0, scale=1): # random generation of 3D gaussian collinear features. X = np.random.multivariate_normal(mean=feature_means, cov=cov, size=n) # generate gaussian white noise. gaussian_noise = np.random.normal(loc=loc, scale=scale, size=n) # make the outcome. y = true_intercept + gaussian_noise for i in range(len(true_coefs)): y += true_coefs[i] * X[:, i] return X, y # settings m = 10000 # number of simulations correlation_hi = 0.98 # degree of collinearity (HIGH) kwargs = { 'n': 100, # sample size 'true_coefs': [13, 0.5, 5, -1, -24], # linear regression coefficients, 2 features 'true_intercept': 2, # y-intercept 'feature_means': [0, 0, 0, 0, 0], # means multivariate normal distribution. This is not important 'loc': 0, # mean of gaussian noise 'scale': 1 # standard deviation of gaussian noise } # high collinearity covariance matrix cov_hi = np.full((len(kwargs['true_coefs']), len(kwargs['true_coefs'])), correlation_hi) np.fill_diagonal(cov_hi, 1) X, y = generate_collinear_data(cov_hi, **kwargs) X_st = sm.add_constant(X) model = sm.OLS(y, X_st).fit() print(model.summary()) Let's try to understand this with numerical simulations. The idea of using numerical simulations is based on the Law of Large Numbers (LLM): the result of performing the same experiment a large number of times will converge to a single value. That is, we should be able to obtain the true confidence interval of the linear regression coefficients for high multicollinearity data. Recall that this is impossible with analytical solutions reported by linear regression analysis result shown in figure (4) due to multicollinearity. I generated 100,000 high multicollinearity data sets with the procedures described in Section 0. Sample data description above , with five features and an intercept: 'true_coefs': [13, 0.5, 5, -1, -24] , and 'true_intercept': 2 . Then, I trained 100,000 different linear models. In figure (5) , we observe that the 50th percentiles of the analytical C.I. are always off from the true value, where as those of the numerical C.I. correctly guess the true value. These offsets are consistent with the effects of multicollinearity I discussed in Section 3.1: Model instability with sampling above . According to LLM, we know that the C.I. of numerical solution must be true, and that anything not consistent with the numerical C.I. is wrong. In figure (6) , we observe that the widths of the analytical C.I. are wider than those of the numerical C.I. This is because multicollinearity increases the analytically computed standard errors, which in turn increases the widths of the C.I. But to be fair, the issues with the overestimated standard error is not that big of a deal; it only adds a small problem, on the top of the big problem, which is the actual offset of the coefficients. Furthermore, sometimes the analytical C.I. of high multicollinearity data is narrower than its true confidence interval, which is the case for the feature $X_1$. While it is generally the case that multicollinearity widens the confidence interval, its not always the case. Figure 5: Unstable model parameters due to high multicollinearity Figure 6: Overestimated confidence intervals due to high multicollinearity Source Code For Figure (5) and (6) import numpy as np import statsmodels.api as sm import matplotlib.pyplot as plt def generate_collinear_data(cov, n=10, true_coefs=[0, 0], true_intercept=0, feature_means=[0, 0, 0], loc=0, scale=1): # random generation of 3D gaussian collinear features. X = np.random.multivariate_normal(mean=feature_means, cov=cov, size=n) # generate gaussian white noise. gaussian_noise = np.random.normal(loc=loc, scale=scale, size=n) # make the outcome. y = true_intercept + gaussian_noise for i in range(len(true_coefs)): y += true_coefs[i] * X[:, i] return X, y # simulation settings m = 100000 # number of simulations. WARNING!!! Large m makes execution time very long correlation = 0.98 # degree of collinearity (HIGH) kwargs = { 'n': 50, # sample size 'true_coefs': [13, 0.5, 5, -1, -24], # linear regression coefficients, 2 features 'true_intercept': 2, # y-intercept 'feature_means': [0, 0, 0, 0, 0], # means multivariate normal distribution. This is not important 'loc': 0, # mean of gaussian noise 'scale': 1 # standard deviation of gaussian noise } # high collinearity covariance matrix cov = np.full((len(kwargs['true_coefs']), len(kwargs['true_coefs'])), correlation) np.fill_diagonal(cov, 1) ######################################### numerical solution ################################################ params = [] for i in range(m): X, y = generate_collinear_data(cov, **kwargs) # generate high collinearity data X_st = sm.add_constant(X) model = sm.OLS(y, X_st).fit() params.append(model.params) params = np.asarray(params) # list to numpy conversion numerical_means = np.mean(params, axis=0) # solution numerical_lo = np.percentile(params, 2.5, axis=0) # lower bound of 95% confidence interval numerical_hi = np.percentile(params, 97.5, axis=0) # upper bound of 95% confidence interval numerical_width = abs(numerical_hi - numerical_lo) # width of confidence interval ######################################### analytical solution ############################################### X, y = generate_collinear_data(cov, **kwargs) # generate high collinearity data X_st = sm.add_constant(X) model = sm.OLS(y, X_st).fit() analytical_means = model.params # solution analytical_lo = model.conf_int(alpha=0.05)[:, 0] # lower bound of 95% confidence interval analytical_hi = model.conf_int(alpha=0.05)[:, 1] # upper bound of 95% confidence interval analytical_width = abs(analytical_hi - analytical_lo) # width of confidence interval ######################################### Plotting: Error bars ############################################## def styling(ax, label): ax.set_xlabel(label, fontsize=25) ax.set_facecolor('#eeeeee') ax.grid(True, linestyle='--', color='#acacac', axis='y') ax.tick_params(color='grey') ax.set_xticklabels([]) ax.set_xlim(0, 2) _ = [spine.set_edgecolor('grey') for spine in ax.spines.values()] ax.text(0.5, 0.1, 'aegis4048.github.io', fontsize=12, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) labels = ['Intercept', '$X_1$', '$X_2$', '$X_3$', '$X_4$', '$X_5$'] true_params = [kwargs['true_intercept']] + kwargs['true_coefs'] x = np.array([1, 2]) fig, axes = plt.subplots(1, len(labels), figsize=(16, 6)) for i, (ax, label, true_param) in enumerate(zip(axes, labels, true_params)): yerr_numerical = [[abs(numerical_lo[i] - numerical_means[i])], [abs(numerical_hi[i] - numerical_means[i])]] yerr_analytical = [[abs(analytical_lo[i] - analytical_means[i])], [abs(analytical_hi[i] - analytical_means[i])]] ax.errorbar(1 - 0.25, numerical_means[i], yerr=yerr_numerical, fmt='o', label='95% numerical C.I.', color='k', markersize=8, capsize=8, elinewidth=2.5, markeredgewidth=2.5) ax.errorbar(1 + 0.25, analytical_means[i], yerr=yerr_analytical, fmt='o', label='95% analytical C.I.', color='grey', markersize=8, capsize=8, elinewidth=2.5, markeredgewidth=2.5) ax.axhline(true_param, linestyle='-.', color='#ad203e', label='True value') styling(ax, label) axes[0].set_ylabel('Range of 95% C.I.', fontsize=30) fig.legend(*ax.get_legend_handles_labels(), loc='lower center', ncol=3, borderaxespad=0.7, fontsize=17) fig.suptitle('Effect of overestimated standard errors - %.f simulations' % m, fontsize=25); fig.tight_layout(rect=[0, 0.10, 1, 0.9]) ######################################### Plotting: Bar chart ############################################### x = np.array([i for i in range(len(analytical_width))]) fig, ax = plt.subplots(figsize=(16, 6)) ax.bar(x - 0.15, numerical_width, align='center', alpha=0.75, width=0.28, color='k', label='Numerical Sol.') ax.bar(x + 0.15, analytical_width, align='center', width=0.28, color='grey', label='Analytical Sol.') ax.text(0.1, 0.65, 'aegis4048.github.io', fontsize=17, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) ax.set_ylabel('Width of 95% C.I.', fontsize=30) ax.legend(fontsize=17, loc='upper left') ax.set_xticks(x) ax.set_xticklabels([str(label) for label in labels], fontsize=25) ax.set_facecolor('#eeeeee') ax.grid(True, linestyle='--', color='#acacac', axis='y') ax.tick_params(color='grey') _ = [spine.set_edgecolor('grey') for spine in ax.spines.values()] fig.tight_layout() WARNING! When training real-life models, you don't have an access to the numerical solutions as in figure (5) and figure (6) , because you only have one sample. This means that you have to rely on linear regression analysis report like figure (4) , which is unreliable under multicollinearity. This is why you need remedial procedures, which I discuss in Section 5: Remedies below . 3.3. Structural multicollinearity with interaction terms Structural multicollinearity occurs when you create synthetic features from the original features. It is a byproduct of the model we specify rather than being present in the data itself. For example, imagine a population modeled by $y = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_0$ with gaussian noise. Let's say that you (a data scientist) don't like the model, and want to add an interaction term to increase statistical power of the model. Your new model becomes $y = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1&#94;2 + \\beta_5 (x_2 x_3) + \\beta_0$ . But there's an obvious problem here. If you add the square of $x_1$ to the model, naturally there is a high correlation between $x_1$ and $x_1&#94;2$ . The same idea goes between $x_2$ and $x_2x_3$ . For demonstraction, I use real-life data: rock properties and natural gas production. Please refer to Section 0: Sample data description of my previous post for more information of this dataset. I also use VIF to quantify the degree of multicollinearity (refer to Section 4.1: VIF below if you are not familiar with VIF). I added interaction terms for features Por , Perm , and AI . In figure (7) , we observe massive increase in VIF for features with their interaction term counterparts. Considering VIF bigger than 10 is considered very severe multicollinearity, this is unacceptable. While mean centering, which I discuss in Section 5.2: Mean centering below , has an effect of reducing high VIF due to interaction terms, it turns out to be useless for real-life applications. Figure 7: Effect of interaction terms on VIF Source Code For Figure (7) import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler ############################################# import data ############################################# # data source: https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v5.csv file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' features = ['Por', 'Perm', 'AI', 'Brittle', 'TOC', 'VR'] df = pd.read_csv(file) df = df[features] ####################################### VIFs with original data ####################################### vifs_orig = np.linalg.inv(df.corr().values).diagonal() df_vif_orig = pd.DataFrame() df_vif_orig['Features'] = df.columns df_vif_orig['VIF'] = vifs_orig ##################################### VIFs with interaction terms ##################################### df['Perm $\\cdot$ AI'] = df['Perm'].values * df['AI'].values df['Por$&#94;2$'] = df['Por'].values ** 2 vifs_inter = np.linalg.inv(df.corr().values).diagonal() df_vif_inter = pd.DataFrame() df_vif_inter['Features'] = df.columns df_vif_inter['VIF'] = vifs_inter print(df_vif_orig) print() print(df_vif_inter) 4. Detection This section discusses two ways to detect multicollinearity with a dataset. You can assess one-to-one relationships of features with correlation matrix. You can assess one-to-many relationships of features with VIF score. VIF Correlation matrix For demonstration, I use real-life data: rock properties and natural gas production. Please refer to Section 0: Sample data description of my previous post for more information of this dataset. 4.1. VIF Variance inflation factor (VIF) represents how much the standard error (SE) of an estimated regression coefficient is increased due to multicollinearity. High value of VIF is undesirable, because it is interpreted as high instability of the value of the regression coefficient. In figure (2) , high SE coefficients are shown as red dots, and the low SE coefficients are shown as black dots. Eq (7) shows the mathematical components that contribute to the instability (SE) of regression coefficients (source: Wikipedia ). Note that the symbol $\\widehat{ }$ means \"estimated value\". $$ \\widehat{SE}&#94;2 (\\hat{\\beta_j}) = \\widehat{var} (\\hat{\\beta_j}) = \\frac{s&#94;2}{(n-1)\\widehat{var}(X_j)} \\cdot VIF (\\hat{\\beta_j}) \\tag{7}$$ where $\\widehat{SE}$ : Estimated standard error $\\widehat{var}$ : Estimated variance $\\hat{\\beta_j}$ : Estimated regression coefficient of the $j$-th independent variable $s&#94;2$ : Mean squared error. Represents the scatter of data $n$ : Sample size $X_j$ : $j$-th independent variable $VIF$ : Variance inflation factor The interpretation of eq (7) is actually very straightforward. Big scatter of data $s&#94;2$ increases SE. On the other hand, obtaining more number of samples $n$ or increasing $\\widehat{var} (\\hat{X_j})$ reduces SE. This intuitively makes sense because the power of statistical estimation increases with greater sample size and variability of samples â€” your model can be trained for more possible number of scenarios. As a result, you are more confident with the values of your regression coefficients, reducing SE. The remaining source that contributes to the instability of coefficients in eq (7) is the degree of multicollinearity, represented as $VIF (\\hat{\\beta_j})$ . It is computed with the following equation: $$ VIF (\\hat{\\beta_j}) = \\frac{1}{1 - R&#94;2_j} \\tag{8}$$ $R&#94;2_j$ is calculated by taking a j-th feature, and regressing it against every other features in the model. This is very similar to the traditional R-squared that most people are familiar with; high R-squared means that a response variable can be well-predicted from its features. In terms of multicollinearity, high $R&#94;2_j$ $\\longleftrightarrow$ high VIF is undesirable, as it means that a feature can be linearly predicted from the other features. Rules of thumb for VIF $VIF (\\hat{\\beta_j}) = 1$ : A feature $X_j$ is orthogonal to all other features. It exibits no sign of multicollinearity. $1 < VIF (\\hat{\\beta_j}) < 5$ : There is some degree of multicollinearity, but it is not significantly high. $5 < VIF (\\hat{\\beta_j}) < 10$ : The degee of multicollinearity is high enough to cause troubles. $10 < VIF (\\hat{\\beta_j})$ : Multicollineartiy does cause problems. Remedial procedures may be needed. Notes: Calculation of VIF assumes a certain structure of a model VIF just shows to what extent a feature can be predicted from the other features by its use of R-squared. While VIF is traditionally understood as a tool to assess how much a feature can be \"linearly\" predicted from the other features, the relationship doesn't have to be strictly linear. Depending on your choice design for VIF calculation, it can assess the degree of nonlinear correlation among features using non-linear models, such as polynomial regression, random forest, neural network, and many more. In Python implementation from scratch below , I forced the VIF to measure the degree of linear relationships among features by using sklearn.linear_model.LinearRegression . If I want it to capture nonlinear relationships, I can force other models, such as sklearn.tree.DecisionTreeClassifier . Note that most softwares assess linear relationships among features with VIF. This, on the other hand, means that VIF in those softwares does not capture non-linear relationships among features. Pythonic Tip: Three ways to compute VIF in Python This section shows the procedures to calculate VIF in Python. I highly recommend you to check out this stackoverflow thread for detailed discussions of python implementation of VIF. Python implementation from scratch While there exists a library in statsmodels that calculates VIF, I thought it would help your understanding of VIF by showing the code implementation from scratch. exog stands for exogenous (independent) variables. Note that I assigned target = 'prod' even if its never used. I wrote it to emphasize the distinction between y and prod . y temporarily assumes a current feature to be a target variable to compute the feature's VIF. 'prod' is a target variable that we ultimately want to predict with our linear model at the end, but it is not needed to calculated VIF. In [4]: import numpy as np import pandas as pd from sklearn.linear_model import LinearRegression # data source: https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v5.csv file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) features = [ 'Por' , 'Perm' , 'AI' , 'Brittle' , 'TOC' , 'VR' ] target = 'Prod' vifs = [] for feature in features : exog = feature not_exog = [ item for item in features if item is not exog ] X = df [ not_exog ] y = df [ exog ] r_squared_j = LinearRegression () . fit ( X , y ) . score ( X , y ) vif = 1 / ( 1 - r_squared_j ) vifs . append ( vif ) df_vif = pd . DataFrame () df_vif [ 'Features' ] = features df_vif [ 'VIF' ] = vifs In [5]: df_vif Out[5]: Features VIF 0 Por 4.329838 1 Perm 2.862487 2 AI 7.660318 3 Brittle 2.165476 4 TOC 7.365773 5 VR 7.057671 We observe significant degree of multicollinearity, as VIFs are bigger than 5 for AI , TOC , and VR . This explains the cause of instability of coefficients in figure (8) . Statsmodel's variance_inflation_factor VIF is implemented in statsmodels with statsmodels.stats.outliers_influence.variance_inflation_factor . Two things to note here. First, as I mentioned in Notes: Calculation of VIF assumes a certain structure of a model above , VIF implemented in most softwares assume linear relationships among features. While VIF function of statsmodels saves a few lines of code, if you suspect the presence of non-linearity among features, you should design your own code that assumes a non-linear model. Second, a constant must be added to the design matrix before calculating VIF. This is a very common & big mistake. Not adding a constant term results in VIF function assuming a linear model without y-intercept. \"Generally it is essential to include the constant in a regression model\", because \"the constant (y-intercept) absorbs the bias for the regression model\", as Jim Frost says in his post . The stackoverflow answer by Alexander suggests two ways to add a constant term. Failing to add the constant term has an effect of making the VIFs of features abnormally high. Correct In [14]: from statsmodels.stats.outliers_influence import variance_inflation_factor from statsmodels.tools.tools import add_constant import numpy as np import pandas as pd # data source: https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v5.csv file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' features = [ 'Por' , 'Perm' , 'AI' , 'Brittle' , 'TOC' , 'VR' ] df = pd . read_csv ( file ) df = df [ features ] df = add_constant ( df ) # alternatively, df = df.assign(const=1) vifs = [ variance_inflation_factor ( df . values , i ) for i in range ( df . shape [ 1 ])] df_vif = pd . DataFrame () df_vif [ 'Features' ] = df . columns df_vif [ 'VIF' ] = vifs In [15]: df_vif Out[15]: Features VIF 0 const 161.797377 1 Por 4.329838 2 Perm 2.862487 3 AI 7.660318 4 Brittle 2.165476 5 TOC 7.365773 6 VR 7.057671 Incorrect In [9]: from statsmodels.stats.outliers_influence import variance_inflation_factor import numpy as np import pandas as pd # data source: https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v5.csv file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' features = [ 'Por' , 'Perm' , 'AI' , 'Brittle' , 'TOC' , 'VR' ] df = pd . read_csv ( file ) df = df [ features ] vifs = [ variance_inflation_factor ( df . values , i ) for i in range ( df . shape [ 1 ])] df_vif = pd . DataFrame () df_vif [ 'Features' ] = df . columns df_vif [ 'VIF' ] = vifs In [10]: df_vif Out[10]: Features VIF 0 Por 62.645242 1 Perm 16.887354 2 AI 122.734765 3 Brittle 19.370696 4 TOC 33.668133 5 VR 259.597778 Linear algebra implementation VIFs can also be computed with linear algebra taking the inverse of a correlation matrix. One problem with this method is that sometimes the data's numerical instability can cause the correlation matrix be not invertible. This is usually the case when the non-diagonal elements of a correlation matrix approximates zero (this means that the data shows no sign of pairwise collinearity) with values like 1.1e-16, -1.2e-17, or 1.3e-16. Such small decimal values cause floating point errors with Python. This problem can be avoided by replacing near-zero values with zero. However, this is the shortest and simplest way to compute VIFs , since you don't have to worry about adding a constant term, which is required if you were to compute VIF with statsmodels. However, just like the statsmodels variance_inflation_factor , it does not capture non-linear relationships among feature. This is because the correlation matrix df.corr() captures only pairwise linearity between two features. Note that it assumes Pearson's correlation matrix, according to the Pandas documentation . In [7]: import numpy as np import pandas as pd # data source: https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v5.csv file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' features = [ 'Por' , 'Perm' , 'AI' , 'Brittle' , 'TOC' , 'VR' ] df = pd . read_csv ( file ) df = df [ features ] vifs = np . linalg . inv ( df . corr () . values ) . diagonal () df_vif = pd . DataFrame () df_vif [ 'Features' ] = df . columns df_vif [ 'VIF' ] = vifs In [8]: df_vif Out[8]: Features VIF 0 Por 4.329838 1 Perm 2.862487 2 AI 7.660318 3 Brittle 2.165476 4 TOC 7.365773 5 VR 7.057671 WARNING! VIF is sensitive to outliers/influential poins . This is because VIF is usually computed with multiple linear regression or Pearson's correlation matrix, which are techniques known to be sensitive to outliers/influential points. If you suspect presense of outliers, you must either 1) remove outliers, or 2) use outlier-robust techniques, such as robust regression or Kendall's rank correlation matrix. 4.2. Correlation matrix One of the remedial procedures for multicollinearity is to drop collinear features, which I discuss in Section 5.1: Elimination below . This method requires knowing which features are linearly correlated with each other. While VIF checks for the presence of overall multicollinearity among features, it does not tell you which features are linearly correlated. This is where correlation matrix comes in. A correlation matrix is a square table of correlation coefficients. Each elements in the table measures a degree of linear relationship between two variables. The values range between -1 ~ 1. A negative value means negative relationship (increasing $X_1$ decreases $X_2$ ), and a positive value means positive relationship (increasing $X_1$ increases $X_2$ ). We usually don't care about the sign of correlation coefficients; we care about the absolute magnitidue of the correlation coefficients. The absolute value of a correlation coefficient close to 1 means strong linearity, and a value close to 0 means random scatter of two features with no linearity. Note that the diagonal elements of a correlation matrix always have a value of 1. This is because we are comparing, for instance, $X_1$ against $X_1$ itself, which are identical. Check figure (8) for bivariate scatter plots and their respective correlation coefficient. We see considerable degree of linearity between $X_3 \\leftrightarrow X_5$ . If you choose to drop collinear features to remedy the effects of multicollinearty, you may consider dropping one of them, as they show large degree of linearity with each other. But beaware, dropping a feature results in loss of overall prediction power of a model. Make sure that the feature you are dropping does not add much prediction power to a model with feature ranking method, such as permutation feature ranking shown in figure (?????????????????????) . Figure 8: Bivariate pairplots and their correlation coefficients Source Code For Figure (8) import matplotlib.pyplot as plt import pandas as pd import seaborn as sns df = pd.read_csv('https://aegis4048.github.io/downloads/notebooks/sample_data/collinear.csv') corr = df.corr(method='kendall').values g = sns.pairplot(df, diag_kind=\"kde\", plot_kws=dict(s=50, linewidth=1, alpha=0.3, color='k', edgecolor='grey'), diag_kws=dict(shade=True, color='grey'), height=2, aspect=1.6) fontsize = 22 for i in range(len(df.columns)): for j in range(len(df.columns)): if i == 0: g.axes[j,i].yaxis.set_label_text(df.columns[j], fontsize=fontsize) if j == len(df.columns) - 1: g.axes[j,i].xaxis.set_label_text(df.columns[i], fontsize=fontsize) g.axes[j, i].text(0.05, 0.8, r'$\\tau$ = %.2f' % corr[j, i], fontsize=15, transform=g.axes[j, i].transAxes) g.axes[j, i].grid(True, linestyle='--', color='#acacac') g.axes[j, i].text(0.25, 0.1, 'aegis4048.github.io', fontsize=8, ha='center', va='center', transform=g.axes[j, i].transAxes, color='grey', alpha=0.5) g.fig.suptitle('Kendall\\'s rank correlation coefficients', fontsize=30) g.fig.tight_layout(rect=[0, 0, 1, 0.94]) A correlation matrix is often visualized in heatmaps, as shown in figure (9) . Note that all correlation matrices are symmetric with respect to their diagonal elements, and that one side of a diagonal is a duplicate of the other side. For this reason, half of a correlation matrix is often masked like this . Figure 9: Heatmap visualization of a correlation matrix Source Code For Figure (9) import matplotlib.pyplot as plt import pandas as pd import seaborn as sns df = pd.read_csv('https://aegis4048.github.io/downloads/notebooks/sample_data/collinear.csv') corr = df.corr(method='kendall').values fig, ax = plt.subplots(figsize=(6, 5)) cmap = sns.diverging_palette(220, 10, as_cmap=True, sep=100) sns.heatmap(corr, cmap=cmap, vmin=-1, vmax=1, center=0, linewidths=.5) fig.suptitle('Kendall\\'s correlation coefficient matrix', fontsize=15) ax.set_xticklabels(list(df.columns)) ax.set_yticklabels(list(df.columns)) fig.tight_layout(rect=[0, 0, 1, 0.94]) WARNING! Do not use correlation matrix as a measure of multicollinearity! Correlation matrix captures pairwise correlations. In the other words, it only captures one-to-one collinearity. It fails to capture one-to-many multicollinearity. A bunch of pairwise low to medium correlations is not a sufficient test for lack of multicollinearity. Use correlation matrix only to detect which features are related to which features. For the test of overall multicollinearity, use VIF instead, which I discuss in Section 4.1: VIF above . I briefly introduce four types of correlation matrices. Note that my intention is to familiarize you with using correlation matrices for the purpose of detecting collinear features. While I highlight some key takeaways, I gloss over some of the details of each types. I recommend you to search more about correlation coefficient on your own. Covariance Pearson's correlation coefficient Spearman's rank correlation coefficient Kendall's rank correlation coefficient Covariance A covariance tells how a feature varies with an another feature. In the matrix diagonal, there are variances, i.e., the covariance of each feature with itself. Unlike correlation coefficient, covariance DOES NOT have a range between -1 ~ 1. Each element in a covariance matrix has a different range of values, depending on the range of two features; a feature ranging between 450 ~ 1150 has much larger variance than a feature ranging between 0.1 ~ 0.9. For this reason, a covariance matrix should not be used to evaluate collinearity . But I still introduced a covariance matrix here for the sake of a cautionary purpose. In [59]: import pandas as pd import matplotlib.pyplot as plt df = pd . read_csv ( 'https://aegis4048.github.io/downloads/notebooks/sample_data/collinear.csv' ) df . cov () Out[59]: $X_1$ $X_2$ $X_3$ $X_4$ $X_5$ $X_1$ 1.248995 -0.347574 0.806363 0.922609 0.418243 $X_2$ -0.347574 1.348896 0.542184 -0.538883 0.517330 $X_3$ 0.806363 0.542184 1.455418 0.260982 1.245386 $X_4$ 0.922609 -0.538883 0.260982 1.493672 -0.035511 $X_5$ 0.418243 0.517330 1.245386 -0.035511 1.647068 Pearson's correlation coefficient Pearson's correlation coefficient is essentially a covariance matrix computed from standardized features. All correlation coefficients have the same range of values, spanning -1 ~ 1. However, Pearson's correlation coefficient is sensitive to influential points/outliers and skewness that undermine linearity of data. A single outlier from a tight linear cluster of 1000 observations has a significant impact on the calculated coefficient value. This is because Pearson's correlation coefficient measures strict linearity of ALL observations. It is recommended to stick with either Spearman's or Kendall's rank correlation coefficients. In [56]: import pandas as pd import matplotlib.pyplot as plt df = pd . read_csv ( 'https://aegis4048.github.io/downloads/notebooks/sample_data/collinear.csv' ) df . corr ( method = 'pearson' ) Out[56]: $X_1$ $X_2$ $X_3$ $X_4$ $X_5$ $X_1$ 1.000000 -0.267780 0.598076 0.675476 0.291603 $X_2$ -0.267780 1.000000 0.386958 -0.379645 0.347075 $X_3$ 0.598076 0.386958 1.000000 0.177007 0.804367 $X_4$ 0.675476 -0.379645 0.177007 1.000000 -0.022640 $X_5$ 0.291603 0.347075 0.804367 -0.022640 1.000000 Spearman's rank correlation coefficient Spearman's rank correlation coefficient is essentially a superior version of Pearson's correlation coefficient. It measure monotonic linearity between two features. It is robust to outliers because it uses rank transformation (I highly recommend you to watch this three minute video ). When the data are ranked, an outlier will simply be recognized as a case that is ranked one above (or below) the next less extreme case. Regardless of whether there is .01 or 5 standard deviations between the most and second most extreme value, that degree of difference is thrown away when data are ranked. In [57]: import pandas as pd import matplotlib.pyplot as plt df = pd . read_csv ( 'https://aegis4048.github.io/downloads/notebooks/sample_data/collinear.csv' ) df . corr ( method = 'spearman' ) Out[57]: $X_1$ $X_2$ $X_3$ $X_4$ $X_5$ $X_1$ 1.000000 -0.242974 0.603061 0.643429 0.306735 $X_2$ -0.242974 1.000000 0.387898 -0.371947 0.344661 $X_3$ 0.603061 0.387898 1.000000 0.170836 0.787571 $X_4$ 0.643429 -0.371947 0.170836 1.000000 -0.001172 $X_5$ 0.306735 0.344661 0.787571 -0.001172 1.000000 Kendall's rank correlation coefficient Kendall's rank correlation coefficient also uses rank transformation. Just like Spearman's Rho, Kendall's Tau is free from no-outlier & normality assumptions. However, researches shown that Kendall's Tau is superior to Spearman's Rho (source: here and here ), as Kendall's Tau has smaller confidence intervals than Spearman's Rho. In the other words, Kendall's Tau is more precise than Spearman's Rho. However, Kendall's Tau has higher algorithm complexity than Spearman's Rho, which is not a big issue with the 21st century's computing power. Note that Kendall's Tau tends to be smaller than Spearman's Rho. In [58]: import pandas as pd import matplotlib.pyplot as plt df = pd . read_csv ( 'https://aegis4048.github.io/downloads/notebooks/sample_data/collinear.csv' ) df . corr ( method = 'kendall' ) Out[58]: $X_1$ $X_2$ $X_3$ $X_4$ $X_5$ $X_1$ 1.000000 -0.160984 0.418345 0.463803 0.204653 $X_2$ -0.160984 1.000000 0.260671 -0.259418 0.233468 $X_3$ 0.418345 0.260671 1.000000 0.115884 0.599463 $X_4$ 0.463803 -0.259418 0.115884 1.000000 0.001342 $X_5$ 0.204653 0.233468 0.599463 0.001342 1.000000 Notes: Partial correlation matrix There is also another tool that is useful for detecting multicollinearity - partial correlation matrix. I tried to explain it myself, but concluded that I can't conceptually explain it better than Wikipedia. The following description is from Wikipedia : \"If we are interested in finding whether or to what extent there is a numerical relationship between two variables of interest, using their correlation coefficient will give misleading results if there is another, confounding, variable that is numerically related to both variables of interest. This misleading information can be avoided by controlling for the confounding variable, which is done by computing the partial correlation coefficient.\" The concept & code snippets for partial correlation is a bit advanced and lengthy. As such, I won't be covering it in this post, but I still wanted to introduce you the presence of this technique. It is implemented in Pingouin . It is the only library I found that allows you to control variables individually while fitting for partial correlation of two variables. 5. Remedies This section discusses four remedial procedures to address multicollinearity. I recommend to try the following remedial procedures in order: increase sample size, mean centering, elimination, and principle component regression (PCR). In the other words, if one remedial procedure doesn't work, try the next one. Note that increasing sample size and mean centering do not have any negative consequences, but elimination and PCR do. Elimination suffers loss in prediction power of a model, and PCR undermines the interpretability of a model. Increase sample size Mean centering Elimination Principle component regression what you are looking for are ways to increase precision 5.1 Increase sample size Multicollinearity makes the values of regression coefficients unstable. In the other words, it increases the width of confidence intervals of coefficients, as shown in figure (3) . The effect of multicollinearity can be reduced by increasing sample size, because increasing sample size has an effect of decreasing standard errors of confidence intervals, which I explain in Notes: sample size and width of confidence interval below . I demonstrate it with numerical simulations. For the simulations, we fit many multiple linear regression models with 2 features and an intercept. We generate two high-multicollinearity synthetic data sets with the procedures described in Section 0. Sample data description above , for sample sizes n = 80 and n = 800 . We set true_coefs = [13, 0.5] and true_intercept = 2 , and add Gaussian noise to the model. We fit m = 2000 different models for each data set. This essentially mimicks drawing 2,000 samples, each of size 800, from a population described by the true model $y = 13 x_1 + 0.5 x_2 + 2$ with Gaussian noise. This is a type of Monte-Carlo method , in which the simulation relies on repeated generation of random numbers to investigate some characteristic of a statistic which is hard to derive analytically. Figure (10) shows the simulation result that highlights the effect of sample sizes on the uncertainty of the values of regression coefficients. We observe that models trained with bigger sample size n = 800 has narrower range of values (red dots) than the models trained with smaller sample size n = 80 (black dots). Figure 10: Effect of sample size on multicollinearity Source Code For Figure (10) import numpy as np from sklearn import linear_model import matplotlib.pyplot as plt def generate_collinear_data(cov, n=10, true_coefs=[0, 0], true_intercept=0, feature_means=[0, 0, 0], loc=0, scale=1): # random generation of 3D gaussian collinear features. X = np.random.multivariate_normal(mean=feature_means, cov=cov, size=n) # generate gaussian white noise. gaussian_noise = np.random.normal(loc=loc, scale=scale, size=n) # make the outcome. y = true_intercept + gaussian_noise for i in range(len(true_coefs)): y += true_coefs[i] * X[:, i] return X, y # settings m = 2000 # number of simulations correlation = 0.98 # degree of collinearity (HIGH) size_lo = 80 size_hi = 800 kwargs = { 'true_coefs': [13, 0.5], # linear regression coefficients, 2 features 'true_intercept': 2, # y-intercept 'feature_means': [0, 0], # means multivariate normal distribution. This is not important 'loc': 0, # mean of gaussian noise 'scale': 1 # standard deviation of gaussian noise } # high collinearity covariance matrix cov = np.full((len(kwargs['true_coefs']), len(kwargs['true_coefs'])), correlation) np.fill_diagonal(cov, 1) # simulation model_hi_size_coefs = [] model_lo_size_coefs = [] for i in range(m): X_hi, y_hi = generate_collinear_data(cov, n=size_hi, **kwargs) ols_hi = linear_model.LinearRegression() model_hi = ols_hi.fit(X_hi, y_hi) model_hi_size_coefs.append(model_hi.coef_) X_lo, y_lo = generate_collinear_data(cov, n=size_lo, **kwargs) ols_lo = linear_model.LinearRegression() model_lo = ols_lo.fit(X_lo, y_lo) model_lo_size_coefs.append(model_lo.coef_) # list to numpy array conversion model_hi_size_coefs = np.array(model_hi_size_coefs) model_lo_size_coefs = np.array(model_lo_size_coefs) # plotting plt.style.use('default') plt.style.use('ggplot') fig, ax = plt.subplots(figsize=(8, 4.5)) ax.scatter(model_lo_size_coefs[:, 0], model_lo_size_coefs[:, 1], s=10, label='High multicollinearity, n = %.f' % size_lo, edgecolor='dimgrey', facecolor='k', alpha=0.7) ax.scatter(model_hi_size_coefs[:, 0], model_hi_size_coefs[:, 1], s=10, label='High multicollinearity, n = %.f' % size_hi, edgecolor='red', facecolor='firebrick', alpha=0.7) ax.set_xlim(10.5, 15.5) ax.set_ylim(-2, 3) ax.set_xlabel('$x_1$ coefficient values', fontsize=16) ax.set_ylabel('$x_2$ coefficient values', fontsize=16) ax.set_title('Effect of sample size on multicollinearity', fontsize=20) ax.legend(facecolor='white', fontsize=11) ax.text(0.2, 0.1, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) fig.tight_layout() The above scatter plot shows the effect of multicollinearity for linear models with two features. How does multicollinearity affect regression coefficients when there are more than two features? In figure (11) , I simulated 10,000 linear models with five features and an intercept: 'true_coefs': [13, 0.5, 5, -1, -24] , and 'true_intercept': 2 . Similar to figure (10) , linear models trained from bigger sample size showed narrower range of coefficients than those from smaller sample size. In statistics, narrow range of model coefficients are ideal, as it is seen as a stable (good) model. Figure 11: Effect of sample size on multicollinearity, 5 features Source Code For Figure (11) import numpy as np import statsmodels.api as sm import matplotlib.pyplot as plt def generate_collinear_data(cov, n=10, true_coefs=[0, 0], true_intercept=0, feature_means=[0, 0, 0], loc=0, scale=1): # random generation of 3D gaussian collinear features. X = np.random.multivariate_normal(mean=feature_means, cov=cov, size=n) # generate gaussian white noise. gaussian_noise = np.random.normal(loc=loc, scale=scale, size=n) # make the outcome. y = true_intercept + gaussian_noise for i in range(len(true_coefs)): y += true_coefs[i] * X[:, i] return X, y # settings m = 10000 # number of simulations correlation = 0.98 # degree of collinearity (HIGH) size_lo = 80 size_hi = 800 kwargs = { 'true_coefs': [13, 0.5, 5, -1, -24], # linear regression coefficients, 2 features 'true_intercept': 2, # y-intercept 'feature_means': [0, 0, 0, 0, 0], # means multivariate normal distribution. This is not important 'loc': 0, # mean of gaussian noise 'scale': 1 # standard deviation of gaussian noise } # high collinearity covariance matrix cov = np.full((len(kwargs['true_coefs']), len(kwargs['true_coefs'])), correlation) np.fill_diagonal(cov, 1) # simulation size_hi_params = [] size_lo_params = [] for i in range(m): # high collinearity data X_hi, y_hi = generate_collinear_data(cov, n=size_hi, **kwargs) X_st_hi = sm.add_constant(X_hi) model_hi = sm.OLS(y_hi, X_st_hi).fit() size_hi_params.append(model_hi.params) # low collinearity data X_lo, y_lo = generate_collinear_data(cov, n=size_lo, **kwargs) X_st_lo = sm.add_constant(X_lo) model_lo = sm.OLS(y_lo, X_st_lo).fit() size_lo_params.append(model_lo.params) # list to numpy conversion size_hi_params = np.asarray(size_hi_params) size_lo_params = np.asarray(size_lo_params) # plotting def styling(ax): ax.set_facecolor('#eeeeee') ax.grid(True, linestyle='--', color='#acacac') ax.tick_params(color='grey') _ = [spine.set_edgecolor('grey') for spine in ax.spines.values()] ax.text(0.5, 0.1, 'aegis4048.github.io', fontsize=12, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) boxplot_styling = { 'sym': '', 'whis': [2.5, 97.5], 'showfliers': False, 'boxprops': dict(linewidth=2.0, color='#4e98c3'), 'whiskerprops': dict(linewidth=2.0, color='#4e98c3', linestyle='--'), 'vert': True, 'capprops': dict(linewidth=2.0, color='k'), 'medianprops': dict(linewidth=2.0, color='#ad203e'), 'widths': (0.4, 0.4) } labels = ['Intercept', '$X_1$', '$X_2$', '$X_3$', '$X_4$', '$X_5$'] fig, axes = plt.subplots(1, len(labels), figsize=(16, 6)) for i, (ax, label) in enumerate(zip(axes, labels)): ax.boxplot([list(size_hi_params[:, i]), list(size_lo_params[:, i])], **boxplot_styling) styling(ax) ax.set_title(label, fontsize=20, y=-0.2) ax.set_xticklabels(['n = %.d' % size_hi, 'n = %.d' % size_lo], fontsize=15) fig.tight_layout(rect=[0, 0.05, 1, 0.91]) fig.suptitle('Range of coefficients with big & small sample sizes - %.f simulations' % m, fontsize=25); Notes: Sample size and width of confidence interval A 95% confidence interval means that you are 95% confident that the true parameter value lies within the interval. The width of this interval gets narrower with increasing sample size. This is intuitive in a sense that, the more samples we have, the less uncertainty we have with our statistical estimation. A 95% confidence interval of a normally distributed statistic $x$ can be described by the following general equation: $$CI_{0.95}&#94;{x} = \\hat{x} \\pm 1.96 \\times SE(\\hat{x}) $$ where $SE(\\hat{x})$ is the standard error of the statistic of interest $x$ . Standard errors have different formulas for different statistics of interest. For mean $\\mu$ : $$SE(\\hat{\\mu}) = \\frac{\\hat{d}}{\\sqrt{n}} $$ where $\\hat{d}$ is sample standard deviation, and $n$ is sample size. For proportion $p$ : $$SE(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}} $$ For j-th multiple regression coefficient $\\beta_j$ : $$SE(\\hat{\\beta_j}) = \\sqrt{\\frac{s&#94;2}{(n-1)\\widehat{var}(X_j)} \\cdot VIF (\\hat{\\beta_j})}$$ which I described in eq (7) above. In all three equations for standard erros, we see one thing in common: sample size $n$ is always in the denominator. Increasing sample size decreases standard errors, which in turn reduces the width of confidence intervals. This is a mathematical explanation as to why it is better to have large number of samples. 5.2 Mean centering WARNING! It is not recommended to use mean centering to deal with multicollinearity, unless numerical stability (=floating point round off error) is a big concern. Quoting EdM from stats.stackexchange.com , \"Back in the dark ages when people did statistical calculations by hand on mechanical (not electronic) calculators having limited precision, there might have been some practical advantages to centering first. But modern computing hardware and software make that unnecessary.\" Frank Harrel also commented that \"I almost never use centering, finding it completely unncessary and confusing.\" However, you might consider centering if you want to improve the interpretation of y-intercept. And yet I introduce mean centering here, for the purpose of \"myth-buster\" , because many articles online introduce mean centering as if it's a working solution for multicollinearity, when it is not. Mean centering is known to reduce VIF (measure of multicollinearity, discussed in Section 4.1: VIF above ) in the presence of interaction terms (which causes structural multicollinearity, discussed in Section 3.3: Structural multicollinearity with interaction terms above ). In the other words, mean centering is irrelavant unless you add interaction terms . With this in mind, I would like to touch on four key points here. Mean centering changes the interpretation of coefficients and y-intercept Mean centering subtracts each independent variable's mean from itself. Assume the following arbitrary linear model with three features and an interaction term: $$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_2 x_3 \\tag{9}$$ Applying mean centering, the model becomes: $$ y = \\beta_0&#94;* + \\beta_1&#94;* (x_1 - \\bar{x_1}) + \\beta_2&#94;* (x_2 - \\bar{x_2}) + \\beta_3&#94;* (x_3 - \\bar{x_3}) + \\beta_4&#94;* (x_2 - \\bar{x_2}) (x_3 - \\bar{x_3}) \\tag{10}$$ where $\\bar{x_i}$ is the mean of the i-th feature, and $\\beta_i&#94;*$ is the new i-th coefficient of the mean-centered model. Rearranging eq (10) , it can be algebracially shown that mean centering changes the values of the coefficients: $$ \\begin{align} \\beta_0&#94;* &= \\beta_0 - \\beta_1\\bar{x_1} - \\beta_2\\bar{x_2} - \\beta_3\\bar{x_3} - \\beta_4\\bar{x_2}\\bar{x_3} \\label{}\\tag{11}\\\\ \\beta_1&#94;* &= \\beta_1 \\label{}\\tag{12}\\\\ \\beta_2&#94;* &= \\beta_2 - \\beta_4 \\bar{x_3} \\label{}\\tag{13}\\\\ \\beta_3&#94;* &= \\beta_3 - \\beta_4 \\bar{x_2} \\label{}\\tag{14}\\\\ \\beta_4&#94;* &= \\beta_4 \\label{}\\tag{15}\\\\ \\end{align} $$ From the above derivation of the mean-centered coefficients, we observe that centering: changes the intercept value ( $\\beta_0&#94;*$ ) changes the coefficient values of the features ( $x_2$ and $x_3$ ) involved with the interaction term ( $x_2 x_3$ ) does not change the coefficient value of the feature ( $x_1$ ) not involved with the interaction term ( $x_2 x_3$ ) does not change the coefficient value of the interaction term ( $x_2 x_3$ ) These observations hold true for all mean-centered models with interaction terms. While centering complicates the interpretation of the coefficients, it improves the interpretation of the y-intercept. Consider a person's height as a dependent variable $y$ and weight as an independent variable $x$ . Before centering, the y-intercept $\\beta_0$ is the height of a person with weight 0, which doesn't make sense. After centering, $\\beta_0$ is the height of a person with average weight, because $x - \\bar{x} = 0$ . In this case, the numerical value of $\\beta_0$ has a real world interpretation. This makes the interpretation of the intercept to be the expected values of $y$ when the features are set to their sample means. Mean centering reduces structural multicollinearty, but does not reduce data multicollinearity Mean centering is known to lower correlation between interaction terms and independent variables involved with them. On the other hand, it does not affect independent variables that are not used to construct the interaction terms. In figure (12) , we observe that the correlations among independent variables are unaffected, while the correlations between interaction terms and indepedent variables are reduced. We observe similar phenomenon with VIFs too, as shown in figure (13) . This tells us that mean centering can only remove multicollinearity caused by one's choice of model, but unable to cure the inherent multicollinearity present in the data itself. However, this reduction in VIF and correlation is misleading in a sense that it makes practitioners to believe that mean centering has actual practical remedial effects on multicollinearity, when it doesn't. I show this in the next two key points below. Figure 12: Effect of mean centering with interaction terms on correlation matrix Source Code For Figure (12) import matplotlib.pyplot as plt import pandas as pd import seaborn as sns import numpy as np import statsmodels.api as sm from sklearn.preprocessing import StandardScaler ############################################# create collinear data ############################################# np.random.seed(1) def generate_collinear_data(cov, n=10, true_coefs=[0, 0], true_intercept=0, feature_means=[0, 0, 0], loc=0, scale=1): # random generation of 3D gaussian collinear features. X = np.random.multivariate_normal(mean=feature_means, cov=cov, size=n) # generate gaussian white noise. gaussian_noise = np.random.normal(loc=loc, scale=scale, size=n) # make the outcome. y = true_intercept + gaussian_noise for i in range(len(true_coefs)): y += true_coefs[i] * X[:, i] return X, y # settings correlation = 0.3 # degree of collinearity (HIGH) kwargs = { 'n': 1000, 'true_coefs': [13, 0.5, 5], # linear regression coefficients, 2 features 'true_intercept': 2, # y-intercept 'feature_means': [15, 1, 8], # means multivariate normal distribution. This is not important 'loc': 0, # mean of gaussian noise 'scale': 1 # standard deviation of gaussian noise } # high collinearity covariance matrix cov = np.full((len(kwargs['true_coefs']), len(kwargs['true_coefs'])), correlation) np.fill_diagonal(cov, 1) X, y = generate_collinear_data(cov, **kwargs) ############################################ Add interaction terms ############################################### features = ['$X_1$', '$X_2$', '$X_3$'] # original df_org = pd.DataFrame(X, columns=features) df_org['$X_2 \\cdot X_3$'] = df_org['$X_2$'].values * df_org['$X_3$'].values corr_org = df_org.corr(method='kendall').values # mean-centered scaler = StandardScaler(with_mean=True, with_std=False) X_cnt = scaler.fit_transform(X) df_cnt = pd.DataFrame(X_cnt, columns=features) df_cnt['$X_2 \\cdot X_3$'] = df_cnt['$X_2$'].values * df_cnt['$X_3$'].values # correlation coefficient matrix corr_cnt = df_cnt.corr(method='kendall').values ###################################################### plot ###################################################### fig, axes = plt.subplots(1, 2, figsize=(11.5, 5)) cmap = sns.diverging_palette(220, 10, as_cmap=True, sep=60) sns.heatmap(corr_org, cmap=cmap, vmin=-1, vmax=1, center=0, linewidths=.5, ax=axes[0], annot=True) sns.heatmap(corr_cnt, cmap=cmap, vmin=-1, vmax=1, center=0, linewidths=.5, ax=axes[1], annot=True) axes[0].set_xticklabels(list(df_org.columns)) axes[0].set_yticklabels(list(df_org.columns)) axes[0].set_title('Original', fontsize=15) axes[0].text(0.3, 0.9, 'aegis4048.github.io', fontsize=12, ha='center', va='center', transform=axes[0].transAxes, color='grey', alpha=0.5) axes[1].set_xticklabels(list(df_cnt.columns)) axes[1].set_yticklabels(list(df_cnt.columns)) axes[1].set_title('Mean-centered', fontsize=15) axes[1].text(0.3, 0.9, 'aegis4048.github.io', fontsize=12, ha='center', va='center', transform=axes[1].transAxes, color='grey', alpha=0.5) axes[1].text(-0.3, 0.43, r'$\\Longrightarrow$', transform=axes[1].transAxes, fontsize=30) fig.suptitle('Kendall\\'s correlation coefficient matrix', fontsize=20) fig.tight_layout(rect=[0, 0, 1, 0.94]) Figure 13: Effect of mean centering with interaction terms on VIFs Source Code For Figure (13) import matplotlib.pyplot as plt import pandas as pd import seaborn as sns import numpy as np import statsmodels.api as sm from sklearn.preprocessing import StandardScaler ############################################# create collinear data ############################################# np.random.seed(1) def generate_collinear_data(cov, n=10, true_coefs=[0, 0], true_intercept=0, feature_means=[0, 0, 0], loc=0, scale=1): # random generation of 3D gaussian collinear features. X = np.random.multivariate_normal(mean=feature_means, cov=cov, size=n) # generate gaussian white noise. gaussian_noise = np.random.normal(loc=loc, scale=scale, size=n) # make the outcome. y = true_intercept + gaussian_noise for i in range(len(true_coefs)): y += true_coefs[i] * X[:, i] return X, y # settings correlation = 0.3 # degree of collinearity (LOW) kwargs = { 'n': 1000, 'true_coefs': [13, 0.5, 5], # linear regression coefficients, 2 features 'true_intercept': 2, # y-intercept 'feature_means': [15, 1, 8], # means multivariate normal distribution. This is not important 'loc': 0, # mean of gaussian noise 'scale': 1 # standard deviation of gaussian noise } # low collinearity covariance matrix cov = np.full((len(kwargs['true_coefs']), len(kwargs['true_coefs'])), correlation) np.fill_diagonal(cov, 1) X, y = generate_collinear_data(cov, **kwargs) ############################################ Add interaction terms ############################################### features = ['$X_1$', '$X_2$', '$X_3$'] # original df_org = pd.DataFrame(X, columns=features) # interaction df_int = pd.DataFrame(X, columns=features) df_int['$X_2 \\cdot X_3$'] = df_int['$X_2$'].values * df_int['$X_3$'].values corr_int = df_int.corr(method='kendall').values # mean-centered scaler = StandardScaler(with_mean=True, with_std=False) X_cnt = scaler.fit_transform(X) df_cnt = pd.DataFrame(X_cnt, columns=features) df_cnt['$X_2 \\cdot X_3$'] = df_cnt['$X_2$'].values * df_cnt['$X_3$'].values ################################################## Commput VIFs ################################################## vifs_org = np.linalg.inv(df_org.corr().values).diagonal() df_vif_org = pd.DataFrame() df_vif_org['Features'] = df_org.columns df_vif_org['VIF'] = vifs_org print(df_vif_org) print() vifs_int = np.linalg.inv(df_int.corr().values).diagonal() df_vif_int = pd.DataFrame() df_vif_int['Features'] = df_int.columns df_vif_int['VIF'] = vifs_int print(df_vif_int) print() vifs_cnt = np.linalg.inv(df_cnt.corr().values).diagonal() df_vif_cnt = pd.DataFrame() df_vif_cnt['Features'] = df_cnt.columns df_vif_cnt['VIF'] = vifs_cnt print(df_vif_cnt) Mean centering does not improve the precision of interaction term's coefficient estimate In general, the purpose of remedial procedures of multicollinearity is to improve the stability of a model. In the other wrods, we want to improve the precision of the true coefficient estimate. While it is true that mean centering reduces the VIF of interaction terms, it turns out that it doesn't reduce the width of the confidence interval of the coefficient estimate for interaction terms, as shown in figure (14) . The figure tells a lot about mean centering in regards to multicollinearity. First , we observe the changed values of y-intercept and coefficients for features involved with the interaction term ( $X_2$ and $X_3$ ). The simulation was run from a population described by the true model $y = 15 + 13 x_1 + 0.5 x_2 + 5 x_3 + 9 x_2 x_3$ with Gaussian noise. However, the coefficient estimates obtained from mean centering deviates from the true coefficient ( red dotted line ), because mean centering alters some mathematical properties of the original model, as described above . While mean centering does not undermine the prediction power of a model, it has an effect of complicating the interpretation of the coefficients. However, if the interpretation of coefficients are not important, it doesn't matter. Second , we see that the coefficient estimates of $X_1$ remain unaffected, because it took no part in constructing the interaction term. This confirms again that mean centering is irrelevant unless you add interaction terms into your model. Third , while we observe reduction in VIF for coefficient estimates of $X_2X_3$ after centering, its confidence interval remains unaffected. This tells us that mean centering does not improve precision of coefficient estimates of interaction terms. Iacobucci, D., Schneider, M.J., Popovich, D.L. et al. (2016) describes this phenomenon as \"Mean centering helps alleviate \"micro\" but not \"macro\" multicollinearity\" . This reduction in VIF, and yet unaffected confidence interval, is the main reason that practitioners often mistakenly think that mean centering helps with multicollinearity, when it doesn't. Fourth , we observe that $X_2$ does not show very high VIF, unlike that of $X_3$ , even if its involved with the interaction term. Also, mean centering didn't really reduces its VIF. This is because $X_2$ is statistically insignificant in the first place. I show this in Notes: Statistical significance in linear regression analysis below . Figure 14: Mean centering does not improve precision of interaction term's coefficient estimate Source Code For Figure (14) import numpy as np import statsmodels.api as sm import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler import pandas as pd def generate_collinear_data(cov, n=10, true_coefs=[0, 0], true_intercept=0, feature_means=[0, 0, 0], loc=0, scale=1, true_coefs_interaction=[0, 0]): # random generation of 3D gaussian collinear features. X = np.random.multivariate_normal(mean=feature_means, cov=cov, size=n) # generate gaussian white noise. gaussian_noise = np.random.normal(loc=loc, scale=scale, size=n) # make the outcome. y = true_intercept + gaussian_noise for i in range(len(true_coefs)): y += true_coefs[i] * X[:, i] y += true_coefs_interaction[0] * X[:, 1] * X[:, 2] return X, y # settings m = 10000 # number of simulations correlation = 0.98 # degree of collinearity (HIGH) kwargs = { 'n': 50, # sample size 'true_coefs': [13, 0.5, 5], # linear regression coefficients, 2 features 'true_intercept': 15, # y-intercept 'feature_means': [-12, 14, 2], # means multivariate normal distribution. This is not important 'loc': 0, # mean of gaussian noise 'scale': 1, # standard deviation of gaussian noise 'true_coefs_interaction': [9] # linear regression coefficients of interaction terms } # high collinearity covariance matrix cov = np.full((len(kwargs['true_coefs']), len(kwargs['true_coefs'])), correlation) np.fill_diagonal(cov, 1) # simulation org_params = [] std_params = [] for i in range(m): ################################################ Original ################################################ np.random.seed(i) # generate high collinearity data X_org, y_org = generate_collinear_data(cov, **kwargs) # add interaction terms interaction_org_1 = X_org[:, 1].reshape(-1, 1) * X_org[:, 2].reshape(-1, 1) # x2 * x3 X_org = np.concatenate([X_org, interaction_org_1], axis=1) # VIF vifs_org = np.linalg.inv(pd.DataFrame(X_org).corr().values).diagonal() # fit linear regression model X_st_org = sm.add_constant(X_org) model_org = sm.OLS(y_org, X_st_org).fit() org_params.append(model_org.params) ############################################## Mean-centered ############################################## np.random.seed(i) # generate high collinearity data X_std, y_std = generate_collinear_data(cov, **kwargs) # mean-center independent variables scaler = StandardScaler(with_std=False) X_std = scaler.fit_transform(X_std) # add interaction terms interaction_std_1 = X_std[:, 1].reshape(-1, 1) * X_std[:, 2].reshape(-1, 1) # x2 * x3 X_std = np.concatenate([X_std, interaction_std_1], axis=1) # VIF vifs_std = np.linalg.inv(pd.DataFrame(X_std).corr().values).diagonal() # fit linear regression model X_st_std = sm.add_constant(X_std) model_std = sm.OLS(y_std, X_st_std).fit() std_params.append(model_std.params) ########################################################################################################## # list to numpy conversion org_params = np.asarray(org_params) std_params = np.asarray(std_params) #################################################### Plot #################################################### # plotting def styling(ax): ax.set_facecolor('#eeeeee') ax.grid(True, linestyle='--', color='#acacac') ax.tick_params(color='grey') _ = [spine.set_edgecolor('grey') for spine in ax.spines.values()] ax.text(0.35, 0.88, 'aegis4048.github.io', fontsize=10.5, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) boxplot_styling = { 'sym': '', 'whis': [2.5, 97.5], 'showfliers': False, 'vert': True, 'capprops': dict(linewidth=2.0, color='k'), 'medianprops': dict(linewidth=2.0, color='k'), 'widths': (0.4), 'patch_artist': True, 'whiskerprops': dict(linewidth=1.5, color='k', linestyle='--'), } labels = ['Intercept', '$X_1$', '$X_2$', '$X_3$', '$X_2X_3$'] true_params = [kwargs['true_intercept']] + kwargs['true_coefs'] + kwargs['true_coefs_interaction'] fig, axes = plt.subplots(1, len(labels), figsize=(16, 6)) for i, (ax, label, true_param) in enumerate(zip(axes, labels, true_params)): bp1 = ax.boxplot(org_params[:, i], positions=[1.0], boxprops=dict(linewidth=1.5, facecolor='#00bfc4'), **boxplot_styling) bp2 = ax.boxplot(std_params[:, i], positions=[1.7], boxprops=dict(linewidth=1.5, facecolor='#f8766d'), **boxplot_styling) styling(ax) ax.set_title(label, fontsize=20, y=-0.2) if i != 0: ax.set_xticklabels(['VIF = {}'.format(int(vifs_org[i - 1])), 'VIF = {}'.format(int(vifs_std[i - 1]))], fontsize=15) else: ax.set_xticklabels([]) ls_handle = ax.axhline(true_param, linestyle='-.', color='#ad203e', label='True value', zorder=9) fig.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0], ls_handle], ['Original', 'Mean-centered', 'True coefficient'], loc='lower center', ncol=3, borderaxespad=0.7, fontsize=17) fig.suptitle('Effect of mean centering for interaction terms - %.f simulations' % m, fontsize=25); fig.tight_layout(rect=[0, 0.1, 1, 0.9]) Mean centering changes the result of linear regression analysis summary report Figure (15) shows the statsmodels summary report about linear regression analysis ran on the true population described by $y = 15 + 13 x_1 + 0.5 x_2 + 5 x_3 + 9 x_2 x_3$ with Gaussian noise. Note that it used the same simulation setting used in Figure (14) above. Three things I want to point out: First , we observe reductions in standard errors std err for independent variables involved with interaction terms ( $x_2$ and $x_3$ ). This is because VIF, which is directly related to standard errors as shown in eq (7) , is reduced after mean centering. However, this has no practical advantage because mean centering changes the interpretation of coefficients. Yes, the standard errors are reduced, but compared to what? Comparing the standad errors of coefficients before & after mean centering is like comparing the size of an apple to an orange. Second , the warning message about the condition number disappeared. This is because mean centering has an effect of improving a matrix's numerical stability. Third, the result of hypothesis testing Figure 15: Effect on mean centering on statsmodel summary Source Code For Figure (15) import numpy as np import statsmodels.api as sm import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler import pandas as pd ############################################# create collinear data ############################################# def generate_collinear_data(cov, n=10, true_coefs=[0, 0], true_intercept=0, feature_means=[0, 0, 0], loc=0, scale=1, true_coefs_interaction=[0, 0]): # random generation of 3D gaussian collinear features. X = np.random.multivariate_normal(mean=feature_means, cov=cov, size=n) # generate gaussian white noise. gaussian_noise = np.random.normal(loc=loc, scale=scale, size=n) # make the outcome. y = true_intercept + gaussian_noise for i in range(len(true_coefs)): y += true_coefs[i] * X[:, i] y += true_coefs_interaction[0] * X[:, 1] * X[:, 2] return X, y # settings m = 10000 # number of simulations correlation = 0.85 # degree of collinearity (HIGH) kwargs = { 'n': 50, # sample size 'true_coefs': [13, 0.5, 5], # linear regression coefficients, 2 features 'true_intercept': 15, # y-intercept 'feature_means': [-12, 14, 2], # means multivariate normal distribution. This is not important 'loc': 0, # mean of gaussian noise 'scale': 1, # standard deviation of gaussian noise 'true_coefs_interaction': [9] # linear regression coefficients of interaction terms } # high collinearity covariance matrix cov = np.full((len(kwargs['true_coefs']), len(kwargs['true_coefs'])), correlation) np.fill_diagonal(cov, 1) seed = np.random.randint(1000) ################################################# Original data ################################################ np.random.seed(563) # generate high collinearity data X_org, y_org = generate_collinear_data(cov, **kwargs) # add interaction terms interaction_org_1 = X_org[:, 1].reshape(-1, 1) * X_org[:, 2].reshape(-1, 1) # x2 * x3 X_org = np.concatenate([X_org, interaction_org_1], axis=1) # VIF vifs_org = np.linalg.inv(pd.DataFrame(X_org).corr().values).diagonal() # fit linear regression model X_st_org = sm.add_constant(X_org) model_org = sm.OLS(y_org, X_st_org).fit() ################################################# Mean-centered ################################################ np.random.seed(563) # generate high collinearity data X_std, y_std = generate_collinear_data(cov, **kwargs) # mean-center independent variables scaler = StandardScaler(with_std=False) X_std = scaler.fit_transform(X_std) # add interaction terms interaction_std_1 = X_std[:, 1].reshape(-1, 1) * X_std[:, 2].reshape(-1, 1) # x2 * x3 X_std = np.concatenate([X_std, interaction_std_1], axis=1) # VIF vifs_std = np.linalg.inv(pd.DataFrame(X_std).corr().values).diagonal() # fit linear regression model X_st_std = sm.add_constant(X_std) model_std = sm.OLS(y_std, X_st_std).fit() ################################################# Print output ################################################# print(model_org.summary()) print() print() print(model_std.summary()) In [9]: # fit linear regression model X_st_s = sm . add_constant ( X_org ) model_org = sm . OLS ( y_org , X_st_org [:, [ 1 , 4 ]]) . fit () model_org . summary () Out[9]: OLS Regression Results Dep. Variable: y R-squared (uncentered): 1.000 Model: OLS Adj. R-squared (uncentered): 1.000 Method: Least Squares F-statistic: 5.190e+05 Date: Wed, 22 Jan 2020 Prob (F-statistic): 9.14e-105 Time: 00:43:41 Log-Likelihood: -89.761 No. Observations: 50 AIC: 183.5 Df Residuals: 48 BIC: 187.3 Df Model: 2 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] x1 11.3754 0.032 355.923 0.000 11.311 11.440 x2 9.4270 0.012 817.851 0.000 9.404 9.450 Omnibus: 0.036 Durbin-Watson: 2.310 Prob(Omnibus): 0.982 Jarque-Bera (JB): 0.205 Skew: -0.038 Prob(JB): 0.902 Kurtosis: 2.696 Cond. No. 5.58 Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. In [ ]: In [1]: import numpy as np import statsmodels.api as sm import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler import pandas as pd ############################################# create collinear data ############################################# def generate_collinear_data ( cov , n = 10 , true_coefs = [ 0 , 0 ], true_intercept = 0 , feature_means = [ 0 , 0 , 0 ], loc = 0 , scale = 1 , true_coefs_interaction = [ 0 , 0 ]): # random generation of 3D gaussian collinear features. X = np . random . multivariate_normal ( mean = feature_means , cov = cov , size = n ) # generate gaussian white noise. gaussian_noise = np . random . normal ( loc = loc , scale = scale , size = n ) # make the outcome. y = true_intercept + gaussian_noise for i in range ( len ( true_coefs )): y += true_coefs [ i ] * X [:, i ] y += true_coefs_interaction [ 0 ] * X [:, 1 ] * X [:, 2 ] return X , y # settings m = 10000 # number of simulations correlation = 0.85 # degree of collinearity (HIGH) kwargs = { 'n' : 50 , # sample size 'true_coefs' : [ 13 , 0.5 , 5 ], # linear regression coefficients, 2 features 'true_intercept' : 15 , # y-intercept 'feature_means' : [ - 12 , 14 , 2 ], # means multivariate normal distribution. This is not important 'loc' : 0 , # mean of gaussian noise 'scale' : 1 , # standard deviation of gaussian noise 'true_coefs_interaction' : [ 9 ] # linear regression coefficients of interaction terms } # high collinearity covariance matrix cov = np . full (( len ( kwargs [ 'true_coefs' ]), len ( kwargs [ 'true_coefs' ])), correlation ) np . fill_diagonal ( cov , 1 ) seed = np . random . randint ( 1000 ) ################################################# Original data ################################################ np . random . seed ( 563 ) # generate high collinearity data X_org , y_org = generate_collinear_data ( cov , ** kwargs ) # add interaction terms interaction_org_1 = X_org [:, 1 ] . reshape ( - 1 , 1 ) * X_org [:, 2 ] . reshape ( - 1 , 1 ) # x2 * x3 X_org = np . concatenate ([ X_org , interaction_org_1 ], axis = 1 ) # VIF vifs_org = np . linalg . inv ( pd . DataFrame ( X_org ) . corr () . values ) . diagonal () # fit linear regression model X_st_org = sm . add_constant ( X_org ) model_org = sm . OLS ( y_org , X_st_org ) . fit () ################################################# Mean-centered ################################################ np . random . seed ( 563 ) # generate high collinearity data X_std , y_std = generate_collinear_data ( cov , ** kwargs ) # mean-center independent variables scaler = StandardScaler ( with_std = False ) X_std = scaler . fit_transform ( X_std ) # add interaction terms interaction_std_1 = X_std [:, 1 ] . reshape ( - 1 , 1 ) * X_std [:, 2 ] . reshape ( - 1 , 1 ) # x2 * x3 X_std = np . concatenate ([ X_std , interaction_std_1 ], axis = 1 ) # VIF vifs_std = np . linalg . inv ( pd . DataFrame ( X_std ) . corr () . values ) . diagonal () # fit linear regression model X_st_std = sm . add_constant ( X_std ) model_std = sm . OLS ( y_std , X_st_std ) . fit () ################################################# Print output ################################################# print ( model_org . summary ()) print () print () print ( model_std . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 1.000 Model: OLS Adj. R-squared: 1.000 Method: Least Squares F-statistic: 2.641e+05 Date: Wed, 22 Jan 2020 Prob (F-statistic): 1.08e-97 Time: 00:14:25 Log-Likelihood: -73.900 No. Observations: 50 AIC: 157.8 Df Residuals: 45 BIC: 167.4 Df Model: 4 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 15.7341 9.062 1.736 0.089 -2.517 33.986 x1 13.1387 0.365 35.960 0.000 12.403 13.875 x2 0.6202 0.428 1.449 0.154 -0.242 1.482 x3 2.3891 2.334 1.023 0.312 -2.312 7.091 x4 9.1545 0.155 58.893 0.000 8.841 9.468 ============================================================================== Omnibus: 1.688 Durbin-Watson: 2.399 Prob(Omnibus): 0.430 Jarque-Bera (JB): 1.590 Skew: 0.335 Prob(JB): 0.452 Kurtosis: 2.439 Cond. No. 2.17e+03 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 2.17e+03. This might indicate that there are strong multicollinearity or other numerical problems. OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 1.000 Model: OLS Adj. R-squared: 1.000 Method: Least Squares F-statistic: 2.641e+05 Date: Wed, 22 Jan 2020 Prob (F-statistic): 1.08e-97 Time: 00:14:25 Log-Likelihood: -73.900 No. Observations: 50 AIC: 157.8 Df Residuals: 45 BIC: 167.4 Df Model: 4 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 132.8894 0.195 682.554 0.000 132.497 133.282 x1 13.1387 0.365 35.960 0.000 12.403 13.875 x2 19.3499 0.346 55.926 0.000 18.653 20.047 x3 130.4646 0.346 377.226 0.000 129.768 131.161 x4 9.1545 0.155 58.893 0.000 8.841 9.468 ============================================================================== Omnibus: 1.688 Durbin-Watson: 2.399 Prob(Omnibus): 0.430 Jarque-Bera (JB): 1.590 Skew: 0.335 Prob(JB): 0.452 Kurtosis: 2.439 Cond. No. 4.73 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. In [ ]: Notes: Statistical significance in linear regression analysis In [28]: import numpy as np import statsmodels.api as sm import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler import pandas as pd ############################################# create collinear data ############################################# def generate_collinear_data ( cov , n = 10 , true_coefs = [ 0 , 0 ], true_intercept = 0 , feature_means = [ 0 , 0 , 0 ], loc = 0 , scale = 1 , true_coefs_interaction = [ 0 , 0 ]): # random generation of 3D gaussian collinear features. X = np . random . multivariate_normal ( mean = feature_means , cov = cov , size = n ) # generate gaussian white noise. gaussian_noise = np . random . normal ( loc = loc , scale = scale , size = n ) # make the outcome. y = true_intercept + gaussian_noise for i in range ( len ( true_coefs )): y += true_coefs [ i ] * X [:, i ] y += true_coefs_interaction [ 0 ] * X [:, 1 ] * X [:, 2 ] return X , y # settings m = 10000 # number of simulations correlation = 0.85 # degree of collinearity (HIGH) kwargs = { 'n' : 50 , # sample size 'true_coefs' : [ 13 , 0.5 , 5 ], # linear regression coefficients, 2 features 'true_intercept' : 15 , # y-intercept 'feature_means' : [ - 12 , 14 , 2 ], # means multivariate normal distribution. This is not important 'loc' : 0 , # mean of gaussian noise 'scale' : 1 , # standard deviation of gaussian noise 'true_coefs_interaction' : [ 9 ] # linear regression coefficients of interaction terms } # high collinearity covariance matrix cov = np . full (( len ( kwargs [ 'true_coefs' ]), len ( kwargs [ 'true_coefs' ])), correlation ) np . fill_diagonal ( cov , 1 ) seed = np . random . randint ( 1000 ) ################################################# Original data ################################################ np . random . seed ( 563 ) # generate high collinearity data X_org , y_org = generate_collinear_data ( cov , ** kwargs ) # add interaction terms interaction_org_1 = X_org [:, 1 ] . reshape ( - 1 , 1 ) * X_org [:, 2 ] . reshape ( - 1 , 1 ) # x2 * x3 X_org = np . concatenate ([ X_org , interaction_org_1 ], axis = 1 ) # VIF vifs_org = np . linalg . inv ( pd . DataFrame ( X_org ) . corr () . values ) . diagonal () # fit linear regression model X_st_org = sm . add_constant ( X_org ) model_org = sm . OLS ( y_org , X_st_org ) . fit () ################################################# Mean-centered ################################################ np . random . seed ( 563 ) # generate high collinearity data X_std , y_std = generate_collinear_data ( cov , ** kwargs ) # mean-center independent variables scaler = StandardScaler ( with_std = False ) X_std = scaler . fit_transform ( X_std ) # add interaction terms interaction_std_1 = X_std [:, 1 ] . reshape ( - 1 , 1 ) * X_std [:, 2 ] . reshape ( - 1 , 1 ) # x2 * x3 X_std = np . concatenate ([ X_std , interaction_std_1 ], axis = 1 ) # VIF vifs_std = np . linalg . inv ( pd . DataFrame ( X_std ) . corr () . values ) . diagonal () # fit linear regression model X_st_std = sm . add_constant ( X_std ) model_std = sm . OLS ( y_std , X_st_std ) . fit () ################################################# Print output ################################################# print ( model_org . summary ()) print () print () print ( model_std . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 1.000 Model: OLS Adj. R-squared: 1.000 Method: Least Squares F-statistic: 2.641e+05 Date: Tue, 21 Jan 2020 Prob (F-statistic): 1.08e-97 Time: 13:24:21 Log-Likelihood: -73.900 No. Observations: 50 AIC: 157.8 Df Residuals: 45 BIC: 167.4 Df Model: 4 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 15.7341 9.062 1.736 0.089 -2.517 33.986 x1 13.1387 0.365 35.960 0.000 12.403 13.875 x2 0.6202 0.428 1.449 0.154 -0.242 1.482 x3 2.3891 2.334 1.023 0.312 -2.312 7.091 x4 9.1545 0.155 58.893 0.000 8.841 9.468 ============================================================================== Omnibus: 1.688 Durbin-Watson: 2.399 Prob(Omnibus): 0.430 Jarque-Bera (JB): 1.590 Skew: 0.335 Prob(JB): 0.452 Kurtosis: 2.439 Cond. No. 2.17e+03 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 2.17e+03. This might indicate that there are strong multicollinearity or other numerical problems. OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 1.000 Model: OLS Adj. R-squared: 1.000 Method: Least Squares F-statistic: 2.641e+05 Date: Tue, 21 Jan 2020 Prob (F-statistic): 1.08e-97 Time: 13:24:21 Log-Likelihood: -73.900 No. Observations: 50 AIC: 157.8 Df Residuals: 45 BIC: 167.4 Df Model: 4 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 132.8894 0.195 682.554 0.000 132.497 133.282 x1 13.1387 0.365 35.960 0.000 12.403 13.875 x2 19.3499 0.346 55.926 0.000 18.653 20.047 x3 130.4646 0.346 377.226 0.000 129.768 131.161 x4 9.1545 0.155 58.893 0.000 8.841 9.468 ============================================================================== Omnibus: 1.688 Durbin-Watson: 2.399 Prob(Omnibus): 0.430 Jarque-Bera (JB): 1.590 Skew: 0.335 Prob(JB): 0.452 Kurtosis: 2.439 Cond. No. 4.73 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. In [ ]: In [13]: np . random . randint ( 1000 ) Out[13]: 217 In [ ]: In [ ]: In [4]: import numpy as np import statsmodels.api as sm import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler import pandas as pd def generate_collinear_data ( cov , n = 10 , true_coefs = [ 0 , 0 ], true_intercept = 0 , feature_means = [ 0 , 0 , 0 ], loc = 0 , scale = 1 , true_coefs_interaction = [ 0 , 0 ]): # random generation of 3D gaussian collinear features. X = np . random . multivariate_normal ( mean = feature_means , cov = cov , size = n ) # generate gaussian white noise. gaussian_noise = np . random . normal ( loc = loc , scale = scale , size = n ) # make the outcome. y = true_intercept + gaussian_noise for i in range ( len ( true_coefs )): y += true_coefs [ i ] * X [:, i ] y += true_coefs_interaction [ 0 ] * X [:, 1 ] * X [:, 2 ] return X , y # settings m = 10000 # number of simulations correlation = 0.98 # degree of collinearity (HIGH) kwargs = { 'n' : 50 , # sample size 'true_coefs' : [ 13 , 0.5 , 5 ], # linear regression coefficients, 2 features 'true_intercept' : 15 , # y-intercept 'feature_means' : [ - 12 , 14 , 2 ], # means multivariate normal distribution. This is not important 'loc' : 0 , # mean of gaussian noise 'scale' : 1 , # standard deviation of gaussian noise 'true_coefs_interaction' : [ 9 ] # linear regression coefficients of interaction terms } # high collinearity covariance matrix cov = np . full (( len ( kwargs [ 'true_coefs' ]), len ( kwargs [ 'true_coefs' ])), correlation ) np . fill_diagonal ( cov , 1 ) # simulation org_params = [] std_params = [] for i in range ( m ): ################################################ Original ################################################ np . random . seed ( i ) # generate high collinearity data X_org , y_org = generate_collinear_data ( cov , ** kwargs ) # add interaction terms interaction_org_1 = X_org [:, 1 ] . reshape ( - 1 , 1 ) * X_org [:, 2 ] . reshape ( - 1 , 1 ) # x2 * x3 X_org = np . concatenate ([ X_org , interaction_org_1 ], axis = 1 ) # VIF vifs_org = np . linalg . inv ( pd . DataFrame ( X_org ) . corr () . values ) . diagonal () # fit linear regression model X_st_org = sm . add_constant ( X_org ) model_org = sm . OLS ( y_org , X_st_org ) . fit () org_params . append ( model_org . params ) ############################################## Mean-centered ############################################## np . random . seed ( i ) # generate high collinearity data X_std , y_std = generate_collinear_data ( cov , ** kwargs ) # mean-center independent variables scaler = StandardScaler ( with_std = False ) X_std = scaler . fit_transform ( X_std ) # add interaction terms interaction_std_1 = X_std [:, 1 ] . reshape ( - 1 , 1 ) * X_std [:, 2 ] . reshape ( - 1 , 1 ) # x2 * x3 X_std = np . concatenate ([ X_std , interaction_std_1 ], axis = 1 ) # VIF vifs_std = np . linalg . inv ( pd . DataFrame ( X_std ) . corr () . values ) . diagonal () # fit linear regression model X_st_std = sm . add_constant ( X_std ) model_std = sm . OLS ( y_std , X_st_std ) . fit () std_params . append ( model_std . params ) ########################################################################################################## # list to numpy conversion org_params = np . asarray ( org_params ) std_params = np . asarray ( std_params ) #################################################### Plot #################################################### # plotting def styling ( ax ): ax . set_facecolor ( '#eeeeee' ) ax . grid ( True , linestyle = '--' , color = '#acacac' ) ax . tick_params ( color = 'grey' ) _ = [ spine . set_edgecolor ( 'grey' ) for spine in ax . spines . values ()] ax . text ( 0.35 , 0.88 , 'aegis4048.github.io' , fontsize = 10.5 , ha = 'center' , va = 'center' , transform = ax . transAxes , color = 'grey' , alpha = 0.5 ) boxplot_styling = { 'sym' : '' , 'whis' : [ 2.5 , 97.5 ], 'showfliers' : False , 'vert' : True , 'capprops' : dict ( linewidth = 2.0 , color = 'k' ), 'medianprops' : dict ( linewidth = 2.0 , color = 'k' ), 'widths' : ( 0.4 ), 'patch_artist' : True , 'whiskerprops' : dict ( linewidth = 1.5 , color = 'k' , linestyle = '--' ), } labels = [ 'Intercept' , '$X_1$' , '$X_2$' , '$X_3$' , '$X_2X_3$' ] true_params = [ kwargs [ 'true_intercept' ]] + kwargs [ 'true_coefs' ] + kwargs [ 'true_coefs_interaction' ] fig , axes = plt . subplots ( 1 , len ( labels ), figsize = ( 16 , 6 )) for i , ( ax , label , true_param ) in enumerate ( zip ( axes , labels , true_params )): bp1 = ax . boxplot ( org_params [:, i ], positions = [ 1.0 ], boxprops = dict ( linewidth = 1.5 , facecolor = '#00bfc4' ), ** boxplot_styling ) bp2 = ax . boxplot ( std_params [:, i ], positions = [ 1.7 ], boxprops = dict ( linewidth = 1.5 , facecolor = '#f8766d' ), ** boxplot_styling ) styling ( ax ) ax . set_title ( label , fontsize = 20 , y =- 0.2 ) if i != 0 : ax . set_xticklabels ([ 'VIF = {} ' . format ( int ( vifs_org [ i - 1 ])), 'VIF = {} ' . format ( int ( vifs_std [ i - 1 ]))], fontsize = 15 ) else : ax . set_xticklabels ([]) ls_handle = ax . axhline ( true_param , linestyle = '-.' , color = '#ad203e' , label = 'True value' , zorder = 9 ) fig . legend ([ bp1 [ \"boxes\" ][ 0 ], bp2 [ \"boxes\" ][ 0 ], ls_handle ], [ 'Original' , 'Mean-centered' , 'True coefficient' ], loc = 'lower center' , ncol = 3 , borderaxespad = 0.7 , fontsize = 17 ) fig . suptitle ( 'Effect of mean centering for interaction terms - %.f simulations' % m , fontsize = 25 ); fig . tight_layout ( rect = [ 0 , 0.1 , 1 , 0.9 ]) In [3]: fig . savefig ( 'multicollinearity_interaction_conf_int.png' , dpi = 400 ) In [10]: import numpy as np import statsmodels.api as sm import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler import pandas as pd ############################################# create collinear data ############################################# def generate_collinear_data ( cov , n = 10 , true_coefs = [ 0 , 0 ], true_intercept = 0 , feature_means = [ 0 , 0 , 0 ], loc = 0 , scale = 1 , true_coefs_interaction = [ 0 , 0 ]): # random generation of 3D gaussian collinear features. X = np . random . multivariate_normal ( mean = feature_means , cov = cov , size = n ) # generate gaussian white noise. gaussian_noise = np . random . normal ( loc = loc , scale = scale , size = n ) # make the outcome. y = true_intercept + gaussian_noise for i in range ( len ( true_coefs )): y += true_coefs [ i ] * X [:, i ] y += true_coefs_interaction [ 0 ] * X [:, 1 ] * X [:, 2 ] return X , y # settings m = 10000 # number of simulations correlation = 0.98 # degree of collinearity (HIGH) kwargs = { 'n' : 50 , # sample size 'true_coefs' : [ 13 , 0.5 , 5 ], # linear regression coefficients, 2 features 'true_intercept' : 15 , # y-intercept 'feature_means' : [ - 12 , 14 , 2 ], # means multivariate normal distribution. This is not important 'loc' : 0 , # mean of gaussian noise 'scale' : 1 , # standard deviation of gaussian noise 'true_coefs_interaction' : [ 9 ] # linear regression coefficients of interaction terms } # high collinearity covariance matrix cov = np . full (( len ( kwargs [ 'true_coefs' ]), len ( kwargs [ 'true_coefs' ])), correlation ) np . fill_diagonal ( cov , 1 ) ################################################# Original data ################################################ np . random . seed ( 1 ) # generate high collinearity data X_org , y_org = generate_collinear_data ( cov , ** kwargs ) # add interaction terms interaction_org_1 = X_org [:, 1 ] . reshape ( - 1 , 1 ) * X_org [:, 2 ] . reshape ( - 1 , 1 ) # x2 * x3 X_org = np . concatenate ([ X_org , interaction_org_1 ], axis = 1 ) # VIF vifs_org = np . linalg . inv ( pd . DataFrame ( X_org ) . corr () . values ) . diagonal () # fit linear regression model X_st_org = sm . add_constant ( X_org ) model_org = sm . OLS ( y_org , X_st_org ) . fit () ################################################# Mean-centered ################################################ np . random . seed ( 1 ) # generate high collinearity data X_std , y_std = generate_collinear_data ( cov , ** kwargs ) # mean-center independent variables scaler = StandardScaler ( with_std = False ) X_std = scaler . fit_transform ( X_std ) # add interaction terms interaction_std_1 = X_std [:, 1 ] . reshape ( - 1 , 1 ) * X_std [:, 2 ] . reshape ( - 1 , 1 ) # x2 * x3 X_std = np . concatenate ([ X_std , interaction_std_1 ], axis = 1 ) # VIF vifs_std = np . linalg . inv ( pd . DataFrame ( X_std ) . corr () . values ) . diagonal () # fit linear regression model X_st_std = sm . add_constant ( X_std ) model_std = sm . OLS ( y_std , X_st_std ) . fit () ################################################# Print output ################################################# print ( model_org . summary ()) print () print () print ( model_std . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 1.000 Model: OLS Adj. R-squared: 1.000 Method: Least Squares F-statistic: 2.848e+05 Date: Mon, 20 Jan 2020 Prob (F-statistic): 1.97e-98 Time: 23:44:48 Log-Likelihood: -65.953 No. Observations: 50 AIC: 141.9 Df Residuals: 45 BIC: 151.5 Df Model: 4 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 52.0233 25.117 2.071 0.044 1.435 102.612 x1 14.1126 1.090 12.943 0.000 11.917 16.309 x2 -1.2390 0.989 -1.252 0.217 -3.232 0.754 x3 6.5611 2.457 2.671 0.011 1.613 11.509 x4 8.9210 0.148 60.449 0.000 8.624 9.218 ============================================================================== Omnibus: 3.143 Durbin-Watson: 1.877 Prob(Omnibus): 0.208 Jarque-Bera (JB): 2.121 Skew: -0.378 Prob(JB): 0.346 Kurtosis: 3.669 Cond. No. 6.72e+03 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 6.72e+03. This might indicate that there are strong multicollinearity or other numerical problems. OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 1.000 Model: OLS Adj. R-squared: 1.000 Method: Least Squares F-statistic: 2.848e+05 Date: Mon, 20 Jan 2020 Prob (F-statistic): 1.97e-98 Time: 23:44:48 Log-Likelihood: -65.953 No. Observations: 50 AIC: 141.9 Df Residuals: 45 BIC: 151.5 Df Model: 4 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 123.9510 0.177 699.380 0.000 123.594 124.308 x1 14.1126 1.090 12.943 0.000 11.917 16.309 x2 16.3530 0.923 17.723 0.000 14.495 18.211 x3 131.1556 0.878 149.347 0.000 129.387 132.924 x4 8.9210 0.148 60.449 0.000 8.624 9.218 ============================================================================== Omnibus: 3.143 Durbin-Watson: 1.877 Prob(Omnibus): 0.208 Jarque-Bera (JB): 2.121 Skew: -0.378 Prob(JB): 0.346 Kurtosis: 3.669 Cond. No. 15.5 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. In [7]: print ( model_org . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 1.000 Model: OLS Adj. R-squared: 1.000 Method: Least Squares F-statistic: 2.848e+05 Date: Mon, 20 Jan 2020 Prob (F-statistic): 1.97e-98 Time: 20:14:02 Log-Likelihood: -65.953 No. Observations: 50 AIC: 141.9 Df Residuals: 45 BIC: 151.5 Df Model: 4 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 52.0233 25.117 2.071 0.044 1.435 102.612 x1 14.1126 1.090 12.943 0.000 11.917 16.309 x2 -1.2390 0.989 -1.252 0.217 -3.232 0.754 x3 6.5611 2.457 2.671 0.011 1.613 11.509 x4 8.9210 0.148 60.449 0.000 8.624 9.218 ============================================================================== Omnibus: 3.143 Durbin-Watson: 1.877 Prob(Omnibus): 0.208 Jarque-Bera (JB): 2.121 Skew: -0.378 Prob(JB): 0.346 Kurtosis: 3.669 Cond. No. 6.72e+03 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 6.72e+03. This might indicate that there are strong multicollinearity or other numerical problems. In [8]: print ( model_std . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 1.000 Model: OLS Adj. R-squared: 1.000 Method: Least Squares F-statistic: 2.848e+05 Date: Mon, 20 Jan 2020 Prob (F-statistic): 1.97e-98 Time: 20:14:09 Log-Likelihood: -65.953 No. Observations: 50 AIC: 141.9 Df Residuals: 45 BIC: 151.5 Df Model: 4 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 123.9510 0.177 699.380 0.000 123.594 124.308 x1 14.1126 1.090 12.943 0.000 11.917 16.309 x2 16.3530 0.923 17.723 0.000 14.495 18.211 x3 131.1556 0.878 149.347 0.000 129.387 132.924 x4 8.9210 0.148 60.449 0.000 8.624 9.218 ============================================================================== Omnibus: 3.143 Durbin-Watson: 1.877 Prob(Omnibus): 0.208 Jarque-Bera (JB): 2.121 Skew: -0.378 Prob(JB): 0.346 Kurtosis: 3.669 Cond. No. 15.5 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. In [ ]: In [ ]: In [ ]: In [5]: model_org . summary () Out[5]: OLS Regression Results Dep. Variable: y R-squared: 1.000 Model: OLS Adj. R-squared: 1.000 Method: Least Squares F-statistic: 3.181e+05 Date: Mon, 20 Jan 2020 Prob (F-statistic): 1.64e-99 Time: 20:05:35 Log-Likelihood: -75.229 No. Observations: 50 AIC: 160.5 Df Residuals: 45 BIC: 170.0 Df Model: 4 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] const 2.0978 19.553 0.107 0.915 -37.284 41.480 x1 12.2991 0.838 14.669 0.000 10.610 13.988 x2 0.7617 0.884 0.862 0.393 -1.018 2.541 x3 5.4631 1.541 3.546 0.001 2.360 8.566 x4 8.9981 0.107 84.047 0.000 8.782 9.214 Omnibus: 1.684 Durbin-Watson: 1.831 Prob(Omnibus): 0.431 Jarque-Bera (JB): 1.432 Skew: -0.256 Prob(JB): 0.489 Kurtosis: 2.348 Cond. No. 4.64e+03 Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 4.64e+03. This might indicate that there are strong multicollinearity or other numerical problems. In [6]: model_std . summary () Out[6]: OLS Regression Results Dep. Variable: y R-squared: 1.000 Model: OLS Adj. R-squared: 1.000 Method: Least Squares F-statistic: 3.181e+05 Date: Mon, 20 Jan 2020 Prob (F-statistic): 1.64e-99 Time: 20:05:41 Log-Likelihood: -75.229 No. Observations: 50 AIC: 160.5 Df Residuals: 45 BIC: 170.0 Df Model: 4 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] const 132.1829 0.215 614.006 0.000 131.749 132.616 x1 12.2991 0.838 14.669 0.000 10.610 13.988 x2 19.0117 0.858 22.166 0.000 17.284 20.739 x3 131.5856 0.825 159.449 0.000 129.923 133.248 x4 8.9981 0.107 84.047 0.000 8.782 9.214 Omnibus: 1.684 Durbin-Watson: 1.831 Prob(Omnibus): 0.431 Jarque-Bera (JB): 1.432 Skew: -0.256 Prob(JB): 0.489 Kurtosis: 2.348 Cond. No. 15.8 Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. In [ ]: In [ ]: In [ ]: In [3]: fig . savefig ( 'multicollinearity_interaction_conf_int.png' , dpi = 400 ) Mean centering changes the interpretation of coefficients and y-intercept Mean centering removes structural multicollinearty, but does not reduce data multicollinearity Correlation matrix pic (only multicollinearity involved with interaction terms are reduced. This is why it's irrelevant when you don't add interaction terms.) VIF does not reduce data colliearity, affects only variables involved with interaction terms. Mean centering does not improve the precision of interaction term's coefficient estimate simulation result The purpose of any remedial procedures of multicollinearity is to increase the precision of the coefficient estimates of features. As explained above, While mean centering reduces the standard errors of coefficient estimates, it has no practical advantage Statsmodel results We observe that the stardard errors have decreased, but this has no meaning. Decreased compared to what? Mean centering subtracts each independent variable's mean from itself. Assume the following arbitrary linear model with three features and an interaction term: $$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_2 x_3 \\tag{9}$$ Applying mean centering, the model becomes: $$ y = \\beta_0&#94;* + \\beta_1&#94;* (x_1 - \\bar{x_1}) + \\beta_2&#94;* (x_2 - \\bar{x_2}) + \\beta_3&#94;* (x_3 - \\bar{x_3}) + \\beta_4&#94;* (x_2 - \\bar{x_2}) (x_3 - \\bar{x_3}) \\tag{10}$$ where $\\bar{x_i}$ is the mean of the i-th feature, and $\\beta_i&#94;*$ is the new i-th coefficient of the mean-centered model. Rearranging eq (10) , it can be algebracially shown that mean centering changes the values of the coefficients: $$ \\begin{align} \\beta_0&#94;* &= \\beta_0 - \\beta_1\\bar{x_1} - \\beta_2\\bar{x_2} - \\beta_3\\bar{x_3} - \\beta_4\\bar{x_2}\\bar{x_3} \\label{}\\tag{11}\\\\ \\beta_1&#94;* &= \\beta_1 \\label{}\\tag{12}\\\\ \\beta_2&#94;* &= \\beta_2 - \\beta_4 \\bar{x_3} \\label{}\\tag{13}\\\\ \\beta_3&#94;* &= \\beta_3 - \\beta_4 \\bar{x_2} \\label{}\\tag{14}\\\\ \\beta_4&#94;* &= \\beta_4 \\label{}\\tag{15}\\\\ \\end{align} $$ From the above derivation of the mean-centered coefficients, we observe that centering: changes the intercept value ( $\\beta_0&#94;*$ ) changes the coefficient values of the features ( $x_2$ and $x_3$ ) involved with the interaction term ( $x_2 x_3$ ) does not change the coefficient value of the feature ( $x_1$ ) not involved with the interaction term ( $x_2 x_3$ ) does not change the coefficient value of the interaction term ( $x_2 x_3$ ) These observations hold true for all mean-centered models with interaction terms. While centering complicates the interpretation of the coefficients, it improves the interpretation of the y-intercept. Consider a person's height as a dependent variable $y$ and weight as an independent variable $x$ . Before centering, the y-intercept $\\beta_0$ is the height of a person with weight 0, which doesn't make sense. After centering, $\\beta_0$ is the height of a person with average weight, because $x - \\bar{x} = 0$ . In this case, the numerical value of $\\beta_0$ has a real world interpretation. This makes the interpretation of the intercept to be the expected values of $y$ when the features are set to their sample means. Let's see how mean centering reduce structural multicollinearity caused by interaction terms with Python demonstration. Figure 12: Effect of mean centering with interaction terms on correlation matrix Source Code For Figure (12) import matplotlib.pyplot as plt import pandas as pd import seaborn as sns import numpy as np import statsmodels.api as sm from sklearn.preprocessing import StandardScaler ############################################# create collinear data ############################################# np.random.seed(1) def generate_collinear_data(cov, n=10, true_coefs=[0, 0], true_intercept=0, feature_means=[0, 0, 0], loc=0, scale=1): # random generation of 3D gaussian collinear features. X = np.random.multivariate_normal(mean=feature_means, cov=cov, size=n) # generate gaussian white noise. gaussian_noise = np.random.normal(loc=loc, scale=scale, size=n) # make the outcome. y = true_intercept + gaussian_noise for i in range(len(true_coefs)): y += true_coefs[i] * X[:, i] return X, y # settings correlation = 0.3 # degree of collinearity (HIGH) kwargs = { 'n': 1000, 'true_coefs': [13, 0.5, 5], # linear regression coefficients, 2 features 'true_intercept': 2, # y-intercept 'feature_means': [15, 1, 8], # means multivariate normal distribution. This is not important 'loc': 0, # mean of gaussian noise 'scale': 1 # standard deviation of gaussian noise } # high collinearity covariance matrix cov = np.full((len(kwargs['true_coefs']), len(kwargs['true_coefs'])), correlation) np.fill_diagonal(cov, 1) X, y = generate_collinear_data(cov, **kwargs) ############################################ Add interaction terms ############################################### features = ['$X_1$', '$X_2$', '$X_3$'] # original df_org = pd.DataFrame(X, columns=features) df_org['$X_2 \\cdot X_3$'] = df_org['$X_2$'].values * df_org['$X_3$'].values corr_org = df_org.corr(method='kendall').values # mean-centered scaler = StandardScaler(with_mean=True, with_std=False) X_cnt = scaler.fit_transform(X) df_cnt = pd.DataFrame(X_cnt, columns=features) df_cnt['$X_2 \\cdot X_3$'] = df_cnt['$X_2$'].values * df_cnt['$X_3$'].values corr_cnt = df_cnt.corr(method='kendall').values ###################################################### plot ###################################################### fig, axes = plt.subplots(1, 2, figsize=(11.5, 5)) cmap = sns.diverging_palette(220, 10, as_cmap=True, sep=60) sns.heatmap(corr_org, cmap=cmap, vmin=-1, vmax=1, center=0, linewidths=.5, ax=axes[0], annot=True) sns.heatmap(corr_cnt, cmap=cmap, vmin=-1, vmax=1, center=0, linewidths=.5, ax=axes[1], annot=True) axes[0].set_xticklabels(list(df_org.columns)) axes[0].set_yticklabels(list(df_org.columns)) axes[0].set_title('Original', fontsize=15) axes[0].text(0.3, 0.9, 'aegis4048.github.io', fontsize=12, ha='center', va='center', transform=axes[0].transAxes, color='grey', alpha=0.5) axes[1].set_xticklabels(list(df_cnt.columns)) axes[1].set_yticklabels(list(df_cnt.columns)) axes[1].set_title('Mean-centered', fontsize=15) axes[1].text(0.3, 0.9, 'aegis4048.github.io', fontsize=12, ha='center', va='center', transform=axes[1].transAxes, color='grey', alpha=0.5) axes[1].text(-0.3, 0.43, r'$\\Longrightarrow$', transform=axes[1].transAxes, fontsize=30) fig.suptitle('Kendall\\'s correlation coefficient matrix', fontsize=20) fig.tight_layout(rect=[0, 0, 1, 0.94]) Notes: statistical significance We observe an interesting phenomenon: x2 is not significant. This explains the reason that the VIF for x2 was low, even though there was an interaction term that involves x2. https://link.springer.com/article/10.3758/s13428-015-0624-x In [26]: from numpy.random import seed from scipy.stats import spearmanr In [41]: from matplotlib import pyplot as plt In [48]: plt . scatter ( x1 , x1 ** 2 ) plt . scatter ( x1_cent , x1_cent ** 2 ) Out[48]: In [ ]: In [43]: x1 = np . array ([ 5 , 10 , 1 , 2 , 9 , - 11 , - 5 , 12 , 15 , 5 ]) x2 = np . array ([ 21 , 203 , 293 , 1 , - 13 , - 53 , - 31 , 1 , - 45 , - 76 ]) x1_cent = x1 - np . mean ( x1 ) x2_cent = x2 - np . mean ( x2 ) print ( spearmanr ( x1 , x1 * x2 )[ 0 ]) print ( spearmanr ( x1_cent , x1_cent * x2_cent )[ 0 ]) -0.480245379507251 -0.4620082131968491 In [36]: x1 - np . mean ( x1 ) Out[36]: array([ -9., -4., -13., -12., -5., -25., -19., -2., 1., 88.]) In [37]: np . mean ( x1 ) Out[37]: 14.0 In [19]: x1 * x2 Out[19]: array([-126, 20, 12, 196, -45, -352, 10, 372, 315]) In [ ]: In [51]: a = np . array ([ 34.6 , 45.0 , 62.3 , 58.9 , 42.5 , 44.3 , 67.9 , 58.5 , 35.6 , 49.6 , 33.0 ]) In [54]: plt . scatter ( x1 , x1 ** 2 ) Out[54]: In [6]: import matplotlib.pyplot as plt import pandas as pd import seaborn as sns import numpy as np import statsmodels.api as sm from sklearn.preprocessing import StandardScaler ############################################# create collinear data ############################################# np . random . seed ( 1 ) def generate_collinear_data ( cov , n = 10 , true_coefs = [ 0 , 0 ], true_intercept = 0 , feature_means = [ 0 , 0 , 0 ], loc = 0 , scale = 1 ): # random generation of 3D gaussian collinear features. X = np . random . multivariate_normal ( mean = feature_means , cov = cov , size = n ) # generate gaussian white noise. gaussian_noise = np . random . normal ( loc = loc , scale = scale , size = n ) # make the outcome. y = true_intercept + gaussian_noise for i in range ( len ( true_coefs )): y += true_coefs [ i ] * X [:, i ] return X , y # settings correlation = 0.3 # degree of collinearity (HIGH) kwargs = { 'n' : 1000 , 'true_coefs' : [ 13 , 0.5 , 5 ], # linear regression coefficients, 2 features 'true_intercept' : 2 , # y-intercept 'feature_means' : [ 15 , 1 , 8 ], # means multivariate normal distribution. This is not important 'loc' : 0 , # mean of gaussian noise 'scale' : 1 # standard deviation of gaussian noise } # high collinearity covariance matrix cov = np . full (( len ( kwargs [ 'true_coefs' ]), len ( kwargs [ 'true_coefs' ])), correlation ) np . fill_diagonal ( cov , 1 ) X , y = generate_collinear_data ( cov , ** kwargs ) ############################################ Add interaction terms ############################################### features = [ '$X_1$' , '$X_2$' , '$X_3$' ] # original df_org = pd . DataFrame ( X , columns = features ) df_org [ '$X_2 \\cdot X_3$' ] = df_org [ '$X_2$' ] . values * df_org [ '$X_3$' ] . values corr_org = df_org . corr ( method = 'kendall' ) . values # mean-centered scaler = StandardScaler ( with_mean = True , with_std = False ) X_cnt = scaler . fit_transform ( X ) df_cnt = pd . DataFrame ( X_cnt , columns = features ) df_cnt [ '$X_2 \\cdot X_3$' ] = df_cnt [ '$X_2$' ] . values * df_cnt [ '$X_3$' ] . values corr_cnt = df_cnt . corr ( method = 'kendall' ) . values ###################################################### plot ###################################################### fig , axes = plt . subplots ( 1 , 2 , figsize = ( 11.5 , 5 )) cmap = sns . diverging_palette ( 220 , 10 , as_cmap = True , sep = 60 ) sns . heatmap ( corr_org , cmap = cmap , vmin =- 1 , vmax = 1 , center = 0 , linewidths =. 5 , ax = axes [ 0 ], annot = True ) sns . heatmap ( corr_cnt , cmap = cmap , vmin =- 1 , vmax = 1 , center = 0 , linewidths =. 5 , ax = axes [ 1 ], annot = True ) axes [ 0 ] . set_xticklabels ( list ( df_org . columns )) axes [ 0 ] . set_yticklabels ( list ( df_org . columns )) axes [ 0 ] . set_title ( 'Original' , fontsize = 15 ) axes [ 0 ] . text ( 0.3 , 0.9 , 'aegis4048.github.io' , fontsize = 12 , ha = 'center' , va = 'center' , transform = axes [ 0 ] . transAxes , color = 'grey' , alpha = 0.5 ) axes [ 1 ] . set_xticklabels ( list ( df_cnt . columns )) axes [ 1 ] . set_yticklabels ( list ( df_cnt . columns )) axes [ 1 ] . set_title ( 'Mean-centered' , fontsize = 15 ) axes [ 1 ] . text ( 0.3 , 0.9 , 'aegis4048.github.io' , fontsize = 12 , ha = 'center' , va = 'center' , transform = axes [ 1 ] . transAxes , color = 'grey' , alpha = 0.5 ) axes [ 1 ] . text ( - 0.3 , 0.43 , r '$\\Longrightarrow$' , transform = axes [ 1 ] . transAxes , fontsize = 30 ) fig . suptitle ( 'Kendall \\' s correlation coefficient matrix' , fontsize = 20 ) fig . tight_layout ( rect = [ 0 , 0 , 1 , 0.94 ]) In [7]: fig . savefig ( 'multicollinearity_mean_centered_kendall.png' , dpi = 400 ) In [38]: spearmanr ( df [ '$X_1$' ] . values , df [ '$X_2$' ] . values ) Out[38]: SpearmanrResult(correlation=0.38305295905295905, pvalue=2.683974867916312e-36) In [ ]: In [18]: import numpy as np df [ '$X_2 \\cdot X_3$' ] = df [ '$X_2$' ] . values * df [ '$X_3$' ] . values In [19]: np . linalg . inv ( df . corr () . values ) . diagonal () Out[19]: array([ 67.44330961, 70.24218782, 124.87759573, 131.95192727]) In [17]: np . linalg . inv ( df . corr () . values ) . diagonal () Out[17]: array([66.18666789, 60.01401724, 36.63775074]) In [ ]: In [ ]: In [ ]: Changes the interpretation of coefficients. Intercept has more meaningful interpretation. Interaction terms are unaffected. Variables not involved with interaction terms are unaffected Intercept has more meaningful interpretation. As far as the interpretation is concerned: Before centering Î±, its meaning is the height of a person with weight 0. Thus, Î± would be a just a numerical intercept (possibly negative) without any real world interpretation. Centering the data around the mean of the weight, will lead to Î± representing the height of a person with average weight. In this case, the numeric value of Î± has a real world interpretation. Figure. https://online.stat.psu.edu/stat462/node/182/ 5.4 Principle component regression One major use of PCA lies in overcoming the multicollinearity problem. PCA can aptly deal with such situations by excluding some of the low-variance principal components in the regression step. In addition, by usually regressing on only a subset of all the principal components, PCR can result in dimension reduction through substantially lowering the effective number of parameters characterizing the underlying model. Handles with feature elimination & lowering collinearity In [ ]: In [2]: import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler ############################################# import data ############################################# file = 'sample_data/collinear.csv' features = [ '$X_1$' , '$X_2$' , '$X_3$' , '$X_4$' , '$X_5$' ] df = pd . read_csv ( file ) df = df [ features ] In [ ]: In [1]: import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler ############################################# import data ############################################# file = 'sample_data/collinear.csv' features = [ '$X_1$' , '$X_2$' , '$X_3$' , '$X_4$' , '$X_5$' ] df = pd . read_csv ( file ) df = df [ features ] ####################################### VIFs with original data ####################################### vifs_orig = np . linalg . inv ( df . corr () . values ) . diagonal () df_vif_orig = pd . DataFrame () df_vif_orig [ 'Features' ] = df . columns df_vif_orig [ 'VIF' ] = vifs_orig ##################################### VIFs with interaction terms ##################################### df [ '$X_2 \\cdot X_3$' ] = df [ '$X_2$' ] . values * df [ '$X_3$' ] . values df [ '$X_1&#94;2$' ] = df [ '$X_1$' ] . values ** 2 vifs_inter = np . linalg . inv ( df . corr () . values ) . diagonal () df_vif_inter = pd . DataFrame () df_vif_inter [ 'Features' ] = df . columns df_vif_inter [ 'VIF' ] = vifs_inter print ( df_vif_orig ) print () print ( df_vif_inter ) Features VIF 0 $X_1$ 5.398315 1 $X_2$ 2.378517 2 $X_3$ 8.598212 3 $X_4$ 2.157740 4 $X_5$ 3.700524 Features VIF 0 $X_1$ 5.420306 1 $X_2$ 2.379443 2 $X_3$ 8.655563 3 $X_4$ 2.186982 4 $X_5$ 3.708457 5 $X_2 \\cdot X_3$ 1.243502 6 $X_1&#94;2$ 1.160139 In [ ]: Structural multicollinearity Data multicollinearity https://stats.stackexchange.com/questions/223432/standardising-non-normally-distributed-predictors-for-regression Centering is just a linear transformation, so it will not change anything about the shapes of the distributions or the relationship between them. Instead, it just slides them in one direction or the other. When the model is additive and linear, centering has nothing to do with collinearity. Centering can only help when there are multiple terms per variable such as square or interaction terms. Making zero intercept makes the model more interpretable https://www3.nd.edu/~rwilliam/stats2/l53.pdf Standardized coefficients interpretation is difficult. Why does centering work? Read (here)[ https://stats.stackexchange.com/questions/65898/why-could-centering-independent-variables-change-the-main-effects-with-moderation ] 5.3 Elimination Demonstration Figure 12: Effect of interaction terms on VIF In practice.... Used to reduce the std error of statsmodel linear regression analysis In [1]: import numpy as np import statsmodels.api as sm import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler import pandas as pd ######################################## Collinear random data generation ######################################### def generate_collinear_data ( cov , n = 10 , true_coefs = [ 0 , 0 ], true_intercept = 0 , feature_means = [ 0 , 0 , 0 ], loc = 0 , scale = 1 , true_coefs_interaction = [ 0 , 0 ]): # random generation of 3D gaussian collinear features. X = np . random . multivariate_normal ( mean = feature_means , cov = cov , size = n ) # generate gaussian white noise. gaussian_noise = np . random . normal ( loc = loc , scale = scale , size = n ) # make the outcome. y = true_intercept + gaussian_noise for i in range ( len ( true_coefs )): y += true_coefs [ i ] * X [:, i ] #y += true_coefs_interaction[0] * (X[:, 0] ** 2) + true_coefs_interaction[1] * X[:, 1] * X[:, 2] return X , y # settings m = 1000 # number of simulations correlation = 0.01 # degree of collinearity (HIGH) kwargs = { 'n' : 10000 , # sample size 'true_coefs' : [ 13 , 0.5 , 5 , 42 ], # linear regression coefficients, 2 features 'true_intercept' : 15 , # y-intercept 'feature_means' : [ - 12 , 14 , 2 , - 20 ], # means multivariate normal distribution. This is not important 'loc' : 0 , # mean of gaussian noise 'scale' : 1 , # standard deviation of gaussian noise 'true_coefs_interaction' : [ - 2 , 9 ] # linear regression coefficients of interaction terms } # high collinearity covariance matrix cov = np . full (( len ( kwargs [ 'true_coefs' ]), len ( kwargs [ 'true_coefs' ])), correlation ) np . fill_diagonal ( cov , 1 ) print ( 'True intercept + coefficients: ' , [ kwargs [ 'true_intercept' ]] + kwargs [ 'true_coefs' ] + kwargs [ 'true_coefs_interaction' ]) print () ############################################# Original ############################################# X_org , y_org = generate_collinear_data ( cov , ** kwargs ) #interaction_org_1 = X_org[:, 0].reshape(-1, 1) ** 2 # x1&#94;2 #interaction_org_2 = X_org[:, 1].reshape(-1, 1) * X_org[:, 2].reshape(-1, 1) # x2 * x3 #X_org = np.concatenate([X_org, interaction_org_1, interaction_org_2], axis=1) X_st_org = sm . add_constant ( X_org ) model_org = sm . OLS ( y_org , X_st_org ) . fit () print ( 'Original: ' , [ ' %.2f ' % elem for elem in model_org . params ]) ########################################## Mean-centered ########################################### X_std , y_std = generate_collinear_data ( cov , ** kwargs ) # centering scaler = StandardScaler ( with_mean = True , with_std = False ) X_std = scaler . fit_transform ( X_std ) # X_std, y_std = scaler.fit_transform(X_std), scaler.fit_transform(y_std.reshape(-1, 1)).flatten() # add interaction terms #interaction_std_1 = X_std[:, 0].reshape(-1, 1) ** 2 # x1&#94;2 #interaction_std_2 = X_std[:, 1].reshape(-1, 1) * X_std[:, 2].reshape(-1, 1) # x2 * x3 #X_std = np.concatenate([X_std, interaction_std_1, interaction_std_2], axis=1) # fit X_std = sm . add_constant ( X_std ) model_std = sm . OLS ( y_std , X_std ) . fit () print ( 'Centering: ' , [ ' %.2f ' % elem for elem in model_std . params ]) True intercept + coefficients: [15, 13, 0.5, 5, 42, -2, 9] Original: ['14.81', '13.00', '0.50', '4.99', '41.99'] Centering: ['-963.70', '13.00', '0.51', '5.00', '42.01'] In [3]: np . mean ( y_std ) Out[3]: -963.7033929573824 In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [15]: vifs_std = np . linalg . inv ( pd . DataFrame ( X_org ) . corr () . values ) . diagonal () In [21]: df = pd . DataFrame ( X_org ) vifs = [ variance_inflation_factor ( df . values , i ) for i in range ( df . shape [ 1 ])] vifs Out[21]: [5086.798113631407, 913.5323590009473, 922.5802364981283, 391.91660765629143, 1408.0394090339094, 921.8096453141474] In [3]: import matplotlib.pyplot as plt import pandas as pd import seaborn as sns import numpy as np import statsmodels.api as sm from sklearn.preprocessing import StandardScaler ############################################# create collinear data ############################################# np . random . seed ( 1 ) def generate_collinear_data ( cov , n = 10 , true_coefs = [ 0 , 0 ], true_intercept = 0 , feature_means = [ 0 , 0 , 0 ], loc = 0 , scale = 1 ): # random generation of 3D gaussian collinear features. X = np . random . multivariate_normal ( mean = feature_means , cov = cov , size = n ) # generate gaussian white noise. gaussian_noise = np . random . normal ( loc = loc , scale = scale , size = n ) # make the outcome. y = true_intercept + gaussian_noise for i in range ( len ( true_coefs )): y += true_coefs [ i ] * X [:, i ] return X , y # settings correlation = 0.98 # degree of collinearity (LOW) kwargs = { 'n' : 1000 , 'true_coefs' : [ 13 , 0.5 , 5 ], # linear regression coefficients, 2 features 'true_intercept' : 2 , # y-intercept 'feature_means' : [ 15 , 1 , 8 ], # means multivariate normal distribution. This is not important 'loc' : 0 , # mean of gaussian noise 'scale' : 1 # standard deviation of gaussian noise } # low collinearity covariance matrix cov = np . full (( len ( kwargs [ 'true_coefs' ]), len ( kwargs [ 'true_coefs' ])), correlation ) np . fill_diagonal ( cov , 1 ) X , y = generate_collinear_data ( cov , ** kwargs ) ############################################ Add interaction terms ############################################### features = [ '$X_1$' , '$X_2$' , '$X_3$' ] # original df_org = pd . DataFrame ( X , columns = features ) # interaction df_int = pd . DataFrame ( X , columns = features ) df_int [ '$X_2 \\cdot X_3$' ] = df_int [ '$X_2$' ] . values * df_int [ '$X_3$' ] . values corr_int = df_int . corr ( method = 'kendall' ) . values # mean-centered scaler = StandardScaler ( with_mean = True , with_std = False ) X_cnt = scaler . fit_transform ( X ) df_cnt = pd . DataFrame ( X_cnt , columns = features ) df_cnt [ '$X_2 \\cdot X_3$' ] = df_cnt [ '$X_2$' ] . values * df_cnt [ '$X_3$' ] . values ################################################## Commput VIFs ################################################## vifs_org = np . linalg . inv ( df_org . corr () . values ) . diagonal () df_vif_org = pd . DataFrame () df_vif_org [ 'Features' ] = df_org . columns df_vif_org [ 'VIF' ] = vifs_org print ( df_vif_org ) print () vifs_int = np . linalg . inv ( df_int . corr () . values ) . diagonal () df_vif_int = pd . DataFrame () df_vif_int [ 'Features' ] = df_int . columns df_vif_int [ 'VIF' ] = vifs_int print ( df_vif_int ) print () vifs_cnt = np . linalg . inv ( df_cnt . corr () . values ) . diagonal () df_vif_cnt = pd . DataFrame () df_vif_cnt [ 'Features' ] = df_cnt . columns df_vif_cnt [ 'VIF' ] = vifs_cnt print ( df_vif_cnt ) Features VIF 0 $X_1$ 31.424442 1 $X_2$ 33.921560 2 $X_3$ 32.850339 Features VIF 0 $X_1$ 31.449014 1 $X_2$ 69.808368 2 $X_3$ 33.896329 3 $X_2 \\cdot X_3$ 47.816783 Features VIF 0 $X_1$ 31.449014 1 $X_2$ 33.926453 2 $X_3$ 32.928956 3 $X_2 \\cdot X_3$ 1.005069 In [18]: X_std . shape Out[18]: (10000, 7) In [19]: from statsmodels.stats.outliers_influence import variance_inflation_factor In [8]: print ( model_org . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 1.000 Model: OLS Adj. R-squared: 1.000 Method: Least Squares F-statistic: 9.406e+05 Date: Sun, 19 Jan 2020 Prob (F-statistic): 0.00 Time: 16:08:58 Log-Likelihood: -1389.1 No. Observations: 1000 AIC: 2788. Df Residuals: 995 BIC: 2813. Df Model: 4 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 12.7842 5.521 2.315 0.021 1.949 23.619 x1 12.7963 0.190 67.257 0.000 12.423 13.170 x2 0.5324 0.185 2.882 0.004 0.170 0.895 x3 5.1008 0.185 27.613 0.000 4.738 5.463 x4 42.0438 0.194 217.259 0.000 41.664 42.424 ============================================================================== Omnibus: 2.696 Durbin-Watson: 1.969 Prob(Omnibus): 0.260 Jarque-Bera (JB): 2.745 Skew: 0.125 Prob(JB): 0.254 Kurtosis: 2.939 Cond. No. 4.90e+03 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 4.9e+03. This might indicate that there are strong multicollinearity or other numerical problems. In [9]: print ( model_std . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 1.000 Model: OLS Adj. R-squared: 1.000 Method: Least Squares F-statistic: 8.617e+05 Date: Sun, 19 Jan 2020 Prob (F-statistic): 0.00 Time: 16:08:59 Log-Likelihood: -1431.0 No. Observations: 1000 AIC: 2872. Df Residuals: 995 BIC: 2896. Df Model: 4 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -961.9543 0.032 -3e+04 0.000 -962.017 -961.891 x1 13.0781 0.196 66.868 0.000 12.694 13.462 x2 0.4888 0.197 2.481 0.013 0.102 0.875 x3 5.2289 0.191 27.359 0.000 4.854 5.604 x4 41.7452 0.204 204.526 0.000 41.345 42.146 ============================================================================== Omnibus: 1.031 Durbin-Watson: 2.008 Prob(Omnibus): 0.597 Jarque-Bera (JB): 0.930 Skew: -0.069 Prob(JB): 0.628 Kurtosis: 3.057 Cond. No. 14.4 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. In [ ]: In [2]: import numpy as np import statsmodels.api as sm import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler import pandas as pd def generate_collinear_data ( cov , n = 10 , true_coefs = [ 0 , 0 ], true_intercept = 0 , feature_means = [ 0 , 0 , 0 ], loc = 0 , scale = 1 , true_coefs_interaction = [ 0 , 0 ]): # random generation of 3D gaussian collinear features. X = np . random . multivariate_normal ( mean = feature_means , cov = cov , size = n ) # generate gaussian white noise. gaussian_noise = np . random . normal ( loc = loc , scale = scale , size = n ) # make the outcome. y = true_intercept + gaussian_noise for i in range ( len ( true_coefs )): y += true_coefs [ i ] * X [:, i ] y += true_coefs_interaction [ 0 ] * X [:, 1 ] * X [:, 2 ] return X , y # settings m = 10000 # number of simulations correlation = 0.98 # degree of collinearity (HIGH) kwargs = { 'n' : 50 , # sample size 'true_coefs' : [ 13 , 0.5 , 5 ], # linear regression coefficients, 2 features 'true_intercept' : 15 , # y-intercept 'feature_means' : [ - 12 , 14 , 2 ], # means multivariate normal distribution. This is not important 'loc' : 0 , # mean of gaussian noise 'scale' : 1 , # standard deviation of gaussian noise 'true_coefs_interaction' : [ 9 ] # linear regression coefficients of interaction terms } # high collinearity covariance matrix cov = np . full (( len ( kwargs [ 'true_coefs' ]), len ( kwargs [ 'true_coefs' ])), correlation ) np . fill_diagonal ( cov , 1 ) # simulation org_params = [] std_params = [] for i in range ( m ): ################################################ Original ################################################ # generate high collinearity data X_org , y_org = generate_collinear_data ( cov , ** kwargs ) # add interaction terms interaction_org_1 = X_org [:, 0 ] . reshape ( - 1 , 1 ) ** 2 # x1&#94;2 interaction_org_1 = X_org [:, 1 ] . reshape ( - 1 , 1 ) * X_org [:, 2 ] . reshape ( - 1 , 1 ) # x2 * x3 X_org = np . concatenate ([ X_org , interaction_org_1 ], axis = 1 ) # VIF vifs_org = np . linalg . inv ( pd . DataFrame ( X_org ) . corr () . values ) . diagonal () # fit linear regression model X_st_org = sm . add_constant ( X_org ) model_org = sm . OLS ( y_org , X_st_org ) . fit () org_params . append ( model_org . params ) ############################################## Mean-centered ############################################## # generate high collinearity data X_std , y_std = generate_collinear_data ( cov , ** kwargs ) # mean-center independent variables scaler = StandardScaler ( with_std = False ) X_std = scaler . fit_transform ( X_std ) # add interaction terms interaction_std_1 = X_std [:, 0 ] . reshape ( - 1 , 1 ) ** 2 # x1&#94;2 interaction_std_1 = X_std [:, 1 ] . reshape ( - 1 , 1 ) * X_std [:, 2 ] . reshape ( - 1 , 1 ) # x2 * x3 X_std = np . concatenate ([ X_std , interaction_std_1 ], axis = 1 ) # VIF vifs_std = np . linalg . inv ( pd . DataFrame ( X_std ) . corr () . values ) . diagonal () # fit linear regression model X_st_std = sm . add_constant ( X_std ) model_std = sm . OLS ( y_std , X_st_std ) . fit () std_params . append ( model_std . params ) ########################################################################################################## # list to numpy conversion org_params = np . asarray ( org_params ) std_params = np . asarray ( std_params ) #################################################### Plot #################################################### # plotting def styling ( ax ): ax . set_facecolor ( '#eeeeee' ) ax . grid ( True , linestyle = '--' , color = '#acacac' ) ax . tick_params ( color = 'grey' ) _ = [ spine . set_edgecolor ( 'grey' ) for spine in ax . spines . values ()] ax . text ( 0.35 , 0.88 , 'aegis4048.github.io' , fontsize = 10.5 , ha = 'center' , va = 'center' , transform = ax . transAxes , color = 'grey' , alpha = 0.5 ) boxplot_styling = { 'sym' : '' , 'whis' : [ 2.5 , 97.5 ], 'showfliers' : False , 'vert' : True , 'capprops' : dict ( linewidth = 2.0 , color = 'k' ), 'medianprops' : dict ( linewidth = 2.0 , color = 'k' ), 'widths' : ( 0.4 ), 'patch_artist' : True , 'whiskerprops' : dict ( linewidth = 1.5 , color = 'k' , linestyle = '--' ), } #labels = ['Intercept', '$X_1$', '$X_2$', '$X_3$', '$X_4$', '$X_1&#94;2$', '$X_2X_3$'] labels = [ 'Intercept' , '$X_1$' , '$X_2$' , '$X_3$' , '$X_2X_3$' ] fig , axes = plt . subplots ( 1 , len ( labels ), figsize = ( 16 , 6 )) for i , ( ax , label ) in enumerate ( zip ( axes , labels )): bp1 = ax . boxplot ( org_params [:, i ], positions = [ 1.0 ], boxprops = dict ( linewidth = 1.5 , facecolor = '#00bfc4' ), ** boxplot_styling ) bp2 = ax . boxplot ( std_params [:, i ], positions = [ 1.7 ], boxprops = dict ( linewidth = 1.5 , facecolor = '#f8766d' ), ** boxplot_styling ) styling ( ax ) ax . set_title ( label , fontsize = 20 , y =- 0.2 ) if i != 0 : ax . set_xticklabels ([ 'VIF = {} ' . format ( int ( vifs_org [ i - 1 ])), 'VIF = {} ' . format ( int ( vifs_std [ i - 1 ]))], fontsize = 15 ) else : ax . set_xticklabels ([]) fig . legend ([ bp1 [ \"boxes\" ][ 0 ], bp2 [ \"boxes\" ][ 0 ]], [ 'Original' , 'Standardized' ], loc = 'lower center' , ncol = 3 , borderaxespad = 0.7 , fontsize = 17 ) fig . suptitle ( 'Effect of standardization for interaction terms - %.f simulations' % m , fontsize = 25 ); fig . tight_layout ( rect = [ 0 , 0.1 , 1 , 0.9 ]) In [3]: fig . savefig ( 'multicollinearity_interaction_conf_int.png' , dpi = 400 ) In [7]: model_org . summary () Out[7]: OLS Regression Results Dep. Variable: y R-squared: 1.000 Model: OLS Adj. R-squared: 1.000 Method: Least Squares F-statistic: 3.626e+05 Date: Mon, 20 Jan 2020 Prob (F-statistic): 8.60e-101 Time: 12:23:12 Log-Likelihood: -73.984 No. Observations: 50 AIC: 158.0 Df Residuals: 45 BIC: 167.5 Df Model: 4 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] const -12.7945 20.675 -0.619 0.539 -54.435 28.846 x1 11.8261 0.898 13.173 0.000 10.018 13.634 x2 1.4098 0.996 1.415 0.164 -0.597 3.417 x3 7.1377 1.967 3.628 0.001 3.175 11.100 x4 8.8867 0.114 77.621 0.000 8.656 9.117 Omnibus: 0.384 Durbin-Watson: 1.796 Prob(Omnibus): 0.825 Jarque-Bera (JB): 0.394 Skew: -0.193 Prob(JB): 0.821 Kurtosis: 2.798 Cond. No. 5.60e+03 Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 5.6e+03. This might indicate that there are strong multicollinearity or other numerical problems. In [40]: import numpy as np import statsmodels.api as sm import matplotlib.pyplot as plt def generate_collinear_data ( cov , n = 10 , true_coefs = [ 0 , 0 ], true_intercept = 0 , feature_means = [ 0 , 0 , 0 ], loc = 0 , scale = 1 ): # random generation of 3D gaussian collinear features. X = np . random . multivariate_normal ( mean = feature_means , cov = cov , size = n ) # generate gaussian white noise. gaussian_noise = np . random . normal ( loc = loc , scale = scale , size = n ) # make the outcome. y = true_intercept + gaussian_noise for i in range ( len ( true_coefs )): y += true_coefs [ i ] * X [:, i ] return X , y # simulation settings m = 100000 # number of simulations. WARNING!!! Large m makes execution time very long correlation = 0.98 # degree of collinearity (HIGH) kwargs = { 'n' : 50 , # sample size 'true_coefs' : [ 13 , 0.5 , 5 , - 1 , - 24 ], # linear regression coefficients, 2 features 'true_intercept' : 2 , # y-intercept 'feature_means' : [ 0 , 0 , 0 , 0 , 0 ], # means multivariate normal distribution. This is not important 'loc' : 0 , # mean of gaussian noise 'scale' : 1 # standard deviation of gaussian noise } # high collinearity covariance matrix cov = np . full (( len ( kwargs [ 'true_coefs' ]), len ( kwargs [ 'true_coefs' ])), correlation ) np . fill_diagonal ( cov , 1 ) ######################################### numerical solution ################################################ params = [] for i in range ( m ): X , y = generate_collinear_data ( cov , ** kwargs ) # generate high collinearity data X_st = sm . add_constant ( X ) model = sm . OLS ( y , X_st ) . fit () params . append ( model . params ) params = np . asarray ( params ) # list to numpy conversion numerical_means = np . mean ( params , axis = 0 ) # solution numerical_lo = np . percentile ( params , 2.5 , axis = 0 ) # lower bound of 95% confidence interval numerical_hi = np . percentile ( params , 97.5 , axis = 0 ) # upper bound of 95% confidence interval numerical_width = abs ( numerical_hi - numerical_lo ) # width of confidence interval ######################################### analytical solution ############################################### X , y = generate_collinear_data ( cov , ** kwargs ) # generate high collinearity data X_st = sm . add_constant ( X ) model = sm . OLS ( y , X_st ) . fit () analytical_means = model . params # solution analytical_lo = model . conf_int ( alpha = 0.05 )[:, 0 ] # lower bound of 95% confidence interval analytical_hi = model . conf_int ( alpha = 0.05 )[:, 1 ] # upper bound of 95% confidence interval analytical_width = abs ( analytical_hi - analytical_lo ) # width of confidence interval ######################################### Plotting: Error bars ############################################## def styling ( ax , label ): ax . set_xlabel ( label , fontsize = 25 ) ax . set_facecolor ( '#eeeeee' ) ax . grid ( True , linestyle = '--' , color = '#acacac' , axis = 'y' ) ax . tick_params ( color = 'grey' ) ax . set_xticklabels ([]) ax . set_xlim ( 0 , 2 ) _ = [ spine . set_edgecolor ( 'grey' ) for spine in ax . spines . values ()] ax . text ( 0.5 , 0.1 , 'aegis4048.github.io' , fontsize = 12 , ha = 'center' , va = 'center' , transform = ax . transAxes , color = 'grey' , alpha = 0.5 ) labels = [ 'Intercept' , '$X_1$' , '$X_2$' , '$X_3$' , '$X_4$' , '$X_5$' ] true_params = [ kwargs [ 'true_intercept' ]] + kwargs [ 'true_coefs' ] x = np . array ([ 1 , 2 ]) fig , axes = plt . subplots ( 1 , len ( labels ), figsize = ( 16 , 6 )) for i , ( ax , label , true_param ) in enumerate ( zip ( axes , labels , true_params )): yerr_numerical = [[ abs ( numerical_lo [ i ] - numerical_means [ i ])], [ abs ( numerical_hi [ i ] - numerical_means [ i ])]] yerr_analytical = [[ abs ( analytical_lo [ i ] - analytical_means [ i ])], [ abs ( analytical_hi [ i ] - analytical_means [ i ])]] ax . errorbar ( 1 - 0.25 , numerical_means [ i ], yerr = yerr_numerical , fmt = 'o' , label = '95% numerical C.I.' , color = 'k' , markersize = 8 , capsize = 8 , elinewidth = 2.5 , markeredgewidth = 2.5 ) ax . errorbar ( 1 + 0.25 , analytical_means [ i ], yerr = yerr_analytical , fmt = 'o' , label = '95 % a nalytical C.I.' , color = 'grey' , markersize = 8 , capsize = 8 , elinewidth = 2.5 , markeredgewidth = 2.5 ) ax . axhline ( true_param , linestyle = '-.' , color = '#ad203e' , label = 'True value' ) styling ( ax , label ) axes [ 0 ] . set_ylabel ( 'Range of 95% C.I.' , fontsize = 30 ) fig . legend ( * ax . get_legend_handles_labels (), loc = 'lower center' , ncol = 3 , borderaxespad = 0.7 , fontsize = 17 ) fig . suptitle ( 'Effect of overestimated standard errors - %.f simulations' % m , fontsize = 25 ); fig . tight_layout ( rect = [ 0 , 0.10 , 1 , 0.9 ]) ######################################### Plotting: Bar chart ############################################### x = np . array ([ i for i in range ( len ( analytical_width ))]) fig , ax = plt . subplots ( figsize = ( 16 , 6 )) ax . bar ( x - 0.15 , numerical_width , align = 'center' , alpha = 0.75 , width = 0.28 , color = 'k' , label = 'Numerical Sol.' ) ax . bar ( x + 0.15 , analytical_width , align = 'center' , width = 0.28 , color = 'grey' , label = 'Analytical Sol.' ) ax . text ( 0.1 , 0.65 , 'aegis4048.github.io' , fontsize = 17 , ha = 'center' , va = 'center' , transform = ax . transAxes , color = 'grey' , alpha = 0.5 ) ax . set_ylabel ( 'Width of 95% C.I.' , fontsize = 30 ) ax . legend ( fontsize = 17 , loc = 'upper left' ) ax . set_xticks ( x ) ax . set_xticklabels ([ str ( label ) for label in labels ], fontsize = 25 ) ax . set_facecolor ( '#eeeeee' ) ax . grid ( True , linestyle = '--' , color = '#acacac' , axis = 'y' ) ax . tick_params ( color = 'grey' ) _ = [ spine . set_edgecolor ( 'grey' ) for spine in ax . spines . values ()] fig . tight_layout () In [42]: print ( model . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.989 Model: OLS Adj. R-squared: 0.987 Method: Least Squares F-statistic: 770.4 Date: Mon, 20 Jan 2020 Prob (F-statistic): 1.20e-41 Time: 11:52:32 Log-Likelihood: -62.903 No. Observations: 50 AIC: 137.8 Df Residuals: 44 BIC: 149.3 Df Model: 5 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 1.9780 0.140 14.088 0.000 1.695 2.261 x1 11.7612 0.822 14.302 0.000 10.104 13.419 x2 -0.1994 0.927 -0.215 0.831 -2.069 1.670 x3 5.9606 0.919 6.483 0.000 4.108 7.814 x4 0.0837 0.879 0.095 0.925 -1.687 1.855 x5 -24.1338 0.749 -32.233 0.000 -25.643 -22.625 ============================================================================== Omnibus: 1.679 Durbin-Watson: 2.465 Prob(Omnibus): 0.432 Jarque-Bera (JB): 1.297 Skew: -0.394 Prob(JB): 0.523 Kurtosis: 2.976 Cond. No. 20.2 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [24]: import numpy as np import statsmodels.api as sm import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler import pandas as pd def generate_collinear_data ( cov , n = 10 , true_coefs = [ 0 , 0 ], true_intercept = 0 , feature_means = [ 0 , 0 , 0 ], loc = 0 , scale = 1 ): # random generation of 3D gaussian collinear features. X = np . random . multivariate_normal ( mean = feature_means , cov = cov , size = n ) # generate gaussian white noise. gaussian_noise = np . random . normal ( loc = loc , scale = scale , size = n ) # make the outcome. y = true_intercept + gaussian_noise for i in range ( len ( true_coefs )): y += true_coefs [ i ] * X [:, i ] return X , y # settings m = 1000 # number of simulations correlation = 0.1 # degree of collinearity (HIGH) kwargs = { 'n' : 1000 , # sample size 'true_coefs' : [ 13 , 0.5 , 5 , 42 ], # linear regression coefficients, 2 features 'true_intercept' : 15 , # y-intercept 'feature_means' : [ - 12 , 14 , 2 , - 20 ], # means multivariate normal distribution. This is not important 'loc' : 0 , # mean of gaussian noise 'scale' : 1 # standard deviation of gaussian noise } # high collinearity covariance matrix cov = np . full (( len ( kwargs [ 'true_coefs' ]), len ( kwargs [ 'true_coefs' ])), correlation ) np . fill_diagonal ( cov , 1 ) # simulation org_params = [] std_params = [] for i in range ( m ): ################################################ Original ################################################ # generate high collinearity data X_org , y_org = generate_collinear_data ( cov , ** kwargs ) # add interaction terms interaction_org_1 = X_org [:, 0 ] . reshape ( - 1 , 1 ) ** 2 # x1&#94;2 interaction_org_2 = X_org [:, 1 ] . reshape ( - 1 , 1 ) * X_org [:, 2 ] . reshape ( - 1 , 1 ) # x2 * x3 X_org = np . concatenate ([ X_org , interaction_org_1 , interaction_org_2 ], axis = 1 ) # VIF vifs_org = np . linalg . inv ( pd . DataFrame ( X_org ) . corr () . values ) . diagonal () # fit linear regression model X_st_org = sm . add_constant ( X_org ) model_org = sm . OLS ( y_org , X_st_org ) . fit () org_params . append ( model_org . params ) ############################################## Standardized ############################################## # generate high collinearity data X_std , y_std = generate_collinear_data ( cov , ** kwargs ) # standardize scaler = StandardScaler ( with_std = False ) X_std , y_std = scaler . fit_transform ( X_std ), scaler . fit_transform ( y_std . reshape ( - 1 , 1 )) . flatten () # add interaction terms interaction_std_1 = X_std [:, 0 ] . reshape ( - 1 , 1 ) ** 2 # x1&#94;2 interaction_std_2 = X_std [:, 1 ] . reshape ( - 1 , 1 ) * X_std [:, 2 ] . reshape ( - 1 , 1 ) # x2 * x3 X_std = np . concatenate ([ X_std , interaction_std_1 , interaction_std_2 ], axis = 1 ) # VIF vifs_std = np . linalg . inv ( pd . DataFrame ( X_std ) . corr () . values ) . diagonal () # fit linear regression model X_st_std = sm . add_constant ( X_std ) model_std = sm . OLS ( y_std , X_st_std ) . fit () std_params . append ( model_std . params ) ########################################################################################################## # list to numpy conversion org_params = np . asarray ( org_params ) std_params = np . asarray ( std_params ) # plotting def styling ( ax ): ax . set_facecolor ( '#eeeeee' ) ax . grid ( True , linestyle = '--' , color = '#acacac' ) ax . tick_params ( color = 'grey' ) _ = [ spine . set_edgecolor ( 'grey' ) for spine in ax . spines . values ()] ax . text ( 0.5 , 0.1 , 'aegis4048.github.io' , fontsize = 10.5 , ha = 'center' , va = 'center' , transform = ax . transAxes , color = 'grey' , alpha = 0.5 ) boxplot_styling = { 'sym' : '' , 'whis' : [ 2.5 , 97.5 ], 'showfliers' : False , 'vert' : True , 'capprops' : dict ( linewidth = 2.0 , color = 'k' ), 'medianprops' : dict ( linewidth = 2.0 , color = 'k' ), 'widths' : ( 0.4 ), 'patch_artist' : True , 'whiskerprops' : dict ( linewidth = 1.5 , color = 'k' , linestyle = '--' ), } labels = [ 'Intercept' , '$X_1$' , '$X_2$' , '$X_3$' , '$X_4$' , '$X_1&#94;2$' , '$X_2X_3$' ] fig , axes = plt . subplots ( 1 , len ( labels ), figsize = ( 16 , 6 )) for i , ( ax , label ) in enumerate ( zip ( axes , labels )): bp1 = ax . boxplot ( org_params [:, i ], positions = [ 1.0 ], boxprops = dict ( linewidth = 1.5 , facecolor = '#00bfc4' ), ** boxplot_styling ) bp2 = ax . boxplot ( std_params [:, i ], positions = [ 1.7 ], boxprops = dict ( linewidth = 1.5 , facecolor = '#f8766d' ), ** boxplot_styling ) styling ( ax ) ax . set_title ( label , fontsize = 20 , y =- 0.2 ) if i != 0 : ax . set_xticklabels ([ 'VIF = {} ' . format ( int ( vifs_org [ i - 1 ])), 'VIF = {} ' . format ( int ( vifs_std [ i - 1 ]))], fontsize = 9 ) else : ax . set_xticklabels ([]) fig . legend ([ bp1 [ \"boxes\" ][ 0 ], bp2 [ \"boxes\" ][ 0 ]], [ 'Original' , 'Standardized' ], loc = 'lower center' , ncol = 3 , borderaxespad = 0.7 , fontsize = 17 ) fig . suptitle ( 'Effect of standardization for interaction terms - %.f simulations' % m , fontsize = 25 ); fig . tight_layout ( rect = [ 0 , 0.1 , 1 , 0.9 ]) In [25]: # generate high collinearity data X_org , y_org = generate_collinear_data ( cov , ** kwargs ) # add interaction terms interaction_org_1 = X_org [:, 0 ] . reshape ( - 1 , 1 ) ** 2 # x1&#94;2 interaction_org_2 = X_org [:, 1 ] . reshape ( - 1 , 1 ) * X_org [:, 2 ] . reshape ( - 1 , 1 ) # x2 * x3 X_org = np . concatenate ([ X_org , interaction_org_1 , interaction_org_2 ], axis = 1 ) # VIF vifs_org = np . linalg . inv ( pd . DataFrame ( X_org ) . corr () . values ) . diagonal () # fit linear regression model X_st_org = sm . add_constant ( X_org ) model_org = sm . OLS ( y_org , X_st_org ) . fit () #org_params.append(model_org.params) In [26]: print ( model_org . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 1.000 Model: OLS Adj. R-squared: 1.000 Method: Least Squares F-statistic: 3.520e+05 Date: Mon, 20 Jan 2020 Prob (F-statistic): 0.00 Time: 11:33:18 Log-Likelihood: -1411.5 No. Observations: 1000 AIC: 2837. Df Residuals: 993 BIC: 2871. Df Model: 6 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 15.6712 3.279 4.779 0.000 9.236 22.107 x1 13.1963 0.508 25.992 0.000 12.200 14.193 x2 0.5118 0.068 7.537 0.000 0.379 0.645 x3 4.9559 0.433 11.442 0.000 4.106 5.806 x4 41.9942 0.032 1304.298 0.000 41.931 42.057 x5 0.0088 0.021 0.418 0.676 -0.033 0.050 x6 0.0046 0.031 0.149 0.881 -0.056 0.065 ============================================================================== Omnibus: 0.167 Durbin-Watson: 2.038 Prob(Omnibus): 0.920 Jarque-Bera (JB): 0.097 Skew: -0.013 Prob(JB): 0.953 Kurtosis: 3.040 Cond. No. 1.60e+04 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 1.6e+04. This might indicate that there are strong multicollinearity or other numerical problems. In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [53]: kwargs = { 'n' : 10000 , # sample size 'true_coefs' : [ 13 , 0.5 , 5 , 42 ], # linear regression coefficients, 2 features 'true_intercept' : 15 , # y-intercept 'feature_means' : [ - 12 , 14 , 2 , - 20 ], # means multivariate normal distribution. This is not important 'loc' : 0 , # mean of gaussian noise 'scale' : 1 , # standard deviation of gaussian noise 'true_coefs_interaction' : [ - 2 , 9 ] # linear regression coefficients of interaction terms } In [4]: import numpy as np import statsmodels.api as sm import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler import pandas as pd def generate_collinear_data ( cov , n = 10 , true_coefs = [ 0 , 0 ], true_intercept = 0 , feature_means = [ 0 , 0 , 0 ], loc = 0 , scale = 1 , true_coefs_interaction = [ 0 , 0 ]): # random generation of 3D gaussian collinear features. X = np . random . multivariate_normal ( mean = feature_means , cov = cov , size = n ) # generate gaussian white noise. gaussian_noise = np . random . normal ( loc = loc , scale = scale , size = n ) # make the outcome. y = true_intercept + gaussian_noise for i in range ( len ( true_coefs )): y += true_coefs [ i ] * X [:, i ] y += true_coefs_interaction [ 0 ] * ( X [:, 0 ] ** 2 ) + true_coefs_interaction [ 1 ] * X [:, 1 ] * X [:, 2 ] return X , y # settings m = 1000 # number of simulations correlation = 0.98 # degree of collinearity (HIGH) kwargs = { 'n' : 10000 , # sample size 'true_coefs' : [ 13 , 0.5 , 5 , 42 ], # linear regression coefficients, 2 features 'true_intercept' : 15 , # y-intercept 'feature_means' : [ - 12 , 14 , 2 , - 20 ], # means multivariate normal distribution. This is not important 'loc' : 0 , # mean of gaussian noise 'scale' : 1 , # standard deviation of gaussian noise 'true_coefs_interaction' : [ - 2 , 9 ] # linear regression coefficients of interaction terms } # high collinearity covariance matrix cov = np . full (( len ( kwargs [ 'true_coefs' ]), len ( kwargs [ 'true_coefs' ])), correlation ) np . fill_diagonal ( cov , 1 ) X_org , y_org = generate_collinear_data ( cov , ** kwargs ) interaction_org_1 = X_org [:, 0 ] . reshape ( - 1 , 1 ) ** 2 # x1&#94;2 interaction_org_2 = X_org [:, 1 ] . reshape ( - 1 , 1 ) * X_org [:, 2 ] . reshape ( - 1 , 1 ) # x2 * x3 X_org = np . concatenate ([ X_org , interaction_org_1 , interaction_org_2 ], axis = 1 ) X_st_org = sm . add_constant ( X_org ) model_org = sm . OLS ( y_org , X_st_org ) . fit () print ([ ' %.2f ' % elem for elem in model_org . params ]) X_std , y_std = generate_collinear_data ( cov , ** kwargs ) # centering scaler = StandardScaler ( with_std = False ) X_std , y_std = scaler . fit_transform ( X_std ), scaler . fit_transform ( y_std . reshape ( - 1 , 1 )) . flatten () # add interaction terms interaction_std_1 = X_std [:, 0 ] . reshape ( - 1 , 1 ) ** 2 # x1&#94;2 interaction_std_2 = X_std [:, 1 ] . reshape ( - 1 , 1 ) * X_std [:, 2 ] . reshape ( - 1 , 1 ) # x2 * x3 X_std = np . concatenate ([ X_std , interaction_std_1 , interaction_std_2 ], axis = 1 ) # fit X_st_std = sm . add_constant ( X_std ) model_std = sm . OLS ( y_std , X_st_std ) . fit () print ([ ' %.2f ' % elem for elem in model_std . params ]) ['16.10', '14.03', '0.78', '5.73', '41.96', '-1.95', '8.94'] ['-6.94', '60.97', '18.50', '130.88', '42.04', '-2.02', '9.01'] In [4]: pd . DataFrame ( X_std ) . describe () Out[4]: 0 1 2 3 count 1.000000e+04 1.000000e+04 1.000000e+04 1.000000e+04 mean 1.941949e-14 6.028493e-14 1.024576e-14 -8.558416e-14 std 9.999832e-01 1.000894e+00 1.001148e+00 9.989619e-01 min -3.798131e+00 -3.658011e+00 -3.961630e+00 -3.853892e+00 25% -6.674661e-01 -6.768570e-01 -6.702250e-01 -6.601235e-01 50% 1.901851e-03 -2.473357e-03 -1.843201e-03 3.266154e-03 75% 6.722287e-01 6.793375e-01 6.758215e-01 6.720686e-01 max 3.681066e+00 3.746267e+00 3.736197e+00 3.648933e+00 In [ ]: In [ ]: In [4]: import numpy as np import statsmodels.api as sm import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler import pandas as pd ######################################## Collinear random data generation ######################################### def generate_collinear_data ( cov , n = 10 , true_coefs = [ 0 , 0 ], true_intercept = 0 , feature_means = [ 0 , 0 , 0 ], loc = 0 , scale = 1 , true_coefs_interaction = [ 0 , 0 ]): # random generation of 3D gaussian collinear features. X = np . random . multivariate_normal ( mean = feature_means , cov = cov , size = n ) # generate gaussian white noise. gaussian_noise = np . random . normal ( loc = loc , scale = scale , size = n ) # make the outcome. y = true_intercept + gaussian_noise for i in range ( len ( true_coefs )): y += true_coefs [ i ] * X [:, i ] y += true_coefs_interaction [ 0 ] * ( X [:, 0 ] ** 2 ) + true_coefs_interaction [ 1 ] * X [:, 1 ] * X [:, 2 ] return X , y # settings m = 1000 # number of simulations correlation = 0.01 # degree of collinearity (HIGH) kwargs = { 'n' : 10000 , # sample size 'true_coefs' : [ 13 , 0.5 , 5 , 42 ], # linear regression coefficients, 2 features 'true_intercept' : 15 , # y-intercept 'feature_means' : [ - 12 , 14 , 2 , - 20 ], # means multivariate normal distribution. This is not important 'loc' : 0 , # mean of gaussian noise 'scale' : 1 , # standard deviation of gaussian noise 'true_coefs_interaction' : [ - 2 , 9 ] # linear regression coefficients of interaction terms } # high collinearity covariance matrix cov = np . full (( len ( kwargs [ 'true_coefs' ]), len ( kwargs [ 'true_coefs' ])), correlation ) np . fill_diagonal ( cov , 1 ) print ( 'True intercept + coefficients: ' , [ kwargs [ 'true_intercept' ]] + kwargs [ 'true_coefs' ] + kwargs [ 'true_coefs_interaction' ]) print () ############################################# Original ############################################# X_org , y_org = generate_collinear_data ( cov , ** kwargs ) interaction_org_1 = X_org [:, 0 ] . reshape ( - 1 , 1 ) ** 2 # x1&#94;2 interaction_org_2 = X_org [:, 1 ] . reshape ( - 1 , 1 ) * X_org [:, 2 ] . reshape ( - 1 , 1 ) # x2 * x3 X_org = np . concatenate ([ X_org , interaction_org_1 , interaction_org_2 ], axis = 1 ) X_st_org = sm . add_constant ( X_org ) model_org = sm . OLS ( y_org , X_st_org ) . fit () print ( 'Original: ' , [ ' %.2f ' % elem for elem in model_org . params ]) ########################################## Mean-centered ########################################### X_std , y_std = generate_collinear_data ( cov , ** kwargs ) # centering scaler = StandardScaler ( with_mean = True , with_std = False ) X_std , y_std = scaler . fit_transform ( X_std ), scaler . fit_transform ( y_std . reshape ( - 1 , 1 )) . flatten () # add interaction terms interaction_std_1 = X_std [:, 0 ] . reshape ( - 1 , 1 ) ** 2 # x1&#94;2 interaction_std_2 = X_std [:, 1 ] . reshape ( - 1 , 1 ) * X_std [:, 2 ] . reshape ( - 1 , 1 ) # x2 * x3 X_std = np . concatenate ([ X_std , interaction_std_1 , interaction_std_2 ], axis = 1 ) # fit X_std = sm . add_constant ( X_std ) model_std = sm . OLS ( y_std , X_std ) . fit () print ( 'Centering: ' , [ ' %.2f ' % elem for elem in model_std . params ]) True intercept + coefficients: [15, 13, 0.5, 5, 42, -2, 9] Original: ['16.32', '13.32', '0.54', '5.10', '41.99', '-1.99', '8.99'] Centering: ['1.95', '60.97', '18.65', '131.01', '42.02', '-1.99', '9.00'] In [48]: pd . DataFrame ( X_std ) . describe () Out[48]: 0 1 2 3 4 5 6 count 10000.0 1.000000e+04 1.000000e+04 1.000000e+04 1.000000e+04 10000.0 10000.000000 mean 0.0 -2.464642e-14 6.360246e-15 -3.808731e-15 -7.968453e-14 0.0 0.981738 std 0.0 1.000702e+00 1.001144e+00 9.978938e-01 9.985619e-01 0.0 1.391677 min 0.0 -3.593256e+00 -3.634766e+00 -3.633980e+00 -3.655352e+00 0.0 -0.140977 25% 0.0 -6.714428e-01 -6.705416e-01 -6.793161e-01 -6.868794e-01 0.0 0.094322 50% 0.0 1.060143e-02 -3.124346e-03 2.171820e-03 8.096544e-03 0.0 0.441181 75% 0.0 6.784886e-01 6.791995e-01 6.747566e-01 6.819694e-01 0.0 1.311172 max 0.0 4.005803e+00 4.106618e+00 4.215257e+00 3.983645e+00 0.0 16.450301 In [22]: df_vif_inter Out[22]: Features VIF 0 Por 4.530729 1 Perm 2.970717 2 AI 7.695542 3 Brittle 2.166582 4 TOC 7.404098 5 VR 7.096311 6 Perm $\\cdot$ AI 1.297737 7 Por$&#94;2$ 1.385846 In [ ]: In [1]: fig . savefig ( 'multicollinearity_standardization_remedy.png' , dpi = 400 ) --------------------------------------------------------------------------- NameError Traceback (most recent call last) in () ----> 1 fig . savefig ( 'multicollinearity_standardization_remedy.png' , dpi = 400 ) NameError : name 'fig' is not defined In [7]: pd . DataFrame ( size_std_params ) . describe () Out[7]: 0 1 2 3 4 5 6 count 1000.000000 1000.000000 1000.000000 1000.000000 1000.000000 1000.000000 1000.000000 mean -0.000146 13.002801 0.507841 5.000135 41.989179 0.003125 -0.003073 std 0.022944 0.195180 0.197242 0.193441 0.192566 0.092238 0.092788 min -0.081821 12.294589 -0.128880 4.436567 41.257252 -0.318026 -0.253208 25% -0.014069 12.880095 0.378706 4.864476 41.862452 -0.055291 -0.067980 50% -0.000085 12.999420 0.510451 4.997171 41.987201 0.002685 -0.003937 75% 0.014928 13.125545 0.638934 5.134087 42.115930 0.069961 0.058610 max 0.075064 13.621075 1.059333 5.610283 42.577125 0.255312 0.281129 In [8]: pd . DataFrame ( size_org_params ) . describe () Out[8]: 0 1 2 3 4 5 6 count 1000.000000 1000.000000 1000.000000 1000.000000 1000.000000 1000.000000 1000.000000 mean 14.748528 12.930296 0.489794 4.957703 42.002232 -0.002818 0.003283 std 12.387985 2.220570 0.266521 1.325275 0.194056 0.092248 0.093669 min -30.154770 4.770519 -0.344287 0.303128 41.476067 -0.338942 -0.336029 25% 6.069846 11.484334 0.303922 4.087688 41.867228 -0.063095 -0.057960 50% 14.784708 12.884449 0.503656 4.964886 42.002738 -0.004521 0.002031 75% 22.961767 14.346504 0.667748 5.799433 42.132009 0.057205 0.065686 max 52.831490 20.078070 1.316792 9.790690 42.684032 0.293824 0.354703 In [3]: X_std Out[3]: array([[ 1.39789552e+00, 1.32063489e+00, 1.24617065e+00, 1.47789766e+00, 1.95411188e+00, 1.64573644e+00], [-2.57071097e+00, -2.71342896e+00, -2.60974932e+00, -2.76383754e+00, 6.60855489e+00, 7.08136940e+00], [-4.44746527e-01, -1.03586546e+00, -9.12895425e-01, -8.41039842e-01, 1.97799474e-01, 9.45636838e-01], ..., [ 1.72236080e+00, 2.03980480e+00, 1.75588729e+00, 1.68061094e+00, 2.96652672e+00, 3.58166732e+00], [ 1.52632151e+00, 1.60617507e+00, 1.49994393e+00, 1.43156500e+00, 2.32965735e+00, 2.40917255e+00], [ 1.96864983e-02, -1.59830987e-01, -3.22168164e-04, 1.21220584e-01, 3.87558215e-04, 5.14924556e-05]]) In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [10]: import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler ############################################# import data ############################################# # data source: https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v5.csv file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' features = [ 'Por' , 'Perm' , 'AI' , 'Brittle' , 'TOC' , 'VR' ] df = pd . read_csv ( file ) df = df [ features ] ####################################### VIFs with original data ####################################### vifs_orig = np . linalg . inv ( df . corr () . values ) . diagonal () df_vif_orig = pd . DataFrame () df_vif_orig [ 'Features' ] = df . columns df_vif_orig [ 'VIF' ] = vifs_orig ##################################### VIFs with interaction terms ##################################### df [ 'Perm $\\cdot$ AI' ] = df [ 'Perm' ] . values * df [ 'AI' ] . values df [ 'Por$&#94;2$' ] = df [ 'Por' ] . values ** 2 vifs_inter = np . linalg . inv ( df . corr () . values ) . diagonal () df_vif_inter = pd . DataFrame () df_vif_inter [ 'Features' ] = df . columns df_vif_inter [ 'VIF' ] = vifs_inter print ( df_vif_orig ) print () print ( df_vif_inter ) ##################################### Mean-centered ##################################### scaler = StandardScaler ( with_std = False ) df_st = pd . DataFrame ( scaler . fit_transform ( df ), columns = df . columns ) df_st [ 'Perm $\\cdot$ AI' ] = df_st [ 'Perm' ] . values * df_st [ 'AI' ] . values df_st [ 'Por$&#94;2$' ] = df_st [ 'Por' ] . values ** 2 vifs_st = np . linalg . inv ( df_st . corr () . values ) . diagonal () df_vif_st = pd . DataFrame () df_vif_st [ 'Features' ] = df_st . columns df_vif_st [ 'VIF' ] = vifs_st df_vif_st Features VIF 0 Por 4.329838 1 Perm 2.862487 2 AI 7.660318 3 Brittle 2.165476 4 TOC 7.365773 5 VR 7.057671 Features VIF 0 Por 69.662331 1 Perm 40.943222 2 AI 14.864118 3 Brittle 2.166582 4 TOC 7.404098 5 VR 7.096311 6 Perm $\\cdot$ AI 36.819112 7 Por$&#94;2$ 71.261566 Out[10]: Features VIF 0 Por 4.530729 1 Perm 2.970717 2 AI 7.695542 3 Brittle 2.166582 4 TOC 7.404098 5 VR 7.096311 6 Perm $\\cdot$ AI 1.297737 7 Por$&#94;2$ 1.385846 In [ ]: In [19]: import numpy as np import statsmodels.api as sm import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler import pandas as pd def generate_collinear_data ( cov , n = 10 , true_coefs = [ 0 , 0 ], true_intercept = 0 , feature_means = [ 0 , 0 , 0 ], loc = 0 , scale = 1 ): # random generation of 3D gaussian collinear features. X = np . random . multivariate_normal ( mean = feature_means , cov = cov , size = n ) # generate gaussian white noise. gaussian_noise = np . random . normal ( loc = loc , scale = scale , size = n ) # make the outcome. y = true_intercept + gaussian_noise for i in range ( len ( true_coefs )): y += true_coefs [ i ] * X [:, i ] return X , y # settings m = 10000 # number of simulations correlation = 0.99 # degree of collinearity (HIGH) kwargs = { 'n' : 1000 , # sample size 'true_coefs' : [ 13 , 0.5 , 5 , 42 ], # linear regression coefficients, 2 features 'true_intercept' : 15 , # y-intercept 'feature_means' : [ - 12 , 14 , 2 , - 20 ], # means multivariate normal distribution. This is not important 'loc' : 0 , # mean of gaussian noise 'scale' : 1 # standard deviation of gaussian noise } # high collinearity covariance matrix cov = np . full (( len ( kwargs [ 'true_coefs' ]), len ( kwargs [ 'true_coefs' ])), correlation ) np . fill_diagonal ( cov , 1 ) # simulation org_params = [] std_params = [] for i in range ( m ): ################################################ Original ################################################ # generate high collinearity data X_org , y_org = generate_collinear_data ( cov , ** kwargs ) # add interaction terms interaction_org_1 = X_org [:, 0 ] . reshape ( - 1 , 1 ) ** 2 # x1&#94;2 interaction_org_2 = X_org [:, 1 ] . reshape ( - 1 , 1 ) * X_org [:, 2 ] . reshape ( - 1 , 1 ) # x2 * x3 X_org = np . concatenate ([ X_org , interaction_org_1 , interaction_org_2 ], axis = 1 ) # VIF vifs_org = np . linalg . inv ( pd . DataFrame ( X_org ) . corr () . values ) . diagonal () # fit linear regression model X_st_org = sm . add_constant ( X_org ) model_org = sm . OLS ( y_org , X_st_org ) . fit () org_params . append ( model_org . params ) ############################################## Standardized ############################################## # generate high collinearity data X_std , y_std = generate_collinear_data ( cov , ** kwargs ) # standardize scaler = StandardScaler ( with_std = False ) X_std , y_std = scaler . fit_transform ( X_std ), scaler . fit_transform ( y_std . reshape ( - 1 , 1 )) . flatten () # add interaction terms interaction_std_1 = X_std [:, 0 ] . reshape ( - 1 , 1 ) ** 2 # x1&#94;2 interaction_std_2 = X_std [:, 1 ] . reshape ( - 1 , 1 ) * X_std [:, 2 ] . reshape ( - 1 , 1 ) # x2 * x3 X_std = np . concatenate ([ X_std , interaction_std_1 , interaction_std_2 ], axis = 1 ) # VIF vifs_std = np . linalg . inv ( pd . DataFrame ( X_std ) . corr () . values ) . diagonal () # fit linear regression model X_st_std = sm . add_constant ( X_std ) model_std = sm . OLS ( y_std , X_st_std ) . fit () std_params . append ( model_std . params ) ########################################################################################################## # list to numpy conversion org_params = np . asarray ( org_params ) std_params = np . asarray ( std_params ) # plotting def styling ( ax ): ax . set_facecolor ( '#eeeeee' ) ax . grid ( True , linestyle = '--' , color = '#acacac' ) ax . tick_params ( color = 'grey' ) _ = [ spine . set_edgecolor ( 'grey' ) for spine in ax . spines . values ()] ax . text ( 0.5 , 0.1 , 'aegis4048.github.io' , fontsize = 10.5 , ha = 'center' , va = 'center' , transform = ax . transAxes , color = 'grey' , alpha = 0.5 ) boxplot_styling = { 'sym' : '' , 'whis' : [ 2.5 , 97.5 ], 'showfliers' : False , 'vert' : True , 'capprops' : dict ( linewidth = 2.0 , color = 'k' ), 'medianprops' : dict ( linewidth = 2.0 , color = 'k' ), 'widths' : ( 0.4 ), 'patch_artist' : True , 'whiskerprops' : dict ( linewidth = 1.5 , color = 'k' , linestyle = '--' ), } labels = [ 'Intercept' , '$X_1$' , '$X_2$' , '$X_3$' , '$X_4$' , '$X_1&#94;2$' , '$X_2X_3$' ] fig , axes = plt . subplots ( 1 , len ( labels ), figsize = ( 16 , 6 )) for i , ( ax , label ) in enumerate ( zip ( axes , labels )): bp1 = ax . boxplot ( org_params [:, i ], positions = [ 1.0 ], boxprops = dict ( linewidth = 1.5 , facecolor = '#00bfc4' ), ** boxplot_styling ) bp2 = ax . boxplot ( std_params [:, i ], positions = [ 1.7 ], boxprops = dict ( linewidth = 1.5 , facecolor = '#f8766d' ), ** boxplot_styling ) styling ( ax ) ax . set_title ( label , fontsize = 20 , y =- 0.2 ) if i != 0 : ax . set_xticklabels ([ 'VIF = {} ' . format ( int ( vifs_org [ i - 1 ])), 'VIF = {} ' . format ( int ( vifs_std [ i - 1 ]))], fontsize = 9 ) else : ax . set_xticklabels ([]) fig . legend ([ bp1 [ \"boxes\" ][ 0 ], bp2 [ \"boxes\" ][ 0 ]], [ 'Original' , 'Standardized' ], loc = 'lower center' , ncol = 3 , borderaxespad = 0.7 , fontsize = 17 ) fig . suptitle ( 'Effect of standardization for interaction terms - %.f simulations' % m , fontsize = 25 ); fig . tight_layout ( rect = [ 0 , 0.1 , 1 , 0.9 ]) In [9]: import numpy as np import statsmodels.api as sm import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler import pandas as pd def generate_collinear_data ( cov , n = 10 , true_coefs = [ 0 , 0 ], true_intercept = 0 , feature_means = [ 0 , 0 , 0 ], loc = 0 , scale = 1 ): # random generation of 3D gaussian collinear features. X = np . random . multivariate_normal ( mean = feature_means , cov = cov , size = n ) # generate gaussian white noise. gaussian_noise = np . random . normal ( loc = loc , scale = scale , size = n ) # make the outcome. y = true_intercept + gaussian_noise for i in range ( len ( true_coefs )): y += true_coefs [ i ] * X [:, i ] return X , y # settings m = 1000 # number of simulations correlation = 0.97 # degree of collinearity (HIGH) kwargs = { 'n' : 100 , # sample size 'true_coefs' : [ 13 , 0.5 , 5 , 42 ], # linear regression coefficients, 2 features 'true_intercept' : 15 , # y-intercept 'feature_means' : [ - 12 , 14 , 2 , - 20 ], # means multivariate normal distribution. This is not important 'loc' : 0 , # mean of gaussian noise 'scale' : 1 # standard deviation of gaussian noise } # high collinearity covariance matrix cov = np . full (( len ( kwargs [ 'true_coefs' ]), len ( kwargs [ 'true_coefs' ])), correlation ) np . fill_diagonal ( cov , 1 ) # simulation org_params = [] std_params = [] for i in range ( m ): ################################################ Original ################################################ # generate high collinearity data X_org , y_org = generate_collinear_data ( cov , ** kwargs ) # VIF vifs_org = np . linalg . inv ( pd . DataFrame ( X_org ) . corr () . values ) . diagonal () # fit linear regression model X_st_org = sm . add_constant ( X_org ) model_org = sm . OLS ( y_org , X_st_org ) . fit () org_params . append ( model_org . params ) ############################################## Standardized ############################################## # generate high collinearity data X_std , y_std = generate_collinear_data ( cov , ** kwargs ) # standardize scaler = StandardScaler ( with_std = False ) X_std , y_std = scaler . fit_transform ( X_std ), scaler . fit_transform ( y_std . reshape ( - 1 , 1 )) . flatten () # VIF vifs_std = np . linalg . inv ( pd . DataFrame ( X_std ) . corr () . values ) . diagonal () # fit linear regression model X_st_std = sm . add_constant ( X_std ) model_std = sm . OLS ( y_std , X_st_std ) . fit () std_params . append ( model_std . params ) ########################################################################################################## # list to numpy conversion org_params = np . asarray ( org_params ) std_params = np . asarray ( std_params ) # plotting def styling ( ax ): ax . set_facecolor ( '#eeeeee' ) ax . grid ( True , linestyle = '--' , color = '#acacac' ) ax . tick_params ( color = 'grey' ) _ = [ spine . set_edgecolor ( 'grey' ) for spine in ax . spines . values ()] ax . text ( 0.5 , 0.1 , 'aegis4048.github.io' , fontsize = 10.5 , ha = 'center' , va = 'center' , transform = ax . transAxes , color = 'grey' , alpha = 0.5 ) boxplot_styling = { 'sym' : '' , 'whis' : [ 2.5 , 97.5 ], 'showfliers' : False , 'vert' : True , 'capprops' : dict ( linewidth = 2.0 , color = 'k' ), 'medianprops' : dict ( linewidth = 2.0 , color = 'k' ), 'widths' : ( 0.4 ), 'patch_artist' : True , 'whiskerprops' : dict ( linewidth = 1.5 , color = 'k' , linestyle = '--' ), } labels = [ 'Intercept' , '$X_1$' , '$X_2$' , '$X_3$' , '$X_4$' ] fig , axes = plt . subplots ( 1 , len ( labels ), figsize = ( 16 , 6 )) for i , ( ax , label ) in enumerate ( zip ( axes , labels )): bp1 = ax . boxplot ( org_params [:, i ], positions = [ 1.0 ], boxprops = dict ( linewidth = 1.5 , facecolor = '#00bfc4' ), ** boxplot_styling ) bp2 = ax . boxplot ( std_params [:, i ], positions = [ 1.7 ], boxprops = dict ( linewidth = 1.5 , facecolor = '#f8766d' ), ** boxplot_styling ) styling ( ax ) ax . set_title ( label , fontsize = 20 , y =- 0.2 ) if i != 0 : ax . set_xticklabels ([ 'VIF = {} ' . format ( int ( vifs_org [ i - 1 ])), 'VIF = {} ' . format ( int ( vifs_std [ i - 1 ]))], fontsize = 9 ) else : ax . set_xticklabels ([]) fig . legend ([ bp1 [ \"boxes\" ][ 0 ], bp2 [ \"boxes\" ][ 0 ]], [ 'Original' , 'Standardized' ], loc = 'lower center' , ncol = 3 , borderaxespad = 0.7 , fontsize = 17 ) fig . suptitle ( 'Effect of standardization for interaction terms - %.f simulations' % m , fontsize = 25 ); fig . tight_layout ( rect = [ 0 , 0.1 , 1 , 0.9 ]) 5.3 Elimination Elimination means to drop collinear features to remove multicollinearity. However, However, this technique requires expertise in each features, or understanding of feature ranking techniques. This is a two step process: rank -> detect If you have small number of features, try multiple different combinations of features until you get stable coefficient values. Removing the predictor with the highest VIF won't necessarily remove all the multicollinearity, but it is a recommended procedure. 5.4 Principle component regression Ridge Ridge regression allows you to analyze data even when severe multicollinearity is present and helps prevent overfitting. This type of model reduces the large, problematic variance that multicollinearity causes by introducing a slight bias in the estimates. The procedure trades away much of the variance in exchange for a little bias, which produces more useful coefficient estimates when multicollinearity is present. Bias vs Vairance trade off Bias due to elimination vs multicollinearity precision Lasso Lasso regression (least absolute shrinkage and selection operator) performs variable selection that aims to increase prediction accuracy by identifying a simpler model. It is similar to Ridge regression but with variable selection. Linearly combine Confounding variables https://statisticsbyjim.com/regression/confounding-variables-bias/ This correlation structure causes confounding variables that are not in the model to bias the estimates that appear in your regression results. For example, removing either X variable will bias the other X variable. When I fit a regression model with only activity, the model had to attribute both opposing effects to activity alone. Hence, the zero correlation. However, when I fit the model with both activity and weight, it could assign the opposing effects to each variable separately. Pythonic Tip: In [13]: import pandas as pd import numpy as np from sklearn import linear_model import matplotlib.pyplot as plt file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) features = [ 'Por' ] target = 'Prod' X = df [ features ] . values . reshape ( - 1 , len ( features )) y = df [ target ] ols = linear_model . LinearRegression () model = ols . fit ( X , y ) model_pred = model . predict ( X ) In [29]: res = y - model_pred res = res . values . reshape ( - 1 , 1 ) In [16]: plt . scatter ( X , res ) Out[16]: In [44]: temp = np . concatenate ([ X , res ], axis = 1 ) df = pd . DataFrame ( temp ) df . corr () Out[44]: 0 1 0 1.000000e+00 1.123793e-15 1 1.123793e-15 1.000000e+00 LASSO or Ridge Regression In [ ]: In [91]: import numpy as np import statsmodels.api as sm import matplotlib.pyplot as plt from statsmodels.stats.outliers_influence import variance_inflation_factor from statsmodels.tools.tools import add_constant def generate_collinear_data ( cov , n = 10 , true_coefs = [ 0 , 0 ], true_intercept = 0 , feature_means = [ 0 , 0 , 0 ], loc = 0 , scale = 1 ): # random generation of 3D gaussian collinear features. X = np . random . multivariate_normal ( mean = feature_means , cov = cov , size = n ) # generate gaussian white noise. gaussian_noise = np . random . normal ( loc = loc , scale = scale , size = n ) # make the outcome. y = true_intercept + gaussian_noise for i in range ( len ( true_coefs )): y += true_coefs [ i ] * X [:, i ] return X , y # settings m = 10000 # number of simulations correlation_lo = 0.95 # degree of collinearity (LOW) kwargs = { 'n' : 100 , # sample size 'true_coefs' : [ 13 , 0.5 ], # linear regression coefficients, 2 features 'true_intercept' : 2 , # y-intercept 'feature_means' : [ 0 , 0 ], # means multivariate normal distribution. This is not important 'loc' : 0 , # mean of gaussian noise 'scale' : 1 # standard deviation of gaussian noise } # low collinearity covariance matrix cov_lo = np . full (( len ( kwargs [ 'true_coefs' ]), len ( kwargs [ 'true_coefs' ])), correlation_lo ) np . fill_diagonal ( cov_lo , 1 ) # simulation hi_col_params = [] lo_col_params = [] X_lo , y_lo = generate_collinear_data ( cov_lo , ** kwargs ) df = pd . DataFrame ( X_lo , columns = [ '$X_ %s $' % str ( i + 1 ) for i in range ( len ( kwargs [ 'true_coefs' ]))]) vifs = np . linalg . inv ( df . corr () . values ) . diagonal () df_vif = pd . DataFrame () df_vif [ 'Features' ] = df . columns df_vif [ 'VIF' ] = vifs df_vif Out[91]: Features VIF 0 $X_1$ 6.907328 1 $X_2$ 6.907328 In [92]: df [ '$X_1&#94;2$' ] = df [ '$X_1$' ] . values ** 2 df [ '$X_1 \\cdot X_2$' ] = df [ '$X_1$' ] . values * df [ '$X_2$' ] . values df [ '$X_1 \\cdot X_2$' ] = df [ '$X_1$' ] . values * df [ '$X_2$' ] . values df . head () Out[92]: $X_1$ $X_2$ $X_1&#94;2$ $X_1 \\cdot X_2$ 0 -0.116359 0.136477 0.013539 -0.015880 1 -0.221138 -0.222141 0.048902 0.049124 2 0.242903 0.527453 0.059002 0.128120 3 -0.694996 -0.413896 0.483020 0.287656 4 -0.630917 -0.614348 0.398056 0.387603 In [123]: import pandas as pd import numpy as np # data source: https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v5.csv file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' features = [ 'Por' , 'Perm' , 'AI' , 'Brittle' , 'TOC' , 'VR' ] df = pd . read_csv ( file ) df = df [ features ] vifs = np . linalg . inv ( df . corr () . values ) . diagonal () df_vif = pd . DataFrame () df_vif [ 'Features' ] = df . columns df_vif [ 'VIF' ] = vifs df_vif Out[123]: Features VIF 0 Por 4.329838 1 Perm 2.862487 2 AI 7.660318 3 Brittle 2.165476 4 TOC 7.365773 5 VR 7.057671 In [131]: import pandas as pd import numpy as np # data source: https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v5.csv file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' features = [ 'Por' , 'Perm' , 'AI' , 'Brittle' , 'TOC' , 'VR' ] df = pd . read_csv ( file ) df = df [ features ] df [ 'Perm $\\cdot$ AI' ] = df [ 'Perm' ] . values * df [ 'AI' ] . values df [ 'Por$&#94;2$' ] = df [ 'Por' ] . values ** 2 df = df . iloc [:, 2 :] vifs = np . linalg . inv ( df . corr () . values ) . diagonal () df_vif = pd . DataFrame () df_vif [ 'Features' ] = df . columns df_vif [ 'VIF' ] = vifs df_vif Out[131]: Features VIF 0 AI 9.211312 1 Brittle 2.135223 2 TOC 7.281669 3 VR 6.784278 4 Perm $\\cdot$ AI 2.552091 5 Por$&#94;2$ 3.927284 In [ ]: In [ ]: In [96]: file = 'sample_data/MulticollinearityExample.csv' df = pd . read_csv ( file )[[ ' %F at' , 'Weight kg' , 'Activity' ]] df [ 'fat * weight' ] = df [ ' %F at' ] . values * df [ 'Weight kg' ] . values vifs = np . linalg . inv ( df . corr () . values ) . diagonal () df_vif = pd . DataFrame () df_vif [ 'Features' ] = df . columns df_vif [ 'VIF' ] = vifs df_vif Out[96]: Features VIF 0 %Fat 14.931555 1 Weight kg 33.948375 2 Activity 1.053005 3 fat * weight 75.059251 In [17]: file = 'sample_data/MulticollinearityExample.csv' df = pd . read_csv ( file )[[ ' %F at' , 'Weight kg' , 'Activity' ]] scaler = StandardScaler ( with_std = False ) df = pd . DataFrame ( scaler . fit_transform ( df ), columns = df . columns ) df [ 'fat * weight' ] = df [ ' %F at' ] . values * df [ 'Weight kg' ] . values vifs = np . linalg . inv ( df . corr () . values ) . diagonal () df_vif = pd . DataFrame () df_vif [ 'Features' ] = df . columns df_vif [ 'VIF' ] = vifs df_vif Out[17]: Features VIF 0 %Fat 3.323870 1 Weight kg 4.745648 2 Activity 1.053005 3 fat * weight 1.991063 In [24]: import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler # data source: https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v5.csv file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' features = [ 'Por' , 'Perm' , 'AI' , 'Brittle' , 'TOC' , 'VR' ] df = pd . read_csv ( file ) df = df [ features ] #scaler = StandardScaler(with_std=False) #df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns) df [ 'Perm $\\cdot$ AI' ] = df [ 'Perm' ] . values * df [ 'AI' ] . values #df['Por$&#94;2$'] = df['Por'].values ** 2 vifs = np . linalg . inv ( df . corr () . values ) . diagonal () df_vif = pd . DataFrame () df_vif [ 'Features' ] = df . columns df_vif [ 'VIF' ] = vifs df_vif Out[24]: Features VIF 0 Por 4.329870 1 Perm 31.111442 2 AI 13.097525 3 Brittle 2.165532 4 TOC 7.401731 5 VR 7.072402 6 Perm $\\cdot$ AI 28.679073 In [48]: df = df . iloc [:, 1 :] In [49]: df_cor = df . corr () pd . DataFrame ( np . linalg . inv ( df . corr () . values ), index = df_cor . index , columns = df_cor . columns ) Out[49]: Por Perm AI Brittle TOC VR Prod Por 16.176635 0.949103 -1.737039 6.897739 -0.963265 2.655457 -17.175719 Perm 0.949103 3.886711 -2.852903 1.072081 -1.333245 2.810110 -5.050239 AI -1.737039 -2.852903 8.736127 0.781396 6.358676 -7.846188 5.175854 Brittle 6.897739 1.072081 0.781396 5.321840 2.147779 -0.686299 -8.865597 TOC -0.963265 -1.333245 6.358676 2.147779 7.400963 -6.199942 0.936102 VR 2.655457 2.810110 -7.846188 -0.686299 -6.199942 8.462222 -5.914026 Prod -17.175719 -5.050239 5.175854 -8.865597 0.936102 -5.914026 24.901696 In [40]: exogs = [ 'Por' , 'Brittle' , 'Perm' , 'TOC' , 'AI' , 'VR' ] data = df [ exogs ] vif_dict , tolerance_dict = {}, {} df_vif = pd . DataFrame ({ 'VIF' : vif_dict , 'Tolerance' : tolerance_dict }) vif_dict [ exog ] = vif # create formula for each exogenous variable for exog in exogs : not_exog = [ i for i in exogs if i != exog ] formula = f \" { exog } ~ { ' + ' . join ( not_exog ) } \" # extract r-squared from the fit r_squared = smf . ols ( formula , data = data ) . fit () . rsquared # calculate VIF vif = 1 / ( 1 - r_squared ) vif_dict [ exog ] = vif # calculate tolerance tolerance = 1 - r_squared tolerance_dict [ exog ] = tolerance # return VIF DataFrame df_vif = pd . DataFrame ({ 'VIF' : vif_dict , 'Tolerance' : tolerance_dict }) df_vif Out[40]: VIF Tolerance AI 7.660318 0.130543 Brittle 2.165476 0.461792 Perm 2.862487 0.349347 Por 4.329838 0.230956 TOC 7.365773 0.135763 VR 7.057671 0.141690 In [27]: exogs = [ 'Por' , 'Brittle' , 'Perm' , 'TOC' , 'AI' , 'VR' ] data = df [ exogs ] vif_dict , tolerance_dict = {}, {} # form input data for each exogenous variable for exog in exogs : not_exog = [ i for i in exogs if i != exog ] X , y = data [ not_exog ], data [ exog ] # extract r-squared from the fit r_squared = LinearRegression () . fit ( X , y ) . score ( X , y ) # calculate VIF vif = 1 / ( 1 - r_squared ) vif_dict [ exog ] = vif # calculate tolerance tolerance = 1 - r_squared tolerance_dict [ exog ] = tolerance # return VIF DataFrame df_vif = pd . DataFrame ({ 'VIF' : vif_dict , 'Tolerance' : tolerance_dict }) df_vif Out[27]: VIF Tolerance AI 7.660318 0.130543 Brittle 2.165476 0.461792 Perm 2.862487 0.349347 Por 4.329838 0.230956 TOC 7.365773 0.135763 VR 7.057671 0.141690 In [52]: from statsmodels.stats.outliers_influence import variance_inflation_factor import pandas as pd import numpy as np import statsmodels.api as sm file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) features = [ 'Por' , 'Brittle' , 'Perm' , 'TOC' , 'AI' , 'VR' ] X = df [ features ] . values . reshape ( - 1 , len ( features )) vif = pd . DataFrame () X = sm . add_constant ( X ) vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) In [53]: vif Out[53]: Features VIF 0 Por 161.797377 1 Brittle 4.329838 2 Perm 2.165476 3 TOC 2.862487 4 AI 7.365773 5 VR 7.660318 In [34]: y = X [:, 0 ] X_new = X [:, 1 :] In [35]: X_new . shape Out[35]: (200, 5) In [36]: from sklearn import linear_model ols = linear_model . LinearRegression () model = ols . fit ( X_new , y ) model . score ( X_new , y ) Out[36]: 0.7690444904327748 In [37]: 1 / ( 1 - model . score ( X_new , y ) ** 2 ) Out[37]: 2.447557580452999 In [ ]: In [66]: file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) features = [ 'Por' , 'Brittle' , 'Perm' , 'TOC' , 'AI' , 'VR' ] df_features = df [ features ] In [ ]: In [57]: from statsmodels.stats.outliers_influence import variance_inflation_factor from statsmodels.tools.tools import add_constant import pandas as pd import numpy as np In [79]: df = pd . DataFrame ( { 'a' : [ 1 , 1 , 2 , 3 , 4 ], 'b' : [ 2 , 2 , 3 , 2 , 1 ], 'c' : [ 4 , 6 , 7 , 8 , 9 ], 'd' : [ 4 , 3 , 4 , 5 , 4 ]} ) In [80]: X = add_constant ( df ) pd . Series ([ variance_inflation_factor ( X . values , i ) for i in range ( X . shape [ 1 ])], index = X . columns ) Out[80]: const 136.875 a 22.950 b 3.000 c 12.950 d 3.000 dtype: float64 In [81]: df_cor = df . corr () temp = pd . DataFrame ( np . linalg . inv ( df_cor . values ), index = df_cor . index , columns = df_cor . columns ) np . diag ( temp ) Out[81]: array([22.95, 3. , 12.95, 3. ]) In [68]: file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) features = [ 'Por' , 'Brittle' , 'Perm' , 'TOC' , 'AI' , 'VR' ] df_features = df [ features ] In [82]: X = add_constant ( df_features ) pd . Series ([ variance_inflation_factor ( X . values , i ) for i in range ( X . shape [ 1 ])], index = X . columns ) Out[82]: const 161.797377 Por 4.329838 Brittle 2.165476 Perm 2.862487 TOC 7.365773 AI 7.660318 VR 7.057671 dtype: float64 In [78]: k_vars = exog . shape [ 1 ] x_i = exog [:, exog_idx ] mask = np . arange ( k_vars ) != exog_idx x_noti = exog [:, mask ] r_squared_i = OLS ( x_i , x_noti ) . fit () . rsquared vif = 1. / ( 1. - r_squared_i ) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) in () ----> 1 k_vars = exog . shape [ 1 ] 2 x_i = exog [ : , exog_idx ] 3 mask = np . arange ( k_vars ) != exog_idx 4 x_noti = exog [ : , mask ] 5 r_squared_i = OLS ( x_i , x_noti ) . fit ( ) . rsquared AttributeError : 'str' object has no attribute 'shape' In [84]: import pandas as pd import statsmodels.formula.api as smf def get_vif ( exogs , data ): '''Return VIF (variance inflation factor) DataFrame Args: exogs (list): list of exogenous/independent variables data (DataFrame): the df storing all variables Returns: VIF and Tolerance DataFrame for each exogenous variable Notes: Assume we have a list of exogenous variable [X1, X2, X3, X4]. To calculate the VIF and Tolerance for each variable, we regress each of them against other exogenous variables. For instance, the regression model for X3 is defined as: X3 ~ X1 + X2 + X4 And then we extract the R-squared from the model to calculate: VIF = 1 / (1 - R-squared) Tolerance = 1 - R-squared The cutoff to detect multicollinearity: VIF > 10 or Tolerance < 0.1 ''' # initialize dictionaries vif_dict , tolerance_dict = {}, {} # create formula for each exogenous variable for exog in exogs : not_exog = [ i for i in exogs if i != exog ] formula = f \" { exog } ~ { ' + ' . join ( not_exog ) } \" # extract r-squared from the fit r_squared = smf . ols ( formula , data = data ) . fit () . rsquared # calculate VIF vif = 1 / ( 1 - r_squared ) vif_dict [ exog ] = vif # calculate tolerance tolerance = 1 - r_squared tolerance_dict [ exog ] = tolerance # return VIF DataFrame df_vif = pd . DataFrame ({ 'VIF' : vif_dict , 'Tolerance' : tolerance_dict }) return df_vif In [96]: get_vif ([ 'a' , 'b' , 'c' , 'd' ], df ) Out[96]: VIF Tolerance a 22.95 0.043573 b 3.00 0.333333 c 12.95 0.077220 d 3.00 0.333333 In [95]: df = pd . DataFrame ( { 'a' : [ 1 , 1 , 2 , 3 , 4 ], 'b' : [ 2 , 2 , 3 , 2 , 1 ], 'c' : [ 4 , 6 , 7 , 8 , 9 ], 'd' : [ 4 , 3 , 4 , 5 , 4 ]} ) In [101]: data = df not_exog = [ 'b' , 'c' , 'd' ] exog = 'a' X , y = data [ not_exog ], data [ exog ] # extract r-squared from the fit r_squared = LinearRegression () . fit ( X , y ) . score ( X , y ) # calculate VIF vif = 1 / ( 1 - r_squared ) vif Out[101]: 22.949999999999985 In [ ]: Introduction Nfoi + Gfgi example, standard error is important (possibly estimating OOIP + OGIP after PCA treament?) Note that this multicollinearity does not degrade the prediction power of a model Key Takeaways Multicollinearity occurs when (can be described by ...) Multicollinearity makes the values of individual regression coefficients are unreliable It can cause change in the signs as well as in the magnitudes from one sample to another sample. If you concluded that feature x1 has a positive impact on a response variable y because the regression coefficient of x1 is positive, this might be wrong. I show this here and here. Multicollinearity does not affect the prediction power on a response variable (bundle) Use VIF to detect and remove collinear features (if VIF > 10, its problematic) Sometimes multicollinearity can be safely ignored ( https://statisticalhorizons.com/multicollinearity ) Multicollinearity among independent variables will result in less reliable statistical inferences. Concept Thought experiment Causes Generally occurs when features are highly correlated Incorrect dummy variable encoding Repetition of the same feature (different units) A feature computed from other features is in the data set. Effects (This makes interpretation unreliable) Model Instability with sampling Python simulation, 2D plot Model instability with feature selection Same screenshot used in previous post Overestimated standard errors 95% conf int Box plots for each regression coefficients, paired with sampling vs regression analysis Notes: Value of regression coefficient is not feature importance (Perhaps significance test) Detection Correlation matrix VIF score A Caution Regarding Rules of Thumb for Variance Inflation Factors https://link.springer.com/article/10.1007/s11135-006-9018-6 Remedies Removal (VIF score + correlation matrix) (If that features is necessary, tough luck) Principle Component Regression PCA is generally used when the aim is to retrieve a latent unmeasured variable or set of variables proxied by measured manifestation variables: for example, economic openness is not directly measured but there are indicators for it such as trade volume, trade tariffs, exchange policy, etc. So pca is implemented in order to assess a synthetic variable for the Economic openness. You will have to find a \"meaning\" for your principal components based on the coordinates of the original variables relative to each component Warning: Interpretation of principle components. Do it only if you really understand what PCA does. Mean centering Do not standardize, by Jim Notes: STD dividing for non-normal distributions https://statisticsbyjim.com/regression/standardize-variables-regression/ Excuses to ignore multicollinearity (or Pitfalls) In [ ]: In [ ]: In [ ]: In [79]: import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model import statsmodels.api as sm In [122]: features = [ 'a1' , 'a2' ] true_coefs = [ 2 , 4 ] true_intercept = 1 n = 100 In [182]: correlation = 0 cov = [[ 1 , correlation ], [ correlation , 1 ]] model_coefs_low = [] for i in range ( 100 ): # random generation of 2D collinear gaussian multivariate distribution with covariance matrix X = np . random . multivariate_normal ( mean = [ 2 , 4 ], cov = cov , size = n ) # generate gaussian white noise gaussian_noise = np . random . normal ( loc = 0 , scale = 1 , size = n ) # make the outcome y = true_intercept + true_coefs [ 0 ] * X [:, 0 ] + true_coefs [ 1 ] * X [:, 1 ] + gaussian_noise # fit linear model ols = linear_model . LinearRegression () model = ols . fit ( X , y ) model_coefs_low . append ( model . coef_ ) model_coefs_low = np . array ( model_coefs_low ) In [184]: from statsmodels.stats.outliers_influence import variance_inflation_factor import pandas as pd features = [ 'a1' , 'a2' ] vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) vif Out[184]: Features VIF 0 a1 3.893106 1 a2 3.893106 In [185]: X_st = sm . add_constant ( X ) model = sm . OLS ( y , X_st ) . fit () predictions = model . predict ( X_st ) print ( model . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.946 Model: OLS Adj. R-squared: 0.945 Method: Least Squares F-statistic: 851.7 Date: Fri, 29 Nov 2019 Prob (F-statistic): 2.97e-62 Time: 21:12:33 Log-Likelihood: -140.88 No. Observations: 100 AIC: 287.8 Df Residuals: 97 BIC: 295.6 Df Model: 2 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 1.3984 0.488 2.868 0.005 0.431 2.366 x1 2.0366 0.102 19.961 0.000 1.834 2.239 x2 3.8715 0.107 36.209 0.000 3.659 4.084 ============================================================================== Omnibus: 1.813 Durbin-Watson: 1.528 Prob(Omnibus): 0.404 Jarque-Bera (JB): 1.409 Skew: 0.284 Prob(JB): 0.494 Kurtosis: 3.122 Cond. No. 23.3 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. In [168]: from statsmodels.stats.outliers_influence import variance_inflation_factor import pandas as pd features = [ 'a1' , 'a2' ] vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) vif Out[168]: Features VIF 0 a1 18.710929 1 a2 18.710929 In [169]: correlation = 0.99 # construct 2D positive-semidefinite covariance matrix cov = [[ 1 , correlation ], [ correlation , 1 ]] model_coefs_high = [] for i in range ( 100 ): # random generation of 2D collinear gaussian multivariate distribution with covariance matrix X = np . random . multivariate_normal ( mean = [ 2 , 4 ], cov = cov , size = n ) # generate gaussian white noise gaussian_noise = np . random . normal ( loc = 0 , scale = 1 , size = n ) # make the outcome y = true_intercept + true_coefs [ 0 ] * X [:, 0 ] + true_coefs [ 1 ] * X [:, 1 ] + gaussian_noise # fit linear model ols = linear_model . LinearRegression () model = ols . fit ( X , y ) model_coefs_high . append ( model . coef_ ) model_coefs_high = np . array ( model_coefs_high ) In [170]: from statsmodels.stats.outliers_influence import variance_inflation_factor import pandas as pd vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) vif Out[170]: Features VIF 0 a1 28.032218 1 a2 28.032218 In [171]: X_st = sm . add_constant ( X ) model = sm . OLS ( y , X_st ) . fit () predictions = model . predict ( X_st ) print ( model . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.961 Model: OLS Adj. R-squared: 0.961 Method: Least Squares F-statistic: 1207. Date: Fri, 29 Nov 2019 Prob (F-statistic): 2.96e-69 Time: 21:11:33 Log-Likelihood: -143.14 No. Observations: 100 AIC: 292.3 Df Residuals: 97 BIC: 300.1 Df Model: 2 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 1.9978 1.587 1.259 0.211 -1.152 5.148 x1 2.8843 0.736 3.920 0.000 1.424 4.345 x2 3.2722 0.752 4.348 0.000 1.779 4.766 ============================================================================== Omnibus: 0.247 Durbin-Watson: 2.229 Prob(Omnibus): 0.884 Jarque-Bera (JB): 0.427 Skew: 0.023 Prob(JB): 0.808 Kurtosis: 2.683 Cond. No. 88.3 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. In [172]: fig , ax = plt . subplots () ax . scatter ( model_coefs_low [:, 0 ], model_coefs_low [:, 1 ], label = 'Low' ) ax . scatter ( model_coefs_high [:, 0 ], model_coefs_high [:, 1 ], label = 'High' ) ax . legend () ax . set_xlabel ( 'a1' , fontsize = 20 ) ax . set_ylabel ( 'a2' , fontsize = 20 ) Out[172]: Text(0, 0.5, 'a2') In [91]: import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from statsmodels.stats.outliers_influence import variance_inflation_factor import statsmodels.api as sm Original In [92]: features = [ 'a1' , 'a2' ] true_coefs = [ 10 , 15 ] true_intercept = 2 n = 100 In [121]: correlation = 0.99 # construct 2D positive-semidefinite covariance matrix cov = [[ 1 , correlation ], [ correlation , 1 ]] model_coefs_orig = [] for i in range ( 100 ): # random generation of 2D collinear gaussian multivariate distribution with covariance matrix X = np . random . multivariate_normal ( mean = [ 2 , 4 ], cov = cov , size = n ) # generate gaussian white noise gaussian_noise = np . random . normal ( loc = 0 , scale = 1 , size = n ) # make the outcome y = true_intercept + true_coefs [ 0 ] * X [:, 0 ] + true_coefs [ 1 ] * X [:, 1 ] + gaussian_noise # fit linear model ols = linear_model . LinearRegression () model = ols . fit ( X , y ) model_coefs_orig . append ( model . coef_ ) model_coefs_orig = np . array ( model_coefs_orig ) In [122]: from statsmodels.stats.outliers_influence import variance_inflation_factor import pandas as pd vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) vif Out[122]: Features VIF 0 a1 19.583652 1 a2 19.583652 In [123]: X_st = sm . add_constant ( X ) sm_model = sm . OLS ( y , X_st ) . fit () predictions = sm_model . predict ( X_st ) print ( sm_model . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.998 Model: OLS Adj. R-squared: 0.998 Method: Least Squares F-statistic: 3.036e+04 Date: Fri, 29 Nov 2019 Prob (F-statistic): 2.17e-136 Time: 21:58:41 Log-Likelihood: -139.20 No. Observations: 100 AIC: 284.4 Df Residuals: 97 BIC: 292.2 Df Model: 2 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 2.9605 1.506 1.966 0.052 -0.029 5.950 x1 10.5327 0.737 14.283 0.000 9.069 11.996 x2 14.5136 0.742 19.568 0.000 13.042 15.986 ============================================================================== Omnibus: 0.174 Durbin-Watson: 1.964 Prob(Omnibus): 0.917 Jarque-Bera (JB): 0.355 Skew: 0.031 Prob(JB): 0.837 Kurtosis: 2.715 Cond. No. 86.1 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. In [124]: print ( 'a1 = [ %.2f ' % np . percentile ( model_coefs_orig [:, 0 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_orig [:, 0 ], 97.5 )) print ( 'a2 = [ %.2f ' % np . percentile ( model_coefs_orig [:, 1 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_orig [:, 1 ], 97.5 )) a1 = [8.86 11.52] a2 = [13.52 16.08] PCA In [125]: import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from statsmodels.stats.outliers_influence import variance_inflation_factor import statsmodels.api as sm In [126]: features = [ 'a1' , 'a2' ] true_coefs = [ 10 , 15 ] true_intercept = 2 n = 100 In [127]: correlation = 0.95 # construct 2D positive-semidefinite covariance matrix cov = [[ 1 , correlation ], [ correlation , 1 ]] model_coefs_pca = [] for i in range ( 100 ): # random generation of 2D collinear gaussian multivariate distribution with covariance matrix X = np . random . multivariate_normal ( mean = [ 2 , 4 ], cov = cov , size = n ) scalar = StandardScaler () X = scalar . fit_transform ( X ) # PCA pca = PCA ( n_components = len ( features )) X = pca . fit_transform ( X ) # generate gaussian white noise gaussian_noise = np . random . normal ( loc = 0 , scale = 1 , size = n ) # make the outcome y = true_intercept + true_coefs [ 0 ] * X [:, 0 ] + true_coefs [ 1 ] * X [:, 1 ] + gaussian_noise # fit linear model ols = linear_model . LinearRegression () model = ols . fit ( X , y ) model_coefs_pca . append ( model . coef_ ) model_coefs_pca = np . array ( model_coefs_pca ) In [128]: from statsmodels.stats.outliers_influence import variance_inflation_factor import pandas as pd vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) vif Out[128]: Features VIF 0 a1 1.0 1 a2 1.0 In [129]: X_st = sm . add_constant ( X ) sm_model = sm . OLS ( y , X_st ) . fit () predictions = sm_model . predict ( X_st ) print ( sm_model . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.994 Model: OLS Adj. R-squared: 0.994 Method: Least Squares F-statistic: 8396. Date: Fri, 29 Nov 2019 Prob (F-statistic): 2.09e-109 Time: 21:58:47 Log-Likelihood: -150.29 No. Observations: 100 AIC: 306.6 Df Residuals: 97 BIC: 314.4 Df Model: 2 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 2.0979 0.110 18.999 0.000 1.879 2.317 x1 10.0127 0.079 126.914 0.000 9.856 10.169 x2 14.2822 0.546 26.166 0.000 13.199 15.366 ============================================================================== Omnibus: 7.355 Durbin-Watson: 1.705 Prob(Omnibus): 0.025 Jarque-Bera (JB): 4.696 Skew: 0.369 Prob(JB): 0.0956 Kurtosis: 2.237 Cond. No. 6.92 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. In [130]: print ( 'a1 = [ %.2f ' % np . percentile ( model_coefs_pca [:, 0 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_pca [:, 0 ], 97.5 )) print ( 'a2 = [ %.2f ' % np . percentile ( model_coefs_pca [:, 1 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_pca [:, 1 ], 97.5 )) a1 = [9.84 10.13] a2 = [14.01 15.98] Centering In [131]: import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from statsmodels.stats.outliers_influence import variance_inflation_factor import statsmodels.api as sm In [132]: features = [ 'a1' , 'a2' ] true_coefs = [ 10 , 15 ] true_intercept = 2 n = 100 In [133]: correlation = 0.95 # construct 2D positive-semidefinite covariance matrix cov = [[ 1 , correlation ], [ correlation , 1 ]] model_coefs_center = [] for i in range ( 100 ): # random generation of 2D collinear gaussian multivariate distribution with covariance matrix X = np . random . multivariate_normal ( mean = [ 2 , 4 ], cov = cov , size = n ) scalar = StandardScaler ( with_std = False ) X = scalar . fit_transform ( X ) # generate gaussian white noise gaussian_noise = np . random . normal ( loc = 0 , scale = 1 , size = n ) # make the outcome y = true_intercept + true_coefs [ 0 ] * X [:, 0 ] + true_coefs [ 1 ] * X [:, 1 ] + gaussian_noise # fit linear model ols = linear_model . LinearRegression () model = ols . fit ( X , y ) model_coefs_center . append ( model . coef_ ) model_coefs_center = np . array ( model_coefs_center ) In [134]: from statsmodels.stats.outliers_influence import variance_inflation_factor import pandas as pd vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) vif Out[134]: Features VIF 0 a1 11.76716 1 a2 11.76716 In [135]: X_st = sm . add_constant ( X ) sm_model = sm . OLS ( y , X_st ) . fit () predictions = sm_model . predict ( X_st ) print ( sm_model . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.999 Model: OLS Adj. R-squared: 0.999 Method: Least Squares F-statistic: 3.898e+04 Date: Fri, 29 Nov 2019 Prob (F-statistic): 1.20e-141 Time: 21:59:00 Log-Likelihood: -134.28 No. Observations: 100 AIC: 274.6 Df Residuals: 97 BIC: 282.4 Df Model: 2 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 2.1242 0.094 22.575 0.000 1.937 2.311 x1 9.6760 0.301 32.136 0.000 9.078 10.274 x2 15.1599 0.303 50.111 0.000 14.559 15.760 ============================================================================== Omnibus: 1.070 Durbin-Watson: 2.045 Prob(Omnibus): 0.586 Jarque-Bera (JB): 1.056 Skew: -0.110 Prob(JB): 0.590 Kurtosis: 2.547 Cond. No. 6.71 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. In [137]: print ( 'a1 = [ %.2f ' % np . percentile ( model_coefs_center [:, 0 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_center [:, 0 ], 97.5 )) print ( 'a2 = [ %.2f ' % np . percentile ( model_coefs_center [:, 1 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_center [:, 1 ], 97.5 )) a1 = [9.37 10.76] a2 = [14.29 15.65] In [136]: fig , ax = plt . subplots () ax . scatter ( model_coefs_orig [:, 0 ], model_coefs_orig [:, 1 ], label = 'orig' ) ax . scatter ( model_coefs_pca [:, 0 ], model_coefs_pca [:, 1 ], label = 'pca' ) ax . scatter ( model_coefs_center [:, 0 ], model_coefs_center [:, 1 ], label = 'center' ) ax . legend () ax . set_xlabel ( 'a1' , fontsize = 20 ) ax . set_ylabel ( 'a2' , fontsize = 20 ) Out[136]: Text(0, 0.5, 'a2') 3 features original In [151]: import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from statsmodels.stats.outliers_influence import variance_inflation_factor import statsmodels.api as sm In [152]: features = [ 'a1' , 'a2' , 'a3' ] true_coefs = [ 2 , 4 , 10 ] true_intercept = 2 n = 100 In [159]: # construct 2D positive-semidefinite covariance matrix cov = [[ 1 , 0.95 , 0.93 ], [ 0.95 , 1 , 0.95 ], [ 0.93 , 0.95 , 1 ]] model_coefs_orig = [] for i in range ( 100 ): # random generation of 2D collinear gaussian multivariate distribution with covariance matrix X = np . random . multivariate_normal ( mean = [ 2 , 4 , 10 ], cov = cov , size = n ) # generate gaussian white noise gaussian_noise = np . random . normal ( loc = 0 , scale = 2 , size = n ) # make the outcome y = true_intercept + true_coefs [ 0 ] * X [:, 0 ] + true_coefs [ 1 ] * X [:, 1 ] + true_coefs [ 2 ] * X [:, 2 ] + gaussian_noise # fit linear model ols = linear_model . LinearRegression () model = ols . fit ( X , y ) model_coefs_orig . append ( model . coef_ ) model_coefs_orig = np . array ( model_coefs_orig ) In [160]: from statsmodels.stats.outliers_influence import variance_inflation_factor import pandas as pd vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) vif Out[160]: Features VIF 0 a1 41.566893 1 a2 246.401621 2 a3 113.336348 In [161]: X_st = sm . add_constant ( X ) sm_model = sm . OLS ( y , X_st ) . fit () predictions = sm_model . predict ( X_st ) print ( sm_model . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.976 Model: OLS Adj. R-squared: 0.975 Method: Least Squares F-statistic: 1298. Date: Fri, 29 Nov 2019 Prob (F-statistic): 1.54e-77 Time: 22:03:37 Log-Likelihood: -214.78 No. Observations: 100 AIC: 437.6 Df Residuals: 96 BIC: 448.0 Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 3.2692 4.590 0.712 0.478 -5.843 12.381 x1 1.1353 0.687 1.653 0.102 -0.228 2.499 x2 4.8426 0.849 5.705 0.000 3.158 6.528 x3 9.7335 0.655 14.870 0.000 8.434 11.033 ============================================================================== Omnibus: 6.239 Durbin-Watson: 1.681 Prob(Omnibus): 0.044 Jarque-Bera (JB): 6.881 Skew: 0.357 Prob(JB): 0.0320 Kurtosis: 4.068 Cond. No. 243. ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. In [162]: print ( 'a1 = [ %.2f ' % np . percentile ( model_coefs_orig [:, 0 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_orig [:, 0 ], 97.5 )) print ( 'a2 = [ %.2f ' % np . percentile ( model_coefs_orig [:, 1 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_orig [:, 1 ], 97.5 )) print ( 'a3 = [ %.2f ' % np . percentile ( model_coefs_orig [:, 2 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_orig [:, 2 ], 97.5 )) a1 = [0.80 3.09] a2 = [2.95 5.45] a3 = [8.42 11.17] PCA In [170]: import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from statsmodels.stats.outliers_influence import variance_inflation_factor import statsmodels.api as sm In [171]: features = [ 'a1' , 'a2' , 'a3' ] true_coefs = [ 2 , 4 , 10 ] true_intercept = 2 n = 100 In [174]: # construct 2D positive-semidefinite covariance matrix cov = [[ 1 , 0.95 , 0.93 ], [ 0.95 , 1 , 0.95 ], [ 0.93 , 0.95 , 1 ]] model_coefs_pca = [] for i in range ( 100 ): # random generation of 2D collinear gaussian multivariate distribution with covariance matrix X = np . random . multivariate_normal ( mean = [ 2 , 4 , 10 ], cov = cov , size = n ) scalar = StandardScaler () X = scalar . fit_transform ( X ) # PCA pca = PCA ( n_components = len ( features )) X = pca . fit_transform ( X ) # generate gaussian white noise gaussian_noise = np . random . normal ( loc = 0 , scale = 2 , size = n ) # make the outcome y = true_intercept + true_coefs [ 0 ] * X [:, 0 ] + true_coefs [ 1 ] * X [:, 1 ] + true_coefs [ 2 ] * X [:, 2 ] + gaussian_noise # fit linear model ols = linear_model . LinearRegression () model = ols . fit ( X , y ) model_coefs_pca . append ( model . coef_ ) model_coefs_pca = np . array ( model_coefs_pca ) In [175]: from statsmodels.stats.outliers_influence import variance_inflation_factor import pandas as pd vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) vif Out[175]: Features VIF 0 a1 1.0 1 a2 1.0 2 a3 1.0 In [176]: X_st = sm . add_constant ( X ) sm_model = sm . OLS ( y , X_st ) . fit () predictions = sm_model . predict ( X_st ) print ( sm_model . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.844 Model: OLS Adj. R-squared: 0.839 Method: Least Squares F-statistic: 172.9 Date: Fri, 29 Nov 2019 Prob (F-statistic): 1.43e-38 Time: 22:07:19 Log-Likelihood: -206.91 No. Observations: 100 AIC: 421.8 Df Residuals: 96 BIC: 432.2 Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 1.6930 0.196 8.658 0.000 1.305 2.081 x1 2.1387 0.115 18.579 0.000 1.910 2.367 x2 3.9743 0.756 5.256 0.000 2.473 5.475 x3 10.8062 0.895 12.076 0.000 9.030 12.582 ============================================================================== Omnibus: 1.576 Durbin-Watson: 2.190 Prob(Omnibus): 0.455 Jarque-Bera (JB): 1.269 Skew: -0.275 Prob(JB): 0.530 Kurtosis: 3.055 Cond. No. 7.77 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. In [177]: print ( 'a1 = [ %.2f ' % np . percentile ( model_coefs_pca [:, 0 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_pca [:, 0 ], 97.5 )) print ( 'a2 = [ %.2f ' % np . percentile ( model_coefs_pca [:, 1 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_pca [:, 1 ], 97.5 )) print ( 'a3 = [ %.2f ' % np . percentile ( model_coefs_pca [:, 2 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_pca [:, 2 ], 97.5 )) a1 = [1.82 2.24] a2 = [2.64 5.85] a3 = [8.33 11.93] Centering In [142]: import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from statsmodels.stats.outliers_influence import variance_inflation_factor import statsmodels.api as sm In [143]: features = [ 'a1' , 'a2' , 'a3' ] true_coefs = [ 2 , 4 , 10 ] true_intercept = 2 n = 100 In [164]: # construct 2D positive-semidefinite covariance matrix cov = [[ 1 , 0.95 , 0.93 ], [ 0.95 , 1 , 0.95 ], [ 0.93 , 0.95 , 1 ]] model_coefs_center = [] for i in range ( 100 ): # random generation of 2D collinear gaussian multivariate distribution with covariance matrix X = np . random . multivariate_normal ( mean = [ 2 , 4 , 10 ], cov = cov , size = n ) scalar = StandardScaler ( with_std = False ) X = scalar . fit_transform ( X ) # generate gaussian white noise gaussian_noise = np . random . normal ( loc = 0 , scale = 2 , size = n ) # make the outcome y = true_intercept + true_coefs [ 0 ] * X [:, 0 ] + true_coefs [ 1 ] * X [:, 1 ] + true_coefs [ 2 ] * X [:, 2 ] + gaussian_noise # fit linear model ols = linear_model . LinearRegression () model = ols . fit ( X , y ) model_coefs_center . append ( model . coef_ ) model_coefs_center = np . array ( model_coefs_center ) In [145]: from statsmodels.stats.outliers_influence import variance_inflation_factor import pandas as pd vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) vif Out[145]: Features VIF 0 a1 11.866335 1 a2 17.993888 2 a3 11.526422 In [146]: X_st = sm . add_constant ( X ) sm_model = sm . OLS ( y , X_st ) . fit () predictions = sm_model . predict ( X_st ) print ( sm_model . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.986 Model: OLS Adj. R-squared: 0.985 Method: Least Squares F-statistic: 2215. Date: Fri, 29 Nov 2019 Prob (F-statistic): 1.82e-88 Time: 22:00:06 Log-Likelihood: -203.14 No. Observations: 100 AIC: 414.3 Df Residuals: 96 BIC: 424.7 Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 2.2459 0.188 11.927 0.000 1.872 2.620 x1 0.7699 0.691 1.114 0.268 -0.602 2.141 x2 4.0557 0.848 4.785 0.000 2.373 5.738 x3 11.2848 0.657 17.170 0.000 9.980 12.589 ============================================================================== Omnibus: 0.196 Durbin-Watson: 1.361 Prob(Omnibus): 0.906 Jarque-Bera (JB): 0.377 Skew: -0.040 Prob(JB): 0.828 Kurtosis: 2.710 Cond. No. 8.88 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. In [168]: print ( 'a1 = [ %.2f ' % np . percentile ( model_coefs_center [:, 0 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_center [:, 0 ], 97.5 )) print ( 'a2 = [ %.2f ' % np . percentile ( model_coefs_center [:, 1 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_center [:, 1 ], 97.5 )) print ( 'a3 = [ %.2f ' % np . percentile ( model_coefs_center [:, 2 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_center [:, 2 ], 97.5 )) a1 = [0.76 3.29] a2 = [2.49 5.35] a3 = [8.65 11.24] In [ ]: In [ ]: In [180]: print ( 'Original' ) print ( 'a1 = [ %.2f ' % np . percentile ( model_coefs_orig [:, 0 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_orig [:, 0 ], 97.5 )) print ( 'a2 = [ %.2f ' % np . percentile ( model_coefs_orig [:, 1 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_orig [:, 1 ], 97.5 )) print ( 'a3 = [ %.2f ' % np . percentile ( model_coefs_orig [:, 2 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_orig [:, 2 ], 97.5 )) print ( ' \\n PCA' ) print ( 'a1 = [ %.2f ' % np . percentile ( model_coefs_pca [:, 0 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_pca [:, 0 ], 97.5 )) print ( 'a2 = [ %.2f ' % np . percentile ( model_coefs_pca [:, 1 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_pca [:, 1 ], 97.5 )) print ( 'a3 = [ %.2f ' % np . percentile ( model_coefs_pca [:, 2 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_pca [:, 2 ], 97.5 )) print ( ' \\n Center' ) print ( 'a1 = [ %.2f ' % np . percentile ( model_coefs_center [:, 0 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_center [:, 0 ], 97.5 )) print ( 'a2 = [ %.2f ' % np . percentile ( model_coefs_center [:, 1 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_center [:, 1 ], 97.5 )) print ( 'a3 = [ %.2f ' % np . percentile ( model_coefs_center [:, 2 ], 2.5 ), ' %.2f ]' % np . percentile ( model_coefs_center [:, 2 ], 97.5 )) Original a1 = [0.80 3.09] a2 = [2.95 5.45] a3 = [8.42 11.17] PCA a1 = [1.82 2.24] a2 = [2.64 5.85] a3 = [8.33 11.93] Center a1 = [0.76 3.29] a2 = [2.49 5.35] a3 = [8.65 11.24] In [ ]: true_coefs = [ 2 , 4 , 10 ] In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [20]: file = 'sample_data/ooip_ogip.csv' df = pd . read_csv ( file ) features = [ 'Eowf' , 'Egwf' ] target = 'F' # preprocessing required by sklearn X = df [ features ] . values . reshape ( - 1 , len ( features )) y = df [ target ] . values # Compute VIF vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) # statsmodel model = sm . OLS ( y , X ) . fit () predictions = model . predict ( X ) print ( model . summary ()) print ( ' \\n\\n\\n\\n ' ) print ( vif ) coefs = [] for i in range ( 1000 ): df_temp = df . sample ( frac = 0.95 ) X = df_temp [ features ] . values . reshape ( - 1 , len ( features )) y = df_temp [ target ] . values ols = linear_model . LinearRegression () model = ols . fit ( X , y ) coefs . append ( model . coef_ ) coefs = np . array ( coefs ) fig , axes = plt . subplots ( 1 , len ( features ), figsize = ( 4 * len ( features ), 4 )) for i , feature in enumerate ( features ): axes [ i ] . hist ( coefs [:, i ], bins = 'auto' ) axes [ i ] . set_title ( feature ) axes [ i ] . set_ylabel ( 'Count' ) axes [ i ] . set_xlabel ( 'Regression coefficient value' ) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.994 Model: OLS Adj. R-squared: 0.994 Method: Least Squares F-statistic: 2252. Date: Wed, 27 Nov 2019 Prob (F-statistic): 9.23e-31 Time: 19:04:04 Log-Likelihood: -362.09 No. Observations: 29 AIC: 728.2 Df Residuals: 27 BIC: 730.9 Df Model: 2 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ x1 9.023e+07 8.73e+06 10.340 0.000 7.23e+07 1.08e+08 x2 -1.802e+07 1.98e+06 -9.082 0.000 -2.21e+07 -1.39e+07 ============================================================================== Omnibus: 9.266 Durbin-Watson: 0.091 Prob(Omnibus): 0.010 Jarque-Bera (JB): 2.362 Skew: -0.164 Prob(JB): 0.307 Kurtosis: 1.641 Cond. No. 244. ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Features VIF 0 Eowf 2787.049357 1 Egwf 2787.049357 In [163]: file = 'sample_data/ooip_ogip.csv' df = pd . read_csv ( file ) features = [ 'Eowf' , 'Egwf' ] target = 'F' # Standardization scalar = StandardScaler () X = scalar . fit_transform ( df [ features ]) # PCA pca = PCA ( n_components = len ( features )) X = pca . fit_transform ( X ) print ( type ( X )) # Reconstruct df with transformed features df_trans = pd . DataFrame ( X ) df_trans [ target ] = df [ target ] df_trans . columns = features + [ target ] # preprocessing required by sklearn X = df_trans [ features ] . values . reshape ( - 1 , len ( features )) y = df_trans [ target ] . values # statsmodel model = sm . OLS ( y , X ) . fit () predictions = model . predict ( X ) print ( model . summary ()) vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) print ( vif ) coefs = [] for i in range ( 1000 ): # randomly sample 70% of the original data. This is essentially bootstrapping df_temp = df_trans . sample ( frac = 0.7 ) X = df_temp [ features ] . values . reshape ( - 1 , len ( features )) y = df_temp [ target ] . values ols = linear_model . LinearRegression ( fit_intercept = False ) model = ols . fit ( X , y ) coefs . append ( model . coef_ ) coefs = np . array ( coefs ) fig , axes = plt . subplots ( 1 , len ( features ), figsize = ( 4 * len ( features ), 4 )) for i , feature in enumerate ( features ): axes [ i ] . hist ( coefs [:, i ], bins = 'auto' ) axes [ i ] . set_title ( feature ) axes [ i ] . set_ylabel ( 'Count' ) axes [ i ] . set_xlabel ( 'Regression coefficient value' ) fig . tight_layout () OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.181 Model: OLS Adj. R-squared: 0.121 Method: Least Squares F-statistic: 2.992 Date: Fri, 29 Nov 2019 Prob (F-statistic): 0.0671 Time: 21:09:39 Log-Likelihood: -433.47 No. Observations: 29 AIC: 870.9 Df Residuals: 27 BIC: 873.7 Df Model: 2 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ x1 -2.412e+05 1.02e+05 -2.362 0.026 -4.51e+05 -3.17e+04 x2 5.196e+06 8.16e+06 0.637 0.530 -1.16e+07 2.19e+07 ============================================================================== Omnibus: 24.882 Durbin-Watson: 0.001 Prob(Omnibus): 0.000 Jarque-Bera (JB): 3.207 Skew: 0.027 Prob(JB): 0.201 Kurtosis: 1.372 Cond. No. 79.9 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Features VIF 0 Eowf 1.0 1 Egwf 1.0 In [17]: sm_model . params Out[17]: array([ 3844830.39400844, -3503719.21148126]) In [18]: pca_inverse = pca . inverse_transform ( sm_model . params ) pca_inverse Out[18]: array([-5196209.2579275 , -241202.03030353]) In [19]: pca_inverse = pca . inverse_transform ( sm_model . params ) scalar . inverse_transform ( pca_inverse ) Out[19]: array([-124291.81061746, -24081.35939605]) In [6]: coefs Out[6]: array([[ -251725.52996658, 6784667.67113444], [ -659868.94590499, -11829722.22955466], [ -191775.09498549, -1408872.44269721], ..., [ -298643.41037758, 3678384.98523384], [ -264200.68853622, 2165056.06443315], [ -201849.14634761, 4924510.92333155]]) In [ ]: In [ ]: In [ ]: In [6]: file = 'sample_data/ooip_ogip.csv' df = pd . read_csv ( file ) features = [ 'Eowf' , 'Egwf' ] target = 'F' In [9]: X = df [ features ] . values . reshape ( - 1 , len ( features )) y = df [ target ] . values ols = linear_model . LinearRegression ( fit_intercept = False ) model = ols . fit ( X , y ) print ( model . score ( X , y )) print ( model . coef_ ) 0.9734009040213185 [ 89137384.70067914 -18363222.42705864] In [10]: # statsmodel X_st = sm . add_constant ( X ) model = sm . OLS ( y , X_st ) . fit () predictions = model . predict ( X_st ) print ( model . summary ()) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.980 Model: OLS Adj. R-squared: 0.979 Method: Least Squares F-statistic: 798.1 Date: Wed, 27 Nov 2019 Prob (F-statistic): 1.14e-28 Time: 17:31:02 Log-Likelihood: -442.12 No. Observations: 36 AIC: 890.2 Df Residuals: 33 BIC: 895.0 Df Model: 2 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 1.183e+05 3.68e+04 3.215 0.003 4.34e+04 1.93e+05 x1 1.093e+08 8.37e+06 13.053 0.000 9.22e+07 1.26e+08 x2 -2.33e+07 1.99e+06 -11.690 0.000 -2.74e+07 -1.92e+07 ============================================================================== Omnibus: 10.563 Durbin-Watson: 0.249 Prob(Omnibus): 0.005 Jarque-Bera (JB): 2.643 Skew: -0.129 Prob(JB): 0.267 Kurtosis: 1.698 Cond. No. 997. ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. In [ ]: In [ ]: multicollinearity is a problem with Mahalanobis distance. https://python-data-science.readthedocs.io/en/latest/unsupervised.html While powerful, its use of correlation can be detrimantal when there is multicollinearity (strong correlations among features). In [23]: file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) . iloc [:, 1 :] features = [ 'Por' , 'Brittle' , 'AI' , 'VR' , 'Perm' , 'TOC' ] target = 'Prod' # preprocessing required by sklearn X = df [ features ] . values . reshape ( - 1 , len ( features )) y = df [ target ] . values # Compute VIF vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) # statsmodel X_st = sm . add_constant ( X ) model = sm . OLS ( y , X_st ) . fit () predictions = model . predict ( X_st ) print ( model . summary ()) print ( ' \\n\\n\\n\\n ' ) print ( vif ) coefs = [] for i in range ( 1000 ): df_temp = df . sample ( frac = 0.95 ) X = df_temp [ features ] . values . reshape ( - 1 , len ( features )) y = df_temp [ target ] . values ols = linear_model . LinearRegression () model = ols . fit ( X , y ) coefs . append ( model . coef_ ) coefs = np . array ( coefs ) fig , axes = plt . subplots ( 1 , len ( features ), figsize = ( 4 * len ( features ), 4 )) for i , feature in enumerate ( features ): axes [ i ] . hist ( coefs [:, i ], bins = 'auto' ) axes [ i ] . set_title ( feature ) axes [ i ] . set_ylabel ( 'Count' ) axes [ i ] . set_xlabel ( 'Regression coefficient value' ) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.960 Model: OLS Adj. R-squared: 0.959 Method: Least Squares F-statistic: 768.8 Date: Wed, 27 Nov 2019 Prob (F-statistic): 8.13e-132 Time: 04:57:10 Log-Likelihood: -1341.7 No. Observations: 200 AIC: 2697. Df Residuals: 193 BIC: 2721. Df Model: 6 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -1230.2620 181.565 -6.776 0.000 -1588.369 -872.155 x1 230.2958 10.022 22.980 0.000 210.530 250.062 x2 24.9967 1.490 16.772 0.000 22.057 27.936 x3 -363.7361 69.866 -5.206 0.000 -501.534 -225.938 x4 783.1874 126.371 6.198 0.000 533.941 1032.434 x5 116.2280 13.986 8.310 0.000 88.642 143.814 x6 -77.4367 80.643 -0.960 0.338 -236.492 81.619 ============================================================================== Omnibus: 16.412 Durbin-Watson: 1.938 Prob(Omnibus): 0.000 Jarque-Bera (JB): 19.341 Skew: -0.607 Prob(JB): 6.31e-05 Kurtosis: 3.920 Cond. No. 762. ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Features VIF 0 Por 62.645242 1 Brittle 19.370696 2 AI 122.734765 3 VR 259.597778 4 Perm 16.887354 5 TOC 33.668133 In [24]: file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) . iloc [:, 1 :] features = [ 'Por' , 'Brittle' , 'AI' , 'VR' , 'Perm' , 'TOC' ] target = 'Prod' # preprocessing required by sklearn X = df [ features ] . values . reshape ( - 1 , len ( features )) y = df [ target ] . values # statsmodel X_st = sm . add_constant ( X ) model = sm . OLS ( y , X_st ) . fit () predictions = model . predict ( X_st ) print ( model . summary ()) # Compute VIF vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) print ( ' \\n\\n\\n\\n ' ) print ( vif ) coefs = [] for i in range ( 1000 ): df_temp = df . sample ( frac = 0.95 ) X = df_temp [ features ] . values . reshape ( - 1 , len ( features )) y = df_temp [ target ] . values ols = linear_model . LinearRegression () model = ols . fit ( X , y ) coefs . append ( model . coef_ ) coefs = np . array ( coefs ) fig , axes = plt . subplots ( 1 , len ( features ), figsize = ( 4 * len ( features ), 4 )) for i , feature in enumerate ( features ): axes [ i ] . hist ( coefs [:, i ], bins = 'auto' ) axes [ i ] . set_title ( feature ) axes [ i ] . set_ylabel ( 'Count' ) axes [ i ] . set_xlabel ( 'Regression coefficient value' ) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.960 Model: OLS Adj. R-squared: 0.959 Method: Least Squares F-statistic: 768.8 Date: Wed, 27 Nov 2019 Prob (F-statistic): 8.13e-132 Time: 04:57:23 Log-Likelihood: -1341.7 No. Observations: 200 AIC: 2697. Df Residuals: 193 BIC: 2721. Df Model: 6 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -1230.2620 181.565 -6.776 0.000 -1588.369 -872.155 x1 230.2958 10.022 22.980 0.000 210.530 250.062 x2 24.9967 1.490 16.772 0.000 22.057 27.936 x3 -363.7361 69.866 -5.206 0.000 -501.534 -225.938 x4 783.1874 126.371 6.198 0.000 533.941 1032.434 x5 116.2280 13.986 8.310 0.000 88.642 143.814 x6 -77.4367 80.643 -0.960 0.338 -236.492 81.619 ============================================================================== Omnibus: 16.412 Durbin-Watson: 1.938 Prob(Omnibus): 0.000 Jarque-Bera (JB): 19.341 Skew: -0.607 Prob(JB): 6.31e-05 Kurtosis: 3.920 Cond. No. 762. ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Features VIF 0 Por 62.645242 1 Brittle 19.370696 2 AI 122.734765 3 VR 259.597778 4 Perm 16.887354 5 TOC 33.668133 In [ ]: In [15]: import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from statsmodels.stats.outliers_influence import variance_inflation_factor import statsmodels.api as sm Raw In [3]: file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) . iloc [:, 1 :] features = [ 'Por' , 'Brittle' ] target = 'Prod' X = df [ features ] . values . reshape ( - 1 , len ( features )) y = df [ target ] . values vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) vif Out[3]: Features VIF 0 Por 7.363348 1 Brittle 7.363348 In [3]: coefs = [] for i in range ( 1000 ): df_temp = df . sample ( frac = 0.7 ) X = df_temp [ features ] . values . reshape ( - 1 , len ( features )) y = df_temp [ target ] . values ols = linear_model . LinearRegression () model = ols . fit ( X , y ) coefs . append ( model . coef_ ) coefs = np . array ( coefs ) for i , feature in enumerate ( features ): fig , ax = plt . subplots () ax . hist ( coefs [:, i ]) ax . set_title ( feature ) In [11]: file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) . iloc [:, 1 :] features = [ 'Por' , 'Brittle' , 'AI' , 'VR' , 'Perm' , 'TOC' ] target = 'Prod' X = df [ features ] . values . reshape ( - 1 , len ( features )) y = df [ target ] . values vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) vif Out[11]: Features VIF 0 Por 225.857649 1 Brittle 39.077458 2 AI 208.845031 3 VR 362.619204 4 Perm 27.982176 5 TOC 35.538260 6 Prod 401.944335 In [5]: coefs = [] for i in range ( 1000 ): df_temp = df . sample ( frac = 0.7 ) X = df_temp [ features ] . values . reshape ( - 1 , len ( features )) y = df_temp [ target ] . values ols = linear_model . LinearRegression () model = ols . fit ( X , y ) coefs . append ( model . coef_ ) coefs = np . array ( coefs ) for i , feature in enumerate ( features ): fig , ax = plt . subplots () ax . hist ( coefs [:, i ]) ax . set_title ( feature ) Centered In [55]: file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) . iloc [:, 1 :] features = [ 'Por' , 'Brittle' ] target = 'Prod' scalar = StandardScaler ( with_std = False ) X = scalar . fit_transform ( df [ features ]) X = X . reshape ( - 1 , len ( features )) y = df [ target ] . values vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) vif Out[55]: Features VIF 0 Por 1.05017 1 Brittle 1.05017 In [56]: coefs = [] for i in range ( 1000 ): df_temp = df . sample ( frac = 0.7 ) X = df_temp [ features ] . values . reshape ( - 1 , len ( features )) y = df_temp [ target ] . values ols = linear_model . LinearRegression () model = ols . fit ( X , y ) coefs . append ( model . coef_ ) coefs = np . array ( coefs ) for i , feature in enumerate ( features ): fig , ax = plt . subplots () ax . hist ( coefs [:, i ]) ax . set_title ( feature ) In [57]: file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) . iloc [:, 1 :] features = [ 'Por' , 'Brittle' , 'AI' , 'VR' , 'Perm' , 'TOC' ] target = 'Prod' scalar = StandardScaler ( with_std = False ) X = scalar . fit_transform ( df [ features ]) X = X . reshape ( - 1 , len ( features )) y = df [ target ] . values vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) vif Out[57]: Features VIF 0 Por 4.329838 1 Brittle 2.165476 2 AI 7.660318 3 VR 7.057671 4 Perm 2.862487 5 TOC 7.365773 In [58]: coefs = [] for i in range ( 1000 ): df_temp = df . sample ( frac = 0.7 ) X = df_temp [ features ] . values . reshape ( - 1 , len ( features )) y = df_temp [ target ] . values ols = linear_model . LinearRegression () model = ols . fit ( X , y ) coefs . append ( model . coef_ ) coefs = np . array ( coefs ) for i , feature in enumerate ( features ): fig , ax = plt . subplots () ax . hist ( coefs [:, i ]) ax . set_title ( feature ) PCA In [6]: import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from statsmodels.stats.outliers_influence import variance_inflation_factor In [25]: file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) . iloc [:, 1 :] features = [ 'Por' , 'Brittle' , 'AI' , 'VR' , 'Perm' , 'TOC' ] target = 'Prod' # Standardization scalar = StandardScaler () X = scalar . fit_transform ( df [ features ]) # PCA pca = PCA ( n_components = len ( features )) X = pca . fit_transform ( X ) # Reconstruct df with transformed features df_trans = pd . DataFrame ( X ) df_trans [ target ] = df [ target ] df_trans . columns = features + [ target ] # preprocessing required by sklearn X = df_trans [ features ] . values . reshape ( - 1 , len ( features )) y = df_trans [ target ] . values # statsmodel X_st = sm . add_constant ( X ) model = sm . OLS ( y , X_st ) . fit () predictions = model . predict ( X_st ) print ( model . summary ()) vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) print ( vif ) coefs = [] for i in range ( 1000 ): # randomly sample 70% of the original data. This is essentially bootstrapping df_temp = df_trans . sample ( frac = 0.7 ) X = df_temp [ features ] . values . reshape ( - 1 , len ( features )) y = df_temp [ target ] . values ols = linear_model . LinearRegression () model = ols . fit ( X , y ) coefs . append ( model . coef_ ) coefs = np . array ( coefs ) fig , axes = plt . subplots ( 1 , len ( features ), figsize = ( 4 * len ( features ), 4 )) for i , feature in enumerate ( features ): axes [ i ] . hist ( coefs [:, i ], bins = 'auto' ) axes [ i ] . set_title ( feature ) axes [ i ] . set_ylabel ( 'Count' ) axes [ i ] . set_xlabel ( 'Regression coefficient value' ) fig . tight_layout () OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.960 Model: OLS Adj. R-squared: 0.959 Method: Least Squares F-statistic: 768.8 Date: Wed, 27 Nov 2019 Prob (F-statistic): 8.13e-132 Time: 04:57:36 Log-Likelihood: -1341.7 No. Observations: 200 AIC: 2697. Df Residuals: 193 BIC: 2721. Df Model: 6 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 4311.2199 14.274 302.032 0.000 4283.067 4339.373 x1 464.2095 8.698 53.368 0.000 447.054 481.365 x2 351.3879 11.515 30.516 0.000 328.676 374.099 x3 -334.4029 15.168 -22.046 0.000 -364.320 -304.486 x4 230.9533 17.364 13.300 0.000 196.705 265.202 x5 456.9751 35.356 12.925 0.000 387.242 526.709 x6 127.3438 66.455 1.916 0.057 -3.728 258.415 ============================================================================== Omnibus: 16.412 Durbin-Watson: 1.938 Prob(Omnibus): 0.000 Jarque-Bera (JB): 19.341 Skew: -0.607 Prob(JB): 6.31e-05 Kurtosis: 3.920 Cond. No. 7.64 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Features VIF 0 Por 1.0 1 Brittle 1.0 2 AI 1.0 3 VR 1.0 4 Perm 1.0 5 TOC 1.0 In [26]: file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) . iloc [:, 1 :] features = [ 'Por' , 'Brittle' , 'AI' , 'Perm' ] target = 'Prod' # Standardization scalar = StandardScaler () X = scalar . fit_transform ( df [ features ]) # PCA pca = PCA ( n_components = len ( features )) X = pca . fit_transform ( X ) # Reconstruct df with transformed features df_trans = pd . DataFrame ( X ) df_trans [ target ] = df [ target ] df_trans . columns = features + [ target ] # preprocessing required by sklearn X = df_trans [ features ] . values . reshape ( - 1 , len ( features )) y = df_trans [ target ] . values # statsmodel X_st = sm . add_constant ( X ) model = sm . OLS ( y , X_st ) . fit () predictions = model . predict ( X_st ) print ( model . summary ()) vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) print ( vif ) coefs = [] for i in range ( 1000 ): df_temp = df_trans . sample ( frac = 0.7 ) X = df_temp [ features ] . values . reshape ( - 1 , len ( features )) y = df_temp [ target ] . values ols = linear_model . LinearRegression () model = ols . fit ( X , y ) coefs . append ( model . coef_ ) coefs = np . array ( coefs ) fig , axes = plt . subplots ( 1 , len ( features ), figsize = ( 4 * len ( features ), 4 )) for i , feature in enumerate ( features ): axes [ i ] . hist ( coefs [:, i ], bins = 'auto' ) axes [ i ] . set_title ( feature ) axes [ i ] . set_ylabel ( 'Count' ) axes [ i ] . set_xlabel ( 'Regression coefficient value' ) fig . tight_layout () OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.940 Model: OLS Adj. R-squared: 0.939 Method: Least Squares F-statistic: 766.9 Date: Wed, 27 Nov 2019 Prob (F-statistic): 4.69e-118 Time: 04:57:42 Log-Likelihood: -1381.5 No. Observations: 200 AIC: 2773. Df Residuals: 195 BIC: 2790. Df Model: 4 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 4311.2199 17.324 248.857 0.000 4277.053 4345.386 x1 517.4663 12.016 43.065 0.000 493.769 541.164 x2 -561.5111 17.913 -31.347 0.000 -596.839 -526.183 x3 89.4061 19.509 4.583 0.000 50.931 127.882 x4 -564.3426 38.993 -14.473 0.000 -641.244 -487.441 ============================================================================== Omnibus: 21.092 Durbin-Watson: 1.926 Prob(Omnibus): 0.000 Jarque-Bera (JB): 26.690 Skew: -0.706 Prob(JB): 1.60e-06 Kurtosis: 4.100 Cond. No. 3.25 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Features VIF 0 Por 1.0 1 Brittle 1.0 2 AI 1.0 3 Perm 1.0 In [27]: file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) . iloc [:, 1 :] features = [ 'Por' , 'Brittle' ] target = 'Prod' # Standardization scalar = StandardScaler () X = scalar . fit_transform ( df [ features ]) # PCA pca = PCA ( n_components = len ( features )) X = pca . fit_transform ( X ) # Reconstruct df with transformed features df_trans = pd . DataFrame ( X ) df_trans [ target ] = df [ target ] df_trans . columns = features + [ target ] # preprocessing required by sklearn X = df_trans [ features ] . values . reshape ( - 1 , len ( features )) y = df_trans [ target ] . values # statsmodel X_st = sm . add_constant ( X ) model = sm . OLS ( y , X_st ) . fit () predictions = model . predict ( X_st ) print ( model . summary ()) vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) print ( vif ) coefs = [] for i in range ( 1000 ): df_temp = df_trans . sample ( frac = 0.7 ) X = df_temp [ features ] . values . reshape ( - 1 , len ( features )) y = df_temp [ target ] . values ols = linear_model . LinearRegression () model = ols . fit ( X , y ) coefs . append ( model . coef_ ) coefs = np . array ( coefs ) fig , axes = plt . subplots ( 1 , len ( features ), figsize = ( 4 * len ( features ), 4 )) for i , feature in enumerate ( features ): axes [ i ] . hist ( coefs [:, i ], bins = 'auto' ) axes [ i ] . set_title ( feature ) axes [ i ] . set_ylabel ( 'Count' ) axes [ i ] . set_xlabel ( 'Regression coefficient value' ) fig . tight_layout () OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.933 Model: OLS Adj. R-squared: 0.932 Method: Least Squares F-statistic: 1373. Date: Wed, 27 Nov 2019 Prob (F-statistic): 2.14e-116 Time: 04:57:46 Log-Likelihood: -1392.8 No. Observations: 200 AIC: 2792. Df Residuals: 197 BIC: 2802. Df Model: 2 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const 4311.2199 18.241 236.347 0.000 4275.247 4347.193 x1 358.7432 16.524 21.710 0.000 326.156 391.330 x2 984.1435 20.635 47.693 0.000 943.450 1024.837 ============================================================================== Omnibus: 10.823 Durbin-Watson: 1.960 Prob(Omnibus): 0.004 Jarque-Bera (JB): 12.252 Skew: -0.444 Prob(JB): 0.00219 Kurtosis: 3.825 Cond. No. 1.25 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Features VIF 0 Por 1.0 1 Brittle 1.0 In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [4]: file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) . iloc [:, 1 :] features = [ 'Por' , 'Brittle' , 'Perm' , 'AI' ] target = 'Prod' scalar = StandardScaler () X = scalar . fit_transform ( df [ features ]) pca = PCA ( n_components = len ( features )) X = pca . fit_transform ( X ) X = X . reshape ( - 1 , len ( features )) y = df [ target ] . values vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) vif Out[4]: Features VIF 0 Por 1.0 1 Brittle 1.0 2 Perm 1.0 3 AI 1.0 In [5]: coefs = [] for i in range ( 1000 ): df_temp = df . sample ( frac = 0.7 ) scalar = StandardScaler () X = scalar . fit_transform ( df_temp . values ) pca = PCA ( n_components = len ( features )) X = pca . fit_transform ( X ) X = X . reshape ( - 1 , len ( features )) y = df_temp [ target ] . values ols = linear_model . LinearRegression () model = ols . fit ( X , y ) coefs . append ( model . coef_ ) coefs = np . array ( coefs ) fig , axes = plt . subplots ( 1 , len ( features ), figsize = ( 4 * len ( features ), 4 )) for i , feature in enumerate ( features ): axes [ i ] . hist ( coefs [:, i ], bins = 'auto' ) axes [ i ] . set_title ( feature ) axes [ i ] . set_ylabel ( 'Count' ) axes [ i ] . set_xlabel ( 'Regression coefficient value' ) In [6]: file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) . iloc [:, 1 :] features = [ 'Por' , 'Brittle' , 'AI' , 'VR' , 'Perm' , 'TOC' ] target = 'Prod' scalar = StandardScaler () X = scalar . fit_transform ( df . values ) pca = PCA ( n_components = len ( features )) X = pca . fit_transform ( X ) X = X . reshape ( - 1 , len ( features )) y = df [ target ] . values vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) vif Out[6]: Features VIF 0 Por 1.0 1 Brittle 1.0 2 AI 1.0 3 VR 1.0 4 Perm 1.0 5 TOC 1.0 In [7]: coefs = [] r = [] for i in range ( 1000 ): df_temp = df . sample ( frac = 0.7 ) scalar = StandardScaler () X = scalar . fit_transform ( df_temp . values ) pca = PCA ( n_components = len ( features )) X = pca . fit_transform ( X ) X = X . reshape ( - 1 , len ( features )) y = df_temp [ target ] . values ols = linear_model . LinearRegression () model = ols . fit ( X , y ) coefs . append ( model . coef_ ) r . append ( model . score ( X , y )) coefs = np . array ( coefs ) for i , feature in enumerate ( features ): fig , ax = plt . subplots () ax . hist ( coefs [:, i ], bins = 'auto' ) ax . set_title ( feature ) In [9]: coefs [:, - 1 ] Out[9]: array([-214.5982385 , 123.74342539, 70.77110659, 56.87212411, 56.53388293, 164.46445716, 131.33881182, 55.34281759, 166.52876831, 58.30200155, 129.30784068, 110.76253426, -8.5205867 , 75.96285387, 162.9913495 , 134.23211329, 86.28961524, 258.79667236, 41.25975719, 174.58824505, 165.76815614, 192.83252518, 162.09912498, 181.0609212 , 133.6041303 , 91.74225786, 162.3437963 , 134.25806308, 167.63417848, -243.73087435, 151.58283645, -182.96664201, -73.94919992, -143.68758187, 211.68019925, 141.48369222, -252.84766516, 238.54549752, 170.26866752, 103.30889493, -216.32541621, 97.50450336, 62.22314081, 122.47958225, -182.42362655, 120.40220087, 82.49752192, 105.8806393 , 78.66881573, 88.34884903, 126.40364343, 7.58734101, 118.10461083, 141.16959885, -241.62083627, -12.79692893, 105.66806216, 67.33793977, -182.7729212 , -185.66768272, 113.02959838, -147.577371 , 172.90681987, 166.71607524, 160.32174304, -253.17436694, -215.42973029, 117.99318185, 86.81386943, 166.3474467 , 168.31909227, 99.31543211, 84.36225258, 190.83614897, 166.36978274, 119.85705129, 123.9452355 , 137.07310241, 127.76083803, -133.45006571, 143.28465943, 114.97030216, 43.27042522, 17.78113247, -161.25208275, 124.07063735, 173.45604768, 41.69840552, 178.20377023, -214.82069899, 117.77185253, -204.3129187 , 166.7098911 , 65.17399365, -269.82371723, 102.69108338, 160.73733486, 183.20859852, 112.17114952, 155.06181784, 140.23243029, 150.31892423, 164.42032406, 102.47760392, 161.40099926, 89.18811551, 142.54813524, 163.8459093 , 94.70866718, 117.98599092, 130.90071382, 184.28189598, 165.8972656 , 177.39778289, -214.44198247, 117.57713244, 231.02078686, 184.48781783, -197.57425337, 65.63679398, 140.83040853, -216.05158204, 176.01468798, 135.37247068, -204.71929022, 64.0407609 , 175.21572955, 37.34610496, -134.3373886 , 109.46703207, 90.34458093, 124.76352298, -254.87203776, 128.86427982, 145.14117026, 177.03812043, 166.93101162, 72.77081618, 42.95643621, 141.97176488, 38.71606935, 54.58565691, 77.29024827, -233.08222067, 162.59733624, -120.56795901, 76.45211708, 128.95358436, 128.48591972, -226.82306485, 88.75151751, 137.83945522, 152.70680012, -179.30522893, 78.22020141, 148.71579243, 221.84497214, 172.49207411, -174.9545077 , 106.61291861, -298.27955536, 130.8311747 , 153.57063648, -221.84596971, 123.75089456, 133.71608275, 154.91642256, 147.08536898, 87.63620031, 61.99201656, 98.01357917, 51.60425638, 129.77294962, 104.72788006, 197.15441446, 42.80900556, -259.25475413, 117.26322811, 79.72794325, 48.76093713, 82.3582025 , 81.46666694, 146.31276074, -171.25164358, 105.88829181, 106.74071543, -137.75765733, 78.97406517, 145.65282194, 116.05146245, -294.87031404, 103.6119505 , 69.10688706, 99.63389298, 113.90143765, -205.98654883, 113.77645793, 89.40750492, 126.96248327, 74.49631732, 145.86949554, -228.46881358, 141.91126602, 174.18691089, 64.77035267, 211.7728219 , 163.43027013, 109.39609479, 50.25732331, 81.43440867, 113.82097296, 146.69618091, 104.63849273, 169.97176988, 203.87547857, 63.04543005, 81.24490821, 62.22944542, 118.55431587, 92.33132352, 99.9320495 , 43.5850936 , 139.53909404, 116.20256345, 146.90346162, 91.94742587, 78.52728245, 170.63712082, 109.78510745, 124.85362365, 84.4674487 , 209.31448742, 53.93278612, 196.61777972, 120.26903527, 124.22050986, -270.33902762, 94.27045688, 137.18583158, 105.52976754, 114.76579881, 130.18441767, 106.59021651, 145.19244637, 88.88359541, 142.72870875, 189.9778387 , 185.00754071, 33.14231613, 205.32655322, -20.0625295 , 158.74597113, 158.91523223, -6.72344696, 113.33682708, 74.77939934, 143.23871165, 124.50904815, 129.26476903, 157.41167186, 70.69449991, 40.53825392, 143.60174365, -8.90313088, 143.66105482, -241.08201005, 212.89977601, 197.31441818, 143.02884636, 96.24283628, 166.58939485, 104.13259687, 144.78853404, -32.07869219, 142.76665685, 166.19909458, 179.80241048, -109.55903629, 105.18717538, 97.92291912, 204.22158625, 99.04936399, 58.33409606, 93.28624923, 136.35710283, 155.48242088, -135.14759777, 118.91147556, 199.17982602, -9.34069085, 154.01305046, 18.25229168, 169.05259145, -189.84314357, 104.91990348, 122.99859997, 206.73769053, 68.51365938, 170.5820345 , 188.13052636, 108.4912106 , 12.5551003 , 139.01200037, 132.43070835, 229.15602216, 158.87987732, -229.28715872, -186.46489715, 117.18319711, 164.68480465, 161.17731457, 103.66155667, 136.31066707, 125.78066452, 183.32139613, 172.54456711, -186.88178832, 72.09207713, 180.52571661, 89.45704004, 187.07587954, -135.39483585, 188.11802663, 80.55184597, 45.81989508, 171.45778593, -149.49135679, 139.97786797, 138.73374551, 22.07968126, -2.47583397, -149.20916852, 60.62878725, 170.20143034, 126.9207523 , 131.90535423, 167.44270919, 45.47840524, 127.90156591, 99.87598859, 46.47398032, -200.93146941, 54.97111728, 137.17601541, 9.97460429, -182.98097907, 99.80620772, 49.04440391, 134.24865397, -107.88476585, -175.27832705, 120.92595914, -253.16441954, 165.22490131, 148.59888911, 171.10251238, 58.23958291, 103.69266801, 110.2388569 , 114.48249395, 73.78178152, 144.47398514, 159.61463097, 128.81431936, -191.74356737, -159.06420557, 103.77765257, 212.08357051, 127.94843758, -257.74345134, 204.11214982, 212.95962459, 75.16522651, 173.44739413, -193.35597128, 40.59701099, -204.84771171, 123.47452911, 135.90692188, 165.32264649, -145.10866496, 47.58991149, 106.37770127, 94.4565895 , 103.07940383, 96.45115738, 206.29318057, 237.98850979, -167.76237191, 132.46070935, 154.8621279 , 175.38719192, 161.48649191, 43.79277713, 98.72192707, 156.94745144, -223.93995835, -144.38019656, 176.08787456, 116.36937766, 84.33224043, 85.1256769 , 187.31990344, 119.65650994, 166.80217543, 99.55651069, 117.71426137, 89.41606522, -148.81436183, 63.23822699, 140.52895134, 90.00418714, 132.20295794, 144.99513127, 98.88973085, 170.8945972 , 118.22272771, 59.19999657, 60.68021453, 94.62171115, 70.6754385 , 110.68542076, 111.57958822, -131.04666286, 7.80540646, -226.43685638, 38.86809486, 128.0038118 , 154.48791729, 126.19561334, 174.99817968, 180.65621769, 175.31442818, -163.98739921, 122.62021723, -192.39646175, -282.26307711, 60.00787977, 71.44608262, 140.88666597, -218.60675444, 99.83975007, -11.48566801, 156.02824298, 43.49554521, 73.27822116, 182.03157725, 125.64866872, 55.3353692 , 120.8324131 , 91.02785753, 76.3190378 , 38.33947759, -283.15629125, 156.96689251, 71.43226848, 99.87530736, 204.62600934, -37.97789975, 169.33753721, 105.64345824, 81.85444944, 97.70409965, 88.44029458, 57.79130644, 88.04663054, 139.62434614, 116.98140715, 187.79230129, -185.80840052, 42.82021855, 199.95089514, 120.70785336, 73.98634859, 201.25453518, -9.84105446, 129.68246056, 139.92526108, 150.38467823, -157.82444304, 190.80110617, 120.80817178, 140.83924496, 109.98577855, 122.47151664, 129.62196694, 158.01557672, 40.92029936, 128.48562782, 51.14944869, -213.35095482, 123.38822929, 214.92169624, -186.19668894, -195.79855181, 127.52855732, 205.5478084 , -207.41536755, 172.47212821, 153.82174782, 145.54904739, 122.76474701, -211.05932714, 167.26163637, -233.39286134, 102.90011347, 44.17812833, 83.39121008, 96.51339018, 48.63179596, -202.76517342, 164.89617447, 90.34279453, 146.5299899 , 72.11710083, -187.25535842, 42.03170125, 75.51331587, 121.47454963, 93.49366643, 94.60815411, 108.15883056, 42.28080938, -211.45437093, -230.31311543, 125.75824274, 108.68478486, 164.47924064, 118.34810123, 81.85278413, 104.07291637, 204.00906228, 157.1644181 , 118.7174911 , 116.31146458, 175.92257175, 64.86570164, 19.81867274, 29.57937867, 63.50623051, 88.43433047, 49.53176045, 97.18739113, 226.5150752 , 90.16982282, 104.17967866, -186.10516142, 131.99856221, 50.44200582, 143.15008562, 79.97996961, 54.99717466, 86.79546117, 103.79538141, 147.35220919, -233.13353194, 136.1243727 , 173.02088671, 90.61245086, 173.86930735, 53.10278068, 133.44197085, 102.44389533, -12.60900381, 115.66797554, 39.11382801, 135.62653194, -230.94874435, 66.54334331, 111.00344646, 162.86653843, 183.02521278, -203.10171436, 132.84793083, -258.48581786, 106.84408644, 183.82461251, 164.09321309, 159.11825437, 174.00583789, 188.79668811, 128.93351675, 69.27768981, 135.79879823, 109.57226248, -154.04359886, 74.46672956, 95.43116375, 105.15849453, 71.64052309, -227.11800745, 170.58951589, 195.86051933, 179.08356632, 69.68358475, 83.44578254, 56.61051653, 122.64097929, 116.05784872, 102.41599344, 78.14728667, 168.43703747, 161.0270686 , 148.09118594, 78.66059012, 40.12480929, 205.6114171 , 9.1816732 , 158.66998079, 138.1471372 , 173.09645811, 102.87057472, 94.04185104, 46.93969947, 158.48065402, 132.68048855, 79.31815647, -238.94334529, 54.88148964, 137.02747352, 80.38468253, 12.45760372, 85.28732316, 166.79962315, 176.36588183, 126.83567365, 181.8045904 , 149.62229982, -1.91848148, -185.79249582, 190.09479723, 48.94017301, 69.39725563, 115.54848444, 130.28992678, 60.42372516, 178.14645684, 53.41501677, -175.74274867, -216.41966579, 81.40654538, -144.48177885, -140.34378978, 20.41991584, 29.70353937, 117.97794245, 25.42524229, -226.87962428, -265.43858076, 181.07074984, -81.81962032, 107.33887063, 97.82715008, 181.88102277, 145.69402133, 81.77282086, 147.57057168, -188.31199074, 178.50758146, 189.92883985, 145.052418 , 92.40063285, 102.70141832, 74.79774191, 109.59968646, -199.27647587, 106.47339989, -157.64071848, -191.0494594 , -171.79110884, 106.28088907, -201.38187929, 179.71712056, 84.85394409, 85.8170404 , 171.66249232, -257.84438912, 43.92177457, 1.84990915, 85.0446179 , 139.89201236, 26.18436448, 66.09915282, -211.63234496, -150.43956688, 175.66578895, 120.54887346, 77.19206487, 102.1296491 , -98.38467378, 56.69941684, 52.17329576, 131.24445343, 15.08552514, 118.69033318, 130.35198117, 119.90965685, -118.16812143, 88.76831358, -7.75905632, 109.53653846, 120.53875047, 118.07473704, 175.61761883, 141.75055273, 107.79439785, -205.60963542, 139.38113279, 114.36966599, 43.97661265, -73.14252598, 62.48222641, 216.01711804, 152.87720264, -203.76172184, 22.66160326, 142.79967892, 132.02986555, 169.91371258, 122.91539482, 125.64148042, -224.84177126, 133.8489585 , 84.75132102, 157.199029 , 170.61462097, -21.96476243, 201.11684914, -189.30317654, -287.42854319, -215.7021593 , 84.67802349, 146.78432996, 74.2347273 , 90.73569791, 96.9243704 , 142.78834402, 138.12833053, 152.09847988, 96.16196593, 78.43206319, -184.9811899 , 117.04448133, 121.25514359, 127.17133535, 118.43513848, 182.45825601, 199.62755487, 86.67041935, -126.45661182, 115.55443071, 271.87141351, -241.8420298 , 88.95135253, 146.46305671, 70.3213282 , 93.02499437, 231.51772907, 158.96983001, 80.44321951, 116.7137775 , 32.234977 , 141.44912268, -170.4731599 , -248.61251483, -152.78909513, 15.94958777, -229.33199242, 107.23805865, 176.44363147, 20.98990623, 139.07957588, 57.63385357, 124.20300142, 113.11075392, 150.87967651, 68.85393149, -272.94214562, -170.57128839, 146.2113289 , 93.62652875, 178.67520811, 36.57911181, 202.56423351, -192.23576562, 99.79662519, 119.5921685 , 41.38807679, 126.51414763, -176.74075177, 141.88309804, 125.03935736, 72.47168313, 110.06692816, 60.85569171, 277.37899613, 103.12101386, -260.73140968, 101.70654299, 159.66391137, 165.3413549 , 96.60762423, 122.01456111, 200.43879986, 109.44979682, 101.85101098, 114.7394358 , 22.62888941, 17.19566236, 154.35102695, 121.52627182, 78.14503087, 175.91641821, 99.20662395, 174.91036762, 184.19344 , 64.19258724, 149.16075586, -196.54288166, 54.83437196, -120.93265707, -240.31317962, 39.83214778, -230.57938611, 150.8329837 , 129.69989314, -150.73105364, 62.64195429, -181.72459524, 60.48847342, 179.45432073, 146.35266025, 160.99768552, 137.89694439, -222.53513895, 126.49569702, 177.34853806, 135.83600535, -187.32581653, 118.40800904, 141.47072048, 203.81388174, 43.32093609, 225.52153568, 128.53407445, 92.50107543, 134.88577851, -141.55877515, 109.8710423 , -120.16393645, 203.55607659, 290.89977559, -257.30174873, 42.23146063, 63.71472821, 133.889166 , 122.64057517, -176.26189376, 85.20228064, 149.3153191 , 237.48666751, 162.22235065, 128.14523675, -137.36266444, 92.40943448, 70.66270241, 134.51077474, 210.46768969, 45.53587543, 104.38156909, 116.53574752, 179.46805735, 63.81555994, 27.43820441, 27.67088283, 173.47879369, 41.65985349, 148.68488144, 134.7297565 , 116.38382669, 181.79129872, 178.00365004, 18.25344575, 138.72263943, 49.9945804 , 61.3167793 , 51.67851411, -250.37295851, 49.68713068, 107.9061124 , 110.82137839, -241.28274539, 175.43863056, 51.13787659, 225.03939931, 63.70903068, 99.18698851, 130.29564418, 16.98534843, 128.91191813, 77.31431789, 68.06825327, 114.98112051, 90.84125097, -151.93712609, 65.96908949, 118.432654 , 16.05957392, 60.33241875, 159.92608344, 124.86730933, 262.28552377, 179.83783612, 38.10933473, 122.7863337 , 225.84370767, 46.22195228, 37.55785597, 144.67351859, 99.16688963, 63.84033947, -237.67240875, 140.4039774 , -113.39380043, -178.93847016, 107.41732539, -269.36692865, 190.3289675 , -189.57711999, 87.80049149, 182.06974008, -229.13268777, -236.93465135, -209.04933734, 44.11004876, 81.542326 , 135.17575909, 183.64992763, 170.49032627, 135.06094722, 126.79945996, 88.30843262, -175.33517654, 155.89907718, 179.90616856, 113.09658143, -16.10999454, -291.00361308, 104.76033283, 178.52041544, 135.85590863, 89.45909192, 94.90369913, 76.40659057, 37.80562954, 163.27740817, 249.71871821, 117.82646127, 154.66170773, 111.67851317, 135.35802724, 6.4223707 , 115.84583189, -5.79290813, 109.9980748 , 82.1547783 , 76.55042845, 80.7215468 , 181.36590452, -181.91546392, 52.43429555, -122.68399971, -131.65791818, 176.06594318, 140.13632605, -220.69223705, 92.9055262 , 87.90566134, 71.48002443, 207.02536898, 29.2892793 , 206.18228686, 218.02084208, 161.04259356, 96.63736526, 72.06887818, 214.00272696, 171.61495174, -210.8682084 , 119.69015909, 91.28868434, 92.34031503, -151.16372216, 118.51640342, 78.63620582, 33.10596376, -178.06644982, 114.66198081, 72.25886726, 105.26504631]) In [ ]: In [ ]: In [ ]: In [ ]: In [59]: df_temp = df . sample ( frac = 0.7 ) scalar = StandardScaler () X = scalar . fit_transform ( df_temp . values ) pca = PCA ( n_components = len ( features )) X = pca . fit_transform ( X ) In [61]: pca . explained_variance_ Out[61]: array([3.36520575, 1.6300189 , 1.05796411, 0.75059836, 0.17773085, 0.04717506]) In [62]: features Out[62]: ['Por', 'Brittle', 'AI', 'VR', 'Perm', 'TOC'] In [ ]: In [ ]: In [70]: import statsmodels.api as sm res = sm . OLS ( y , X ) . fit () --------------------------------------------------------------------------- ImportError Traceback (most recent call last) in () ----> 1 import statsmodels . api as sm 2 3 res = sm . OLS ( y , X ) . fit ( ) C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\api.py in () 14 from . import robust 15 from . robust . robust_linear_model import RLM ---> 16 from .discrete.discrete_model import (Poisson, Logit, Probit, 17 MNLogit , NegativeBinomial , 18 GeneralizedPoisson , C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\discrete\\discrete_model.py in () 43 44 from statsmodels . base . l1_slsqp import fit_l1_slsqp ---> 45 from statsmodels . distributions import genpoisson_p 46 47 try : C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\distributions\\__init__.py in () 1 from . empirical_distribution import ECDF , monotone_fn_inverter , StepFunction ----> 2 from . edgeworth import ExpandedNormal 3 from . discrete import genpoisson_p , zipoisson , zigenpoisson , zinegbin C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\distributions\\edgeworth.py in () 5 import numpy as np 6 from numpy . polynomial . hermite_e import HermiteE ----> 7 from scipy . misc import factorial 8 from scipy . stats import rv_continuous 9 import scipy . special as special ImportError : cannot import name 'factorial' In [28]: import pandas as pd import numpy as np from sklearn import linear_model import matplotlib.pyplot as plt from sklearn.decomposition import PCA from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.preprocessing import StandardScaler file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) . iloc [:, 1 :] ######################################################################################## features = [ 'Por' , 'AI' , 'VR' , 'Perm' , 'TOC' ] target = 'Prod' X = df [ features ] . values . reshape ( - 1 , len ( features )) y = df [ target ] . values pca = PCA ( n_components = len ( features )) X_new = pca . fit_transform ( X ) vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X_new , i ) for i in range ( len ( features ))]) vif Out[28]: Features VIF 0 Por 1.0 1 AI 1.0 2 VR 1.0 3 Perm 1.0 4 TOC 1.0 In [29]: ols = linear_model . LinearRegression () model = ols . fit ( X_new , y ) print ( 'Features : %s ' % features ) print ( 'Regression Coefficients : ' , model . coef_ ) print ( 'R-squared : %.2f ' % model . score ( X_new , y )) print ( 'Y-intercept : %.2f ' % model . intercept_ ) print ( '' ) Features : ['Por', 'AI', 'VR', 'Perm', 'TOC'] Regression Coefficients : [ 260.59912732 -18.97712619 90.43637731 -404.33137156 -2682.99310784] R-squared : 0.90 Y-intercept : 4311.22 In [ ]: In [ ]: In [18]: import pandas as pd import numpy as np from sklearn import linear_model import matplotlib.pyplot as plt from sklearn.decomposition import PCA from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.preprocessing import StandardScaler file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) . iloc [:, 1 :] scalar = StandardScaler ( with_std = False ) X_new = scalar . fit_transform ( df . values ) df = pd . DataFrame ( X_new , columns = df . columns ) ######################################################################################## features = [ 'Por' , 'Brittle' , 'AI' ] target = 'Prod' X = df [ features ] . values . reshape ( - 1 , len ( features )) y = df [ target ] . values vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) vif Out[18]: Features VIF 0 Por 1.313968 1 Brittle 1.051171 2 AI 1.271905 In [19]: ols = linear_model . LinearRegression () model = ols . fit ( X , y ) print ( 'Features : %s ' % features ) print ( 'Regression Coefficients : ' , model . coef_ ) print ( 'R-squared : %.2f ' % model . score ( X , y )) print ( 'Y-intercept : %.2f ' % model . intercept_ ) print ( '' ) Features : ['Por', 'Brittle', 'AI'] Regression Coefficients : [319.43449102 31.38905371 -11.04275682] R-squared : 0.93 Y-intercept : -0.00 In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [ ]: In [37]: import pandas as pd import numpy as np from sklearn import linear_model import matplotlib.pyplot as plt from sklearn.decomposition import PCA from statsmodels.stats.outliers_influence import variance_inflation_factor file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) ######################################################################################## features = [ 'Por' , 'Brittle' , 'AI' , 'VR' , 'Perm' , 'TOC' ] target = 'Prod' X = df [ features ] . values . reshape ( - 1 , len ( features )) y = df [ target ] . values vif = pd . DataFrame () vif [ 'Features' ] = features vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( X , i ) for i in range ( len ( features ))]) vif Out[37]: Features VIF 0 Por 62.645242 1 Brittle 19.370696 2 AI 122.734765 3 VR 259.597778 4 Perm 16.887354 5 TOC 33.668133 In [ ]: In [ ]: In [ ]: In [ ]: In [25]: import pandas as pd import numpy as np from statsmodels.stats.outliers_influence import variance_inflation_factor file = '500_Person_Gender_Height_Weight_Index.csv' df = pd . read_csv ( file )[[ 'Height' , 'Weight' , 'Index' ]] vif = pd . DataFrame () vif [ 'Features' ] = df . columns vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( df . values , i ) for i in range ( df . shape [ 1 ])]) In [26]: vif Out[26]: Features VIF 0 Height 11.213321 1 Weight 37.566315 2 Index 20.881544 In [ ]: In [64]: features = [ 'Por' , 'Brittle' , 'AI' , 'VR' , 'Perm' , 'TOC' ] In [68]: features = [ 'Por' , 'Brittle' , 'AI' , 'VR' , 'Perm' , 'TOC' ] file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file )[ features ] vif = pd . DataFrame () vif [ 'Features' ] = df . columns vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( df . values , i ) for i in range ( df . shape [ 1 ])]) In [69]: vif Out[69]: Features VIF 0 Por 42.751102 1 Brittle 8.576566 2 Perm 15.590663 3 TOC 10.075020 In [59]: import pandas as pd import numpy as np from sklearn import linear_model import matplotlib.pyplot as plt file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) ######################################################################################## features = [ 'Por' , 'Brittle' ] target = 'Prod' X = df [ features ] . values . reshape ( - 1 , len ( features )) y = df [ target ] . values ols = linear_model . LinearRegression () model = ols . fit ( X , y ) print ( 'Features : %s ' % features ) print ( 'Regression Coefficients : ' , [ round ( item , 2 ) for item in model . coef_ ]) print ( 'R-squared : %.2f ' % model . score ( X , y )) print ( 'Y-intercept : %.2f ' % model . intercept_ ) print ( '' ) Features : ['Por', 'Brittle'] Regression Coefficients : [320.39, 31.38] R-squared : 0.93 Y-intercept : -2003.01 In [62]: file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) df = df . iloc [:, 1 : - 1 ] vif = pd . DataFrame () vif [ 'Features' ] = df . columns vif [ 'VIF' ] = np . array ([ variance_inflation_factor ( df . values , i ) for i in range ( df . shape [ 1 ])]) vif Out[62]: Features VIF 0 Por 62.645242 1 Perm 16.887354 2 AI 122.734765 3 Brittle 19.370696 4 TOC 33.668133 5 VR 259.597778 Multicollinearity can affect any regression model with more than one predictor. It occurs when two or more predictor variables overlap so much in what they measure that their effects are indistinguishable. In [67]: import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) df = df . iloc [:, 1 : - 1 ] corr = df . corr ( method = 'spearman' ) # Set up the matplotlib figure fig , ax = plt . subplots ( figsize = ( 6 , 5 )) # Generate a custom diverging colormap cmap = sns . diverging_palette ( 220 , 10 , as_cmap = True , sep = 10 ) # Draw the heatmap with the mask and correct aspect ratio sns . heatmap ( corr , cmap = cmap , vmin =- 1 , vmax = 1 , center = 0 , linewidths =. 5 ) fig . suptitle ( 'Correlation matrix of features' , fontsize = 15 ) ax . text ( 0.77 , 0.2 , 'aegis4048.github.io' , fontsize = 13 , ha = 'center' , va = 'center' , transform = ax . transAxes , color = 'grey' , alpha = 0.5 ) fig . tight_layout () In [ ]:","tags":"Machine Learning","url":"/multicollinearity_concept_effects_detection_remedies_and_exceptions_in_python","loc":"/multicollinearity_concept_effects_detection_remedies_and_exceptions_in_python"},{"title":"Multiple Linear Regression and Visualization in Python","text":"import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from mpl_toolkits.mplot3d import Axes3D ######################################## Data preparation ######################################### file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd.read_csv(file) X = df[['Por', 'Brittle']].values.reshape(-1,2) Y = df['Prod'] ######################## Prepare model data point for visualization ############################### x = X[:, 0] y = X[:, 1] z = Y x_pred = np.linspace(6, 24, 30) # range of porosity values y_pred = np.linspace(0, 100, 30) # range of brittleness values xx_pred, yy_pred = np.meshgrid(x_pred, y_pred) model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T ################################################ Train ############################################# ols = linear_model.LinearRegression() model = ols.fit(X, Y) predicted = model.predict(model_viz) ############################################## Evaluate ############################################ r2 = model.score(X, Y) ############################################## Plot ################################################ plt.style.use('default') fig = plt.figure(figsize=(9, 4)) ax1 = fig.add_subplot(121, projection='3d') ax2 = fig.add_subplot(122, projection='3d') axes = [ax1, ax2] for ax in axes: ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5) ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0') ax.set_xlabel('Porosity (%)', fontsize=12) ax.set_ylabel('Brittleness', fontsize=12) ax.set_zlabel('Gas Prod. (Mcf/day)', fontsize=12) ax.locator_params(nbins=4, axis='x') ax.locator_params(nbins=5, axis='x') ax1.view_init(elev=28, azim=120) ax2.view_init(elev=4, azim=114) fig.suptitle('3D multiple linear regression model', fontsize=20) fig.tight_layout() There are many advanced machine learning methods with robust prediction accuracy. While complex models may outperform simple models in predicting a response variable, simple models are better for understanding the impact & importance of each feature on a response variable. When the task at hand can be described by a linear model, linear regression triumphs over all other machine learning methods in feature interpretation due to its simplicity. This post attempts to help your understanding of linear regression in multi-dimensional feature space, model accuracy assessment, and provide code snippets for multiple linear regression in Python. Contents 0 Sample data description 1 Multiple linear regression Notes: Data encoding - regression with categorical variables Pythonic Tip: 2D regression with scikit-learn Pythonic Tip: Forcing zero y-intercept Pythonic Tip: 3D+ regression with scikit-learn 2 Introduction to multicollinearity 0. Sample data description We will use one sample data throughout this post. The sample data is relevant to the oil & gas industry. It is originally from Dr. Michael Pyrcz , petroleum engineering professor at the University of Texas at Austin. The original data can be found from his github repo . You can also use direct download , or directly access it using pandas url like below: In [ ]: In [ ]: In [1]: import pandas as pd file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) In [2]: df . head ( 10 ) Out[2]: Well Por Perm AI Brittle TOC VR Prod 0 1 12.08 2.92 2.80 81.40 1.16 2.31 4165.196191 1 2 12.38 3.53 3.22 46.17 0.89 1.88 3561.146205 2 3 14.02 2.59 4.01 72.80 0.89 2.72 4284.348574 3 4 17.67 6.75 2.63 39.81 1.08 1.88 5098.680869 4 5 17.52 4.57 3.18 10.94 1.51 1.90 3406.132832 5 6 14.53 4.81 2.69 53.60 0.94 1.67 4395.763259 6 7 13.49 3.60 2.93 63.71 0.80 1.85 4104.400989 7 8 11.58 3.03 3.25 53.00 0.69 1.93 3496.742701 8 9 12.52 2.72 2.43 65.77 0.95 1.98 4025.851153 9 10 13.25 3.94 3.71 66.20 1.14 2.65 4285.026122 Description of headers Well : well index Por : well average porosity (%) Perm : permeability (mD) AI : accoustic impedance (kg/m2s*10&#94;6) Brittle : brittleness ratio (%) TOC : total organic carbon (%) VR : vitrinite reflectance (%) Prod : gas production per day (MCFD) - Response Variable We have six features ( Por, Perm, AI, Brittle, TOC, VR ) to predict the response variable ( Prod ). Based on the permutation feature importances shown in figure (1) , Por is the most important feature, and Brittle is the second most important feature. Permutation feature ranking is out of the scope of this post, and will not be discussed in detail. Feature importances are obtained with rfpimp python library. For more information about permutation feature ranking, refer to this article: Beware Default Random Forest Importances Figure 1: Permutation feature ranking Source Code For Figure (1) import rfpimp import pandas as pd import numpy as np from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split ######################################## Data preparation ######################################### file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd.read_csv(file) features = ['Por', 'Perm', 'AI', 'Brittle', 'TOC', 'VR', 'Prod'] ######################################## Train/test split ######################################### df_train, df_test = train_test_split(df, test_size=0.20) df_train = df_train[features] df_test = df_test[features] X_train, y_train = df_train.drop('Prod',axis=1), df_train['Prod'] X_test, y_test = df_test.drop('Prod',axis=1), df_test['Prod'] ################################################ Train ############################################# rf = RandomForestRegressor(n_estimators=100, n_jobs=-1) rf.fit(X_train, y_train) ############################### Permutation feature importance ##################################### imp = rfpimp.importances(rf, X_test, y_test) ############################################## Plot ################################################ fig, ax = plt.subplots(figsize=(6, 3)) ax.barh(imp.index, imp['Importance'], height=0.8, facecolor='grey', alpha=0.8, edgecolor='k') ax.set_xlabel('Importance score') ax.set_title('Permutation feature importance') ax.text(0.8, 0.15, 'aegis4048.github.io', fontsize=12, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) plt.gca().invert_yaxis() fig.tight_layout() 1. Multiple linear regression Multiple linear regression model has the following structure: $$ y = \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_n x_n + \\beta_0\\tag{1}$$ where $y$ : response variable $n$ : number of features $x_n$ : $n$ -th feature $\\beta_n$ : regression coefficient (weight) of the $n$ -th feature $\\beta_0$ : y -intercept Bivarate linear regression model (that can be visualized in 2D space) is a simplification of eq (1) . Bivariate model has the following structure: $$ y = \\beta_1 x_1 + \\beta_0 \\tag{2}$$ A picture is worth a thousand words. Let's try to understand the properties of multiple linear regression models with visualizations. First, 2D bivariate linear regression model is visualized in figure (2) , using Por as a single feature. Although porosity is the most important feature regarding gas production, porosity alone captured only 74% of variance of the data. Figure 2: 2D Linear regression model Source Code For Figure (2) import pandas as pd import matplotlib.pyplot as plt from sklearn import linear_model ######################################## Data preparation ######################################### file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd.read_csv(file) X = df['Por'].values.reshape(-1,1) y = df['Prod'].values ################################################ Train ############################################# ols = linear_model.LinearRegression() model = ols.fit(X, y) response = model.predict(X) ############################################## Evaluate ############################################ r2 = model.score(X, y) ############################################## Plot ################################################ plt.style.use('default') plt.style.use('ggplot') fig, ax = plt.subplots(figsize=(8, 4)) ax.plot(X, response, color='k', label='Regression model') ax.scatter(X, y, edgecolor='k', facecolor='grey', alpha=0.7, label='Sample data') ax.set_ylabel('Gas production (Mcf/day)', fontsize=14) ax.set_xlabel('Porosity (%)', fontsize=14) ax.text(0.8, 0.1, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) ax.legend(facecolor='white', fontsize=11) ax.set_title('$R&#94;2= %.2f$' % r2, fontsize=18) fig.tight_layout() How would the model look like in 3D space? Let's take a look at figure (3) . Due to the 3D nature of the plot, multiple plots were generated from different angles. Two features ( Por and Brittle ) were used to predict the response variable Prod . With the help of the additional feature Brittle , the linear model experience significant gain in accuracy, now capturing 93% variability of data. Figure 3: 3D Linear regression model with strong features Source Code For Figure (3) import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from mpl_toolkits.mplot3d import Axes3D ######################################## Data preparation ######################################### file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd.read_csv(file) X = df[['Por', 'Brittle']].values.reshape(-1,2) Y = df['Prod'] ######################## Prepare model data point for visualization ############################### x = X[:, 0] y = X[:, 1] z = Y x_pred = np.linspace(6, 24, 30) # range of porosity values y_pred = np.linspace(0, 100, 30) # range of brittleness values xx_pred, yy_pred = np.meshgrid(x_pred, y_pred) model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T ################################################ Train ############################################# ols = linear_model.LinearRegression() model = ols.fit(X, Y) predicted = model.predict(model_viz) ############################################## Evaluate ############################################ r2 = model.score(X, Y) ############################################## Plot ################################################ plt.style.use('default') fig = plt.figure(figsize=(12, 4)) ax1 = fig.add_subplot(131, projection='3d') ax2 = fig.add_subplot(132, projection='3d') ax3 = fig.add_subplot(133, projection='3d') axes = [ax1, ax2, ax3] for ax in axes: ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5) ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0') ax.set_xlabel('Porosity (%)', fontsize=12) ax.set_ylabel('Brittleness', fontsize=12) ax.set_zlabel('Gas Prod. (Mcf/day)', fontsize=12) ax.locator_params(nbins=4, axis='x') ax.locator_params(nbins=5, axis='x') ax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax1.transAxes, color='grey', alpha=0.5) ax2.text2D(0.3, 0.42, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax2.transAxes, color='grey', alpha=0.5) ax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax3.transAxes, color='grey', alpha=0.5) ax1.view_init(elev=28, azim=120) ax2.view_init(elev=4, azim=114) ax3.view_init(elev=60, azim=165) fig.suptitle('$R&#94;2 = %.2f$' % r2, fontsize=20) fig.tight_layout() How would the 3D linear model look like if less powerful features are selected? Let's choose Por and VR as our new features and fit a linear model. In figure (4) below, we see that R-squared decreased compared to figure (3) above. The effect of decreased model performance can be visually observed by comparing their middle plots; the scatter plots in figure (3) are more densely populated around the 2D model plane than the scatter plots in figure (4) . Figure 4: 3D Linear regression model with weak features Source Code For Figure (4) import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from mpl_toolkits.mplot3d import Axes3D ######################################## Data preparation ######################################### file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd.read_csv(file) X = df[['Por', 'VR']].values.reshape(-1,2) Y = df['Prod'] ######################## Prepare model data point for visualization ############################### x = X[:, 0] y = X[:, 1] z = Y x_pred = np.linspace(6, 24, 30) # range of porosity values y_pred = np.linspace(0.93, 2.9, 30) # range of VR values xx_pred, yy_pred = np.meshgrid(x_pred, y_pred) model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T ################################################ Train ############################################# ols = linear_model.LinearRegression() model = ols.fit(X, Y) predicted = model.predict(model_viz) ############################################## Evaluate ############################################ r2 = model.score(X, Y) ############################################## Plot ################################################ plt.style.use('default') fig = plt.figure(figsize=(12, 4)) ax1 = fig.add_subplot(131, projection='3d') ax2 = fig.add_subplot(132, projection='3d') ax3 = fig.add_subplot(133, projection='3d') axes = [ax1, ax2, ax3] for ax in axes: ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5) ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0') ax.set_xlabel('Porosity (%)', fontsize=12) ax.set_ylabel('VR', fontsize=12) ax.set_zlabel('Gas Prod. (Mcf/day)', fontsize=12) ax.locator_params(nbins=4, axis='x') ax.locator_params(nbins=5, axis='x') ax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax1.transAxes, color='grey', alpha=0.5) ax2.text2D(0.3, 0.42, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax2.transAxes, color='grey', alpha=0.5) ax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax3.transAxes, color='grey', alpha=0.5) ax1.view_init(elev=27, azim=112) ax2.view_init(elev=16, azim=-51) ax3.view_init(elev=60, azim=165) fig.suptitle('$R&#94;2 = %.2f$' % r2, fontsize=20) fig.tight_layout() The full-rotation view of linear models are constructed below in a form of gif. Notice that the blue plane is always projected linearly, no matter of the angle. This is the reason that we call this a multiple \"LINEAR\" regression model. The model will always be linear, no matter of the dimensionality of your features. When you have more than 3 features, the model will be very difficult to be visualized, but you can expect that high dimensional linear models will also exhibit linear trend within their feature space. Figure 5: Porosity and Brittleness Linear model GIF Figure 6: Porosity and VR Linear model GIF The gif was generated by creating 360 different plots viewed from different angles with the following code snippet, and combined into a single gif from imgflip . In [ ]: for ii in np . arange ( 0 , 360 , 1 ): ax . view_init ( elev = 32 , azim = ii ) fig . savefig ( 'gif_image %d .png' % ii ) Notes: Data encoding - regression with categorical variables Regression requires features to be continuous. What happens if you have categorical features that are important? Do you have to ignore categorical variables, and run regression only with continuous variables? We can encode categorical variables into numerical variables to avoid this issue. Take a look at the below figure. The feature level was originally a categorial variable with three categories of ordinality. This means that there are hierarchy among the categories (ex: low < medium < high), and that their encoding needs to capture their ordinality. It is achieved by converting them in to 1, 2, and 3. This kind of encoding is called integer encoding What if there are no ordinality among the categories of a feature? Then we use a technique called one-hot encoding to prevent a model from assuming natural ordering among categories that may suffer from model bias. Instead of using integer variables, we use binary variables. 1 indicates that the sample data falls into the specified category, while 0 indicates the otherwise. One-hot encoding is used in almost all natural languages problems, because vocabularies do not have ordinal relationships among themselves. Pythonic Tip: 2D linear regression with scikit-learn Linear regression is implemented in scikit-learn with sklearn.linear_model (check the documentation ). For code demonstration, we will use the same oil & gas data set described in Section 0: Sample data description above . Data prepration First, import modules and data. We will use a single feature: Por . Response variable is Prod . In [39]: import pandas as pd from sklearn import linear_model import numpy as np file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) Preprocess If you get error messages like ValueError: Expected 2D array, got 1D array instead: ... , its the issue of preprocessing. Most scikit-learn training functions require reshape of features, such as reshape(-1, len(features)) . In case you import data from Pandas dataframe, the first argument is always -1 , and the second argument is the number of features, in a form of an integer. This preprocessing will also be required when you make predictions based on the fitted model later. Check the shape of your features and response variable if you are experiencing errors. In [40]: import numpy as np features = [ 'Por' ] target = 'Prod' X = df [ features ] . values . reshape ( - 1 , len ( features )) y = df [ target ] . values In [41]: print ( X . shape ) print ( y . shape ) (200, 1) (200,) Fit linear model Since we have only one feature, the linear model we want to fit has the following structure: $$ \\text{Gas Prod.} = \\beta_1 \\cdot \\text{Por} + \\beta_0 \\tag{3}$$ Let's find out the values of $\\beta_1$ (regression coefficient) and $\\beta_2$ (y-intercept). Just like many other scikit-learn libraries, you instantiate the training model object with linear_model.LinearRegression() , and than fit the model with the feature X and the response variable y . Note that ols stands for Ordinary Least Squares. In [42]: from sklearn import linear_model ols = linear_model . LinearRegression () model = ols . fit ( X , y ) The linear regression coefficient can be accessed in a form of class attribute with model.coef_ In [43]: model . coef_ Out[43]: array([287.78074285]) The y-intercept can be accessed in a form of class attribute with model.intercept_ In [44]: model . intercept_ Out[44]: -2.9444310537137426 Based on the result of the fit, we conclude that the gas production can be predicted from porosity, with the following linear model: $$ \\text{Gas Prod.} = 287.78 \\cdot \\text{Por} - 2.94 \\tag{4}$$ Accuracy assessment: $R&#94;2$ How good was your model? You can evaluate your model performance in a form of R-squared, with model.score(X, y) . X is the features, and y is the response variable used to fit the model. In [45]: model . score ( X , y ) Out[45]: 0.7428880535051594 Make future prediction Scikit-learn supports making predictions based on the fitted model with model.predict(X) method. X is a feature that requires preprocessing explained above . Recall that we used only one feature, and that len(features) = 1 . Let's say that you want to predict gas production when porosity is 15%. Then: In [46]: x_pred = np . array ([ 15 ]) x_pred = x_pred . reshape ( - 1 , len ( features )) # preprocessing required by scikit-learn functions In [47]: model . predict ( x_pred ) Out[47]: array([4313.76671169]) According to the model, gas production = 4313 Mcf/day when porosity = 15%. What if we want predict the response variable from multiple instances of a feature? Let's try porosity 14% and 18%. Then: In [48]: x_pred = np . array ([ 14 , 18 ]) x_pred = x_pred . reshape ( - 1 , len ( features )) # preprocessing required by scikit-learn functions In [49]: model . predict ( x_pred ) Out[49]: array([4025.98596884, 5177.10894024]) We can extend on this, and draw a prediction line for all possible values of the feature. Reasonable real-life values of rock porosity ranges between $[0, 40]$ . In [50]: x_pred = np . linspace ( 0 , 40 , 200 ) # 200 data points between 0 ~ 40 x_pred = x_pred . reshape ( - 1 , len ( features )) # preprocessing required by scikit-learn functions y_pred = model . predict ( x_pred ) In [52]: import matplotlib.pyplot as plt plt . style . use ( 'default' ) plt . style . use ( 'ggplot' ) fig , ax = plt . subplots ( figsize = ( 7 , 3.5 )) ax . plot ( x_pred , y_pred , color = 'k' , label = 'Regression model' ) ax . scatter ( X , y , edgecolor = 'k' , facecolor = 'grey' , alpha = 0.7 , label = 'Sample data' ) ax . set_ylabel ( 'Gas production (Mcf/day)' , fontsize = 14 ) ax . set_xlabel ( 'Porosity (%)' , fontsize = 14 ) ax . legend ( facecolor = 'white' , fontsize = 11 ) ax . text ( 0.55 , 0.15 , '$y = %.2f x_1 - %.2f $' % ( model . coef_ [ 0 ], abs ( model . intercept_ )), fontsize = 17 , transform = ax . transAxes ) fig . tight_layout () WARNING! Be careful when predicting a point outside the observed range of feautures. The relationship among variables may change as you move outside the observed range, but you never know because you don't have the data. The observed relationship may be locally linear, but there may be unobserved curves on the outside range of your data. Pythonic Tip: Forcing zero y-intercept Sometimes you want to force y-intercept = 0. This can be done by setting fit_intercept=False when instantiating the linear regression model class. Printing the model y-intercept will output 0.0 . But be aware, \"generally it is essential to include the constant in a regression model\", because \"the constant (y-intercept) absorbs the bias for the regression model\", as Jim Frost says in his post . In [53]: ols = linear_model . LinearRegression ( fit_intercept = False ) model = ols . fit ( X , y ) In [54]: model . intercept_ Out[54]: 0.0 In figure (7) , I generated some synthetic data below to illustrate the effect of forcing zero y-intercept. Forcing a zero y-intercept can be both desirable or undesirable. If you have a reason to believe that y-intercept must be zero, set fit_intercept=False . Figure 7: Effect of forcing zero y-intercept Source Code For Figure (7) import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model df = pd.read_csv('https://aegis4048.github.io/downloads/notebooks/sample_data/linear_data.csv') features = ['X'] target = 'Y' X = df[features].values.reshape(-1, len(features)) y = df[target].values x_pred = np.linspace(0, 40, 200).reshape(-1, len(features)) # prediction line ##################################### Fit linear model #################################### ols_1 = linear_model.LinearRegression() model_1 = ols_1.fit(X, y) response_1 = model_1.predict(x_pred) ################################# Force zero y-intercept ################################## ols_2 = linear_model.LinearRegression(fit_intercept=False) model_2 = ols_2.fit(X, y) response_2 = model_2.predict(x_pred) ########################################## Plot ########################################### plt.style.use('default') plt.style.use('ggplot') fig, axes = plt.subplots(1, 2, figsize=(10, 4)) fig.suptitle('Effect of enforcing zero y-intercept', fontsize=15) axes[0].plot(x_pred, response_1, color='k', label='Regression model') axes[0].scatter(X, y, edgecolor='k', facecolor='grey', alpha=0.7, label='Sample data') axes[0].set_ylabel('Y', fontsize=14) axes[0].set_xlabel('X', fontsize=14) axes[0].legend(facecolor='white', fontsize=11, loc='best') axes[0].set_ylim(0, 1600) axes[0].set_xlim(0, 0.05) axes[0].text(0.47, 0.15, '$y = %.1f x_1 + %.1f $' % (model_1.coef_[0], model_1.intercept_), fontsize=12, transform=axes[0].transAxes) axes[0].text(0.77, 0.3, 'aegis4048.github.io', fontsize=10, ha='center', va='center', transform=axes[0].transAxes, color='grey', alpha=0.5) axes[1].plot(x_pred, response_2, color='k', label='Regression model') axes[1].scatter(X, y, edgecolor='k', facecolor='grey', alpha=0.7, label='Sample data') axes[1].set_ylabel('Y', fontsize=14) axes[1].set_xlabel('X', fontsize=14) axes[1].legend(facecolor='white', fontsize=11, loc='best') axes[1].set_ylim(0, 1600) axes[1].set_xlim(0, 0.05) axes[1].text(0.55, 0.15, '$y = %.1f x_1 + %.1f $' % (model_2.coef_[0], model_2.intercept_), fontsize=12, transform=axes[1].transAxes) axes[1].text(0.77, 0.3, 'aegis4048.github.io', fontsize=10, ha='center', va='center', transform=axes[1].transAxes, color='grey', alpha=0.5) fig.tight_layout(rect=[0, 0, 1, 0.94]) Pythonic Tip: 3D+ linear regression with scikit-learn Fit multi-linear model Let's fit the model with four features: Por , Brittle , Perm , and TOC . Then the model of our interest has the following structure: $$ \\text{Gas Prod.} = \\beta_1 \\cdot \\text{Por} + \\beta_2 \\cdot \\text{Brittle} + \\beta_3 \\cdot \\text{Perm} + \\beta_4 \\cdot \\text{TOC} + \\beta_0 \\tag{5}$$ With scikit-learn, fitting 3D+ linear regression is no different from 2D linear regression, other than declaring multiple features in the beginning. The rest is exactly the same. We will declare four features: features = ['Por', 'Brittle', 'Perm', 'TOC']. In [16]: import pandas as pd import numpy as np from sklearn import linear_model file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) features = [ 'Por' , 'Brittle' , 'Perm' , 'TOC' ] target = 'Prod' X = df [ features ] . values . reshape ( - 1 , len ( features )) y = df [ target ] . values ols = linear_model . LinearRegression () model = ols . fit ( X , y ) In [17]: model . coef_ Out[17]: array([244.60011793, 31.58801063, 86.87367291, 325.19354135]) In [15]: model . intercept_ Out[15]: -1616.4561900851832 Based on the result of the fit, we obtain the following linear regression model: $$ \\text{Gas Prod.} = 244.6 \\cdot \\text{Por} + 31.6 \\cdot \\text{Brittle} + 86.9 \\cdot \\text{Perm} + 325.2 \\cdot \\text{TOC} - 1616.5 \\tag{6}$$ Accuracy assessment: $R&#94;2$ In the same we evaluated model performance with 2D linear model above , we can evaluate the 3D+ model performance with R-squared with model.score(X, y) . X is the features, and y is the response variable used to fit the model. In [106]: model . score ( X , y ) Out[106]: 0.9452003827311295 Make future prediction Let's make one prediction of gas production rate when: Por = 12 (%) Brittle = 81 (%) VR = 2.31 (%) AI = 2.8 (kg/m2s*10&#94;6) In [107]: x_pred = np . array ([ 12 , 81 , 2.31 , 2.8 ]) x_pred = x_pred . reshape ( - 1 , len ( features )) In [108]: model . predict ( x_pred ) Out[108]: array([4555.02177976]) This time, let's make two predictions of gas production rate when: Por = 12 (%) Brittle = 81 (%) VR = 2.31 (%) AI = 2.8 (kg/m2s*10&#94;6) Por = 15 (%) Brittle = 60 (%) VR = 2.5 (%) AI = 1 (kg/m2s*10&#94;6) In [109]: x_pred = np . array ([[ 12 , 81 , 2.31 , 2.8 ], [ 15 , 60 , 2.5 , 1 ]]) x_pred = x_pred . reshape ( - 1 , len ( features )) In [100]: model . predict ( x_pred ) Out[100]: array([4555.02177976, 5312.03205244]) 2. Introduction to multicollinearity While an accuracy of a multi-linear model in predicting a response variable may be reliable, the value of individual regression coefficient may not be reliable under multicollinearity. Note that the value of regression coefficient for porosity in eq (4) is 287.7, while it is 244.6 in eq (6) . In figure (8) , I simulated multiple model fits with different combinations of features to show the fluctuating regression coefficient values, even when the R-squared value is high. Figure 8: Unstable regression coefficients due to multicollinearity Source Code For Figure (8) import pandas as pd import numpy as np from sklearn import linear_model import matplotlib.pyplot as plt file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd.read_csv(file) ######################################################################################## features = ['Por', 'Brittle', 'Perm', 'TOC', 'AI', 'VR'] target = 'Prod' X = df[features].values.reshape(-1, len(features)) y = df[target].values ols = linear_model.LinearRegression() model = ols.fit(X, y) print('Features : %s' % features) print('Regression Coefficients : ', [round(item, 2) for item in model.coef_]) print('R-squared : %.2f' % model.score(X, y)) print('Y-intercept : %.2f' % model.intercept_) print('') ######################################################################################## features = ['Por', 'Brittle', 'Perm', 'TOC', 'VR'] target = 'Prod' X = df[features].values.reshape(-1, len(features)) y = df[target].values ols = linear_model.LinearRegression() model = ols.fit(X, y) print('Features : %s' % features) print('Regression Coefficients : ', [round(item, 2) for item in model.coef_]) print('R-squared : %.2f' % model.score(X, y)) print('Y-intercept : %.2f' % model.intercept_) print('') ######################################################################################## features = ['Por', 'Brittle', 'Perm', 'TOC', 'AI'] target = 'Prod' X = df[features].values.reshape(-1, len(features)) y = df[target].values ols = linear_model.LinearRegression() model = ols.fit(X, y) print('Features : %s' % features) print('Regression Coefficients : ', [round(item, 2) for item in model.coef_]) print('R-squared : %.2f' % model.score(X, y)) print('Y-intercept : %.2f' % model.intercept_) print('') ######################################################################################## features = ['Por', 'Brittle', 'Perm', 'TOC'] target = 'Prod' X = df[features].values.reshape(-1, len(features)) y = df[target].values ols = linear_model.LinearRegression() model = ols.fit(X, y) print('Features : %s' % features) print('Regression Coefficients : ', [round(item, 2) for item in model.coef_]) print('R-squared : %.2f' % model.score(X, y)) print('Y-intercept : %.2f' % model.intercept_) print('') ######################################################################################## features = ['Por', 'Brittle', 'Perm', 'AI'] target = 'Prod' X = df[features].values.reshape(-1, len(features)) y = df[target].values ols = linear_model.LinearRegression() model = ols.fit(X, y) print('Features : %s' % features) print('Regression Coefficients : ', [round(item, 2) for item in model.coef_]) print('R-squared : %.2f' % model.score(X, y)) print('Y-intercept : %.2f' % model.intercept_) print('') ######################################################################################## features = ['Por', 'Brittle', 'Perm', 'VR'] target = 'Prod' X = df[features].values.reshape(-1, len(features)) y = df[target].values ols = linear_model.LinearRegression() model = ols.fit(X, y) print('Features : %s' % features) print('Regression Coefficients : ', [round(item, 2) for item in model.coef_]) print('R-squared : %.2f' % model.score(X, y)) print('Y-intercept : %.2f' % model.intercept_) print('') ######################################################################################## features = ['Por', 'Brittle', 'TOC', 'VR'] target = 'Prod' X = df[features].values.reshape(-1, len(features)) y = df[target].values ols = linear_model.LinearRegression() model = ols.fit(X, y) print('Features : %s' % features) print('Regression Coefficients : ', [round(item, 2) for item in model.coef_]) print('R-squared : %.2f' % model.score(X, y)) print('Y-intercept : %.2f' % model.intercept_) print('') ######################################################################################## features = ['Por', 'Brittle', 'TOC'] target = 'Prod' X = df[features].values.reshape(-1, len(features)) y = df[target].values ols = linear_model.LinearRegression() model = ols.fit(X, y) print('Features : %s' % features) print('Regression Coefficients : ', [round(item, 2) for item in model.coef_]) print('R-squared : %.2f' % model.score(X, y)) print('Y-intercept : %.2f' % model.intercept_) print('') ######################################################################################## features = ['Por', 'Brittle', 'VR'] target = 'Prod' X = df[features].values.reshape(-1, len(features)) y = df[target].values ols = linear_model.LinearRegression() model = ols.fit(X, y) print('Features : %s' % features) print('Regression Coefficients : ', [round(item, 2) for item in model.coef_]) print('R-squared : %.2f' % model.score(X, y)) print('Y-intercept : %.2f' % model.intercept_) print('') ######################################################################################## features = ['Por', 'Brittle', 'AI'] target = 'Prod' X = df[features].values.reshape(-1, len(features)) y = df[target].values ols = linear_model.LinearRegression() model = ols.fit(X, y) print('Features : %s' % features) print('Regression Coefficients : ', [round(item, 2) for item in model.coef_]) print('R-squared : %.2f' % model.score(X, y)) print('Y-intercept : %.2f' % model.intercept_) print('') ######################################################################################## features = ['Por', 'Brittle'] target = 'Prod' X = df[features].values.reshape(-1, len(features)) y = df[target].values ols = linear_model.LinearRegression() model = ols.fit(X, y) print('Features : %s' % features) print('Regression Coefficients : ', [round(item, 2) for item in model.coef_]) print('R-squared : %.2f' % model.score(X, y)) print('Y-intercept : %.2f' % model.intercept_) The simulation result tells us that even if the model is good at predicting the response variable given features (high R-squared), linear model is not robust enough to fully understand the effect of individual features on the response variable. In such circumstance, we can't trust the values of regression coefficients. Where is this instability coming from? This is because the Por , TOC , and Perm shows strong linear correlation with one another, as shown in the below spearnman's correlation matrix in figure (9) . Figure 9: Correlation matrix of features Source Code For Figure (9) import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd.read_csv(file) df = df.iloc[:, 1:-1] corr = df.corr(method='spearman') # Generate a mask for the upper triangle mask = np.zeros_like(corr, dtype=np.bool) mask[np.triu_indices_from(mask)] = True # Set up the matplotlib figure fig, ax = plt.subplots(figsize=(6, 5)) # Generate a custom diverging colormap cmap = sns.diverging_palette(220, 10, as_cmap=True, sep=100) # Draw the heatmap with the mask and correct aspect ratio sns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0, linewidths=.5) fig.suptitle('Correlation matrix of features', fontsize=15) ax.text(0.77, 0.2, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) fig.tight_layout() When more than two features are used for prediction, you must consider the possibility of each features interacting with one another. For thought experiment, think of two features $x_1$ and $x_2$, and a response variable $y$. Assume that $x_1$ is positively related to $y$. In the other words, increasing $x_1$ increases $y$, and decreasing $x_1$ also decreases $y$. $x_2$ is negatively related to $y$. There is a positive correlation between $x_1$ and $x_2$. Under this sitution, when you increase $x_1$, you expect to increase the value of $y$ because of the positive relationship between $x_1$ and $y$, but this is not always true because increasing $x_1$ also increases $x_2$, which in turn decreases $y$ . The situation in which features are correlated with one another is called muticollinearity . Under multicollinearity, the values of individual regression coefficients are unreliable, and the impact of individual features on a response variable is obfuscated. However, prediction on a response variable is still reliable. When using linear regression coefficients to make business decisions, you must remove the effect of multicollinearity to obtain reliable regression coefficients. Let's say that you are doing a medical research on cervical cancer. You trained a linear regression model with patients' survival rate with respect to many features, in which water consumption being one of them. Your linear regression coefficient for water consumption reports that if a patient increases water consumption by 1.5 L everyday, his survival rate will increase by 2%. Can you trust this analysis? The answer is yes, if there is no sign of multicollinearity. You can actually tell the patient, with confidence, that he must drink more water to increase his chance of survival. However, if there is a sign of multicollinearity, this analysis is not valid. Note that multicollinearity is not restricted on 1 vs 1 relationship. Even if there is minimum 1 vs 1 correlation among features, three or more features together may show multicollinearity. Also note that multicollinearity does not affect prediction accuracy . While the values of individual coefficients may be unreliable, it does not undermine the prediction power of the model. Multicollinearity is an issue only when you want to study the impact of individual features on a response variable. The details of detection & remedies of mutlicollinearity is not discussed here (though I plan to write about it very soon). While the focus of this post is only on multiple linear regression itself, I still wanted to grab your attention as to why you should not always trust your regression coefficients.","tags":"Machine Learning","url":"/mutiple_linear_regression_and_visualization_in_python","loc":"/mutiple_linear_regression_and_visualization_in_python"},{"title":"Spatial Simulation 1: Basics of Variograms","text":"The code snippet assumes Anaconda 5.2.0 version of Python virtual environment Acknowledgement I would like to acknowledge Micahel Pyrcz , Associate Professor at the University of Texas at Austin in the Petroleum and Geosystems Engineering, for developing course materials that helped me write this article. Check out his Youtube Lecture on Variogram , and Variogram Excel numerical demo on his Github repo to help yourself better understand the statistical theories and concepts. Let's say that you are a spatial data analyst of a gold mining company, and want to know the distribution of gold percentage over 100m x 100m mining area. To understand the characteritics of the rock formations, you take 100 random rock samples from the mining area, but obviously these 100 data points are not enough to estimate gold percentage over every single spatial locations in the area. So you analyze the available data (100 rock samples from random locations) and simulate full 2D-surface plot for gold percentage over the mining area. This 2D surface simulation from sparse spatial data is a sequential process that involved a series of geostatistical techniques. Steps: Plot experimental variogram Fit variogram model Apply kriging Apply simulation on top of Kriging Run simulation multiple times and perform additioanl data analyses as needed In this post, the concepts, theory, and methodology of plotting a variogram will be covered. Experimental Variogram: Theory Variogram is a measure of dissimilarity over a distance. It shows how two data points are correlated from a spatial perspective, and provides useful insights when trying to estimate the value of an unknown location using collected sample data from other locations. Tobler's first law of geography states that \"everything is related to everything else, but near things are more related than distant things.\" Variogram shows the correlation between two spatial data points over distances. For example, terrains 1 km apart from each other are more likely to be similar than terrains 100 km apart from each other. Oil wells 500 ft apart from each other are more likely to show similar reservoir characteristics than oil wells 5000 ft apart from each other. Variogram is a function of variance over distance. It has the following equation and plot: $$\\gamma(h) = \\frac{1}{2N(h)}\\sum_{\\alpha =1}&#94;{N(h)}\\left ( z(u_{\\alpha })-z(u_{\\alpha} + h) \\right)&#94;2$$ Variables Explained $\\gamma(h)$ = a measure of dissimilarity vs distance. It is a spatial variance between two data points separated by the distance, $h$. $N(h)$ = number of all data point pairs separated by the distance, $h$. $h$ = lag distance. Separation between two data points. $u_{\\alpha }$ = data point on 2D or 3D space at the location, $\\alpha$. $u_{a} + h$ = data point separated from $u_{\\alpha }$ by the distance, $h$. $z(u_{\\alpha })$ = numerical value of data point, $u_{\\alpha }$ $z(u_{\\alpha} + h)$ = numerical value of data point, $u_{\\alpha} + h$ $\\sigma&#94;2$ = sill. Variance at lag distance, $h$, in which spatial data pairs lose correlation. Observation 1: $z(u_{\\alpha })$ - $z(u_{\\alpha} + h)$ There are two data points on the image: $z(u_{\\alpha })$ and $z(u_{\\alpha } + h)$. These two points are separated by the lag distance, $h$. The equation for variogram observes the difference between these two data points: $$z(u_{\\alpha })-z(u_{\\alpha} + h)$$ Observation 2: $N(h)$ $N(h)$ accounts for all data point pairs that are separated by lag distance $h$. Although only horizontal separation is shown in the image, separation between two data points can be horizontal, vertical, or diagonal. Variogram will calculate the difference between all pairs of data points, $z(u_{\\alpha })-z(u_{\\alpha} + h)$, that are separated lag distance, $h$. $$\\sum_{\\alpha =1}&#94;{N(h)}\\left ( z(u_{\\alpha })-z(u_{\\alpha} + h) \\right)&#94;2$$ Observation 3: $\\gamma (h)$ $\\gamma (h)$ denotes for variability of spatial data points at a lag distance, $h$. Recall that variogram accounts for all pairs separated by distance, $h$. It may seem very simple, but one little dot on a variogram plot is actually obtained after iterating for all pairs separated by $h$. $\\underline{ h = 1m }$ $\\underline{ h = 2m }$ $\\underline{ h = 3m }$ Observe how there were less data pairs connected by red lines for $h = 3m$. As the $h$ increases, there will be fewer number of pairs that are separated by $h$ due to spatial limitation. Observation 4: Sill ($\\sigma&#94;2$) Sill ($\\sigma&#94;2$) is the variance in which spatial data pairs lose correlation. As the distance between two data points increases, it will be less likely that those two data points are related to one another. You may assume that the oil wells separated by 100 ft exibit similar geologic characteristics, but you can't assume the same for a well in Texas and a well in California. Variogram works the similar way. Notes Spatial variance may never reach the sill if there is a trend. Ex: Area trend between well variability Observation 5: Range ($a$) Range is a distance in which the spatial variability reaches the sill ($\\sigma&#94;2$). Let's say that you are an exploration engineer for drilling a new oil well. You have drilled wells A, B, C, D that are each 100ft, 200ft, 300ft, and 400ft apart from the zone you want to drill a next new well, and want to know if you can use the data from the previously drilled wells. The geostatisticians in your team report that the geologic formation in the region has a range of 350 ft. This means that the rocks in the region lose geologic correlation with one another if they are more than 350 ft apart â€” you can't use data from well D because it is 400 ft apart. Observation 6: Nugget Effect ($c_{0}$) The nugget effect refers to the nonzero intercept of the variogram and is an overall estimate of error caused by measurement inaccuracy and environmental variability occurring at fine enough scales to be unresolved by the sampling interval. At distance $h = 0$, we would expect the spatial variance between pairs will be zero, but some variables seem to change in an abrupt manner in very short distance. The nugget effect is like the random noise. It's just the small scale variability that you can't estimate with your large scale variability model. However, if there is no expectation of high degree of discontinuity at distances shorter than the minimum data spacing, experts tend to ignore nugget effect ub geologic engineering. Summary In variogram, low variance ($\\gamma$) represents stronger correlation among data pairs. Spatial data pairs lose correlation with one another when variance ($\\gamma$) at lag distance ($h$) reaches the sill ($\\sigma&#94;2$). If there are variance poitns that exceeds the sill, it indicates the presence of trend, and needs to be detrended before variogram modeling. More information about trend will be discussed later. Experimental Variogram: Search Template Parameters Calculating variogram is challenging because real-life data are not as clean as the 2-D grid sample images shown above. Real-life data are often sparse data that are irregularly spaced. The sparse data you will get in real-life will have very few, or even no data point pairs that are EXACTLY $h$ distance apart from each other. Furthermore, the data points will not always be orthogonal or pararell to each other â€” there will be diagonalities. regular Spacing irregular Spacing So how do we get pairs separated by lag distance, $h$? We need to consider distance, tolerance, azimuth direction, azimuth tolerance, dip direction, dip tolerance, bandwith in horizontal plane, and bandwidth in vertical plane â€” through a set of guidelines called Variogram Search Template . variogram Search Template Choice of Azimuth (Directionality) ellipsoidal growth in orthogonal directions, 3D Natural process does not lead to omnidirectionality. Typically there is a direction of major continuity, such as statigraphic surface, and a direction minimum continuity such as perpendicular layers. The choice of azimuth is carefully decided after combining knowledge of geologists and geologic understanding of the region from previously sampled data. There are three orthogonal directions â€” horizontal major, horizontal minor, and vertical. The \"vertical\" directional is assumed to be the direction orthogonal to the horizontal plane; the vertical direction doesn't have to correspond to the Z-axis. The three mutually orthogonal directions grow in ellipsoidal shape, with the horizontal major axis constituting the longer radius the ellipse. Azimuth decides the direction of horizontal major axis. In typical geometry, an angle ($\\theta$) is measured from the positive X-axis, but in variogram, azimuth is measured from the positive Y-axis. Notes According to an article written by Jared Deutsch , in the absence of strong geologic evidence, a neutral isotropic model can be constructed to assist in determining a principle direction (in later code implementation, isotropy is established by setting azi_tol=90 . You are allowing for $\\pm$90 degrees of azimuth tolerance). Using ordinary kriging, a neutral model would be constructed with an isotropic, long range, high nugget effect variogram. You will check if you can observe any directionality in this neutral model, and combine it with other geologic knowledge to assist in determinng the principle direction. Choice of Azimuth Tolerance Azimuth tolerance should be chosen such that maximum number of pairs are found in the search template and exclude unreasonable associations. A common choice of azimuth tolerance is 22.5Â°. However, it can be changed to make a decision about whether to increase precision, or stability. Reducing angle tolerance will give precise variogram ( figure 1 ), and increasing angle tolerance will give stable variogram ( figure 2 ). figure 1: smaller azimuth tol -> precise figure 2: bigger azimuth tol -> stable Choice of Lag Distance A variogram should span less than the maximum length of the field. For example, if the field A has a dimension of 100km x 100km, the maximum lag distance ($h$) of the calculated variogram should be less than 50 km. Calculating longer lags results in pairing samples from the edges of the field with each other. Due to spatial limitation of the field (recall that field A is only 100km long), there will be fewer pairs that are 80, 90, 100km apart than pairs that are 5, 10, 20km apart from each other. These variogram points will be much less informed than shorter distance pairs, with fewer pairs supporting the calculation. Observe how the figure 4 shows misleading information about the region. The figure tells us that the correlation becomes stronger for pairs separated by $h > 50$ as their distance increases. This is inconsistent with natural process â€” we expect the spatial correlation among pairs to decrease as they are further apart from each other. figure 3: sample Data figure 4: total lag = field length figure 5: total lag = field length / 2 Increasing the number of lag distance will result in fewer lag bins, which means that there will be fewer points on the calculated variogram ( figure 6 ). There will be a trade-off between stability and precision, and an engineer must carefully choose the parameters that is the best representative of the geology of the region. figure 6: stable, but imprecise figure 7: precise, but unstable Choice of Lag Tolerance Lag tolerance is usually half of the lag distance ( figure 8 ). Choosing lag tolerance smaller than 1/2 lag distance will result in missing out data pairs that are not within the lag tolerance ( figure 9 ). Choosing lag tolerance bigger than 1/2 lag distance will result in overlapping of data pairs ( figure 10 ). figure 8: lag tol. = 1/2 lag dist. figure 9: lag tol. < 1/2 lag dist. figure 10: lag tol. > 1/2 lag dist. Although the lag tolerance is usually half of the lag distance, in cases of erratic variograms, we may choose to overlap calculations by setting lag tolerance that is greater than half of the lag distance to increase stability in a variogram. Overlapping calculations has an effect of smoothing out and reducing noise in the calculated variogram ( figure 12 ). Smoothing out the calculated variogram maybe helpful in fitting a variogram model later. However, be careful not to smooth out too much, as it will result in imprecise variogram model that is not representative of the regional geology. A decision between stability and precision must be made. figure 11: noisy figure 12: smooth Choice of Bandwidth Just like azimuth tolerance, bandwidth should be chosen such that maximum number of pairs are found in the search template and exclude unreasonable associations. According to Jared Deutsch , bandwidths are infrequently used as they seldom improve the stability of the calculaed variogram. Instead, a carefully chosen set of angle tolerances (azimuth and dip) are applied. Experimental Variogram: Python Implementation In [ ]: In [ ]:","tags":"Geostatistics","url":"/spatial-simulation-1-basics-of-variograms","loc":"/spatial-simulation-1-basics-of-variograms"}]};